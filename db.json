{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next_8.8/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_100.jpg","path":"images/avatar_100.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_200.jpg","path":"images/avatar_200.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_300.jpg","path":"images/avatar_300.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_500.jpg","path":"images/avatar_500.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar_80.jpg","path":"images/avatar_80.jpg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_16.ico","path":"images/ye_16.ico","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_32.ico","path":"images/ye_32.ico","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/English-Writing.md","hash":"c42b80c014276b7360c35106dbcb2b36db9b72e5","modified":1654394131525},{"_id":"source/_posts/Paper-Reading.md","hash":"cbb2cd2392bf99f0ebdd728761d619e61f40e15d","modified":1654394131544},{"_id":"source/about/index.md","hash":"e0e80648f5891dd9044b1e96cd5d14bc756f4ac0","modified":1654394131545},{"_id":"source/categories/index.md","hash":"495c6e5e08f4b6ac726384fe4cb4c52c5db8de50","modified":1654394131546},{"_id":"source/tags/index.md","hash":"c772e1afbdb89aea2f3824bdb3351bdd89b701b1","modified":1654394131547},{"_id":"source/_posts/GNN.md","hash":"0bcb9b2af289dfa0c2c8801187777e55bc02a067","modified":1654394131526},{"_id":"source/_posts/Monocular-Vision.md","hash":"3455882c9fb762ed3132a19f0bf864aef8603c6b","modified":1654779043865},{"_id":"themes/next_8.8/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1654394160752},{"_id":"themes/next_8.8/.eslintrc.json","hash":"611e15c3fcb41dc68fa8532ee595a1262a1b5a8a","modified":1654394160754},{"_id":"themes/next_8.8/.gitattributes","hash":"aeeca2f1e987d83232d7870d1435a4e3ed66b648","modified":1654394160755},{"_id":"themes/next_8.8/.gitignore","hash":"087b7677078303acb2acb47432165950e4d29b43","modified":1654394160817},{"_id":"themes/next_8.8/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1654394160817},{"_id":"themes/next_8.8/_vendors.yml","hash":"ba72c575e627697a050614411706cb20206d4b71","modified":1654394160821},{"_id":"themes/next_8.8/LICENSE.md","hash":"8cfb03967dd4cbaf3b825271ffce0039aa3fc22a","modified":1654394160818},{"_id":"themes/next_8.8/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1654394160822},{"_id":"themes/next_8.8/package.json","hash":"e527d094273cf3be4766630bbfe6cc8cf1eeb529","modified":1654394160939},{"_id":"themes/next_8.8/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1654394160939},{"_id":"themes/next_8.8/.githooks/pre-commit","hash":"b69b9d0b51e27d5d4c87c3242f5067c2cda26e44","modified":1654394160769},{"_id":"themes/next_8.8/.githooks/install.js","hash":"305c2a269818466eed9e381b866c6cd1ad7f8afd","modified":1654394160767},{"_id":"themes/next_8.8/_config.yml","hash":"3e35b293f3d24d23b063c721f1b2e84287fb2d79","modified":1654394160820},{"_id":"themes/next_8.8/.github/CONTRIBUTING.md","hash":"2fdca1040427cabfe27cae6754ec5e027ec7092e","modified":1654394160773},{"_id":"themes/next_8.8/.github/config.yml","hash":"0956bf71b6f36632b63b14d26580458041a5abd2","modified":1654394160799},{"_id":"themes/next_8.8/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1654394160800},{"_id":"themes/next_8.8/.github/PULL_REQUEST_TEMPLATE.md","hash":"a103e2d875f7434191859e5b42075cfa9a4cbcb3","modified":1654394160798},{"_id":"themes/next_8.8/.github/labeler.yml","hash":"ff76a903609932a867082b8ccced906e9910533a","modified":1654394160802},{"_id":"themes/next_8.8/.github/label-commenter-config.yml","hash":"a1aa85a2fc66ff0c52c65bd97b0fa282e297a73f","modified":1654394160801},{"_id":"themes/next_8.8/.github/release-drafter.yml","hash":"de38f816e3023e0a5c1fd1f3c2b626f78bc35246","modified":1654394160802},{"_id":"themes/next_8.8/README.md","hash":"43fe29330352545446a532e6630866251129882a","modified":1654394160819},{"_id":"themes/next_8.8/docs/AUTHORS.md","hash":"579014d47f45b27fd1618b9709f0efe9585c7449","modified":1654394160824},{"_id":"themes/next_8.8/docs/LICENSE.txt","hash":"d1cd5a8e83d3bbdb50f902d2b487813da95ddfd3","modified":1654394160824},{"_id":"themes/next_8.8/languages/README.md","hash":"b1c96465b3bc139bf5ba6200974b66581d8ff85a","modified":1654394160830},{"_id":"themes/next_8.8/languages/ar.yml","hash":"cc7e3e2855348563d746f15c4752b9c63fcdd91a","modified":1654394160830},{"_id":"themes/next_8.8/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1654394160831},{"_id":"themes/next_8.8/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1654394160823},{"_id":"themes/next_8.8/.github/CODE_OF_CONDUCT.md","hash":"593ae64e72d43c020a697eac65b1f9c3483ff097","modified":1654394160771},{"_id":"themes/next_8.8/languages/de.yml","hash":"83023c4246b93a2f89f342afe29a7b9e1185f74f","modified":1654394160831},{"_id":"themes/next_8.8/languages/en.yml","hash":"66445143decfbb5eb7031eb370698e31d5222a7a","modified":1654394160832},{"_id":"themes/next_8.8/languages/es.yml","hash":"07955d78028cea2a590c63fdc2c01ca3ee05a727","modified":1654394160873},{"_id":"themes/next_8.8/languages/fa.yml","hash":"e09fad889ab3ae87874093e1acd51edc9297d869","modified":1654394160874},{"_id":"themes/next_8.8/languages/id.yml","hash":"d7c337ca72efb0bd02ade8b5560c559384ad84dd","modified":1654394160875},{"_id":"themes/next_8.8/languages/fr.yml","hash":"328c255c82e9b561e20a9f51a4d84abc63d1b90a","modified":1654394160875},{"_id":"themes/next_8.8/languages/it.yml","hash":"c038ff0cadbe405750d980bcacfd3900acf96905","modified":1654394160876},{"_id":"themes/next_8.8/languages/ja.yml","hash":"57a35b21aca04ce8bca64fb5933f35626c462ea3","modified":1654394160877},{"_id":"themes/next_8.8/languages/ko.yml","hash":"d6e2add7488065ec4f7d21cfcf7f0eaa877a84f4","modified":1654394160877},{"_id":"themes/next_8.8/languages/pt-BR.yml","hash":"305025e932832328b7e2a8a584638a23c462e68f","modified":1654394160878},{"_id":"themes/next_8.8/languages/nl.yml","hash":"e47858bd1e0d0622c15366ae6c0513d996f589e3","modified":1654394160878},{"_id":"themes/next_8.8/languages/pt.yml","hash":"ff93459250c33d3c7ba06c30164cc4208edf9b33","modified":1654394160879},{"_id":"themes/next_8.8/languages/si.yml","hash":"c15ed758dbad890e856f4fc281208d7b78cc1a59","modified":1654394160880},{"_id":"themes/next_8.8/languages/ru.yml","hash":"7d13108f4a70ff6a162508a49678e4a477fa7b56","modified":1654394160879},{"_id":"themes/next_8.8/languages/uk.yml","hash":"f32871f67c63d26bc4e3e15df9b01f5a41236a50","modified":1654394160881},{"_id":"themes/next_8.8/languages/vi.yml","hash":"e452ea8c48993262a3e8fce9d92072cafabfc734","modified":1654394160882},{"_id":"themes/next_8.8/languages/tr.yml","hash":"d3262d2221b0583a52e5d20a3cd1380f5dc49378","modified":1654394160881},{"_id":"themes/next_8.8/languages/zh-CN.yml","hash":"f8379d15038e22ef7039d91272cb4f36842dbbe1","modified":1654394160883},{"_id":"themes/next_8.8/languages/zh-HK.yml","hash":"c1ee97ceb56da76ecdc7b69fa975f28c8574441b","modified":1654394160884},{"_id":"themes/next_8.8/languages/zh-TW.yml","hash":"70c45076ad722b777956048fcc430eac37844c11","modified":1654394160884},{"_id":"themes/next_8.8/layout/_layout.njk","hash":"2842f3e9fdde5bbd14cac89629221e68d80c8ea1","modified":1654394160886},{"_id":"themes/next_8.8/layout/index.njk","hash":"fa52c3049871e879980cb6abccdea3792ca4ce70","modified":1654394160936},{"_id":"themes/next_8.8/layout/post.njk","hash":"707a50e50b90df5fbeaf8407d12895d04163a290","modified":1654394160937},{"_id":"themes/next_8.8/layout/category.njk","hash":"82f541452cae76a94ee15cb8d8a888f44260a0fd","modified":1654394160935},{"_id":"themes/next_8.8/layout/archive.njk","hash":"aa491dba8f746e626c273a920effedf7d0b32170","modified":1654394160935},{"_id":"themes/next_8.8/test/index.js","hash":"983a505399796b9d9e174ba46d89abbdde38f8ee","modified":1654394161543},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/config.yml","hash":"daeedc5da2ee74ac31cf71846b766ca6499e9fc6","modified":1654394160779},{"_id":"themes/next_8.8/layout/tag.njk","hash":"b6c017d30d08ddd30d66e9c6f3a71aa65d214eac","modified":1654394160938},{"_id":"themes/next_8.8/layout/page.njk","hash":"fddfdee95f5da86eab8a85d6eb1901996d2153cf","modified":1654394160937},{"_id":"themes/next_8.8/.github/workflows/label-commenter.yml","hash":"7dec949b13131783e726facb2f4acde0945db1b8","modified":1654394160803},{"_id":"themes/next_8.8/.github/workflows/labeler.yml","hash":"46d0b29dc561fe571d91fd06a7c8ef606b984c72","modified":1654394160804},{"_id":"themes/next_8.8/.github/workflows/linter.yml","hash":"b57d876c90d1645a52bbba8a52d47ad0b0c96140","modified":1654394160805},{"_id":"themes/next_8.8/.github/workflows/lock.yml","hash":"58eca481fd71088a8ae1dbc04645bcfc03460b87","modified":1654394160805},{"_id":"themes/next_8.8/.github/workflows/release-drafter.yml","hash":"359b74890a47d784e35a5cc3c7885d5cdf302e82","modified":1654394160814},{"_id":"themes/next_8.8/.github/workflows/stale.yml","hash":"32e7dfb55ecf8af66aebfed471be09ef2eb10e18","modified":1654394160815},{"_id":"themes/next_8.8/.github/workflows/tester.yml","hash":"645bb69d0b6cc062c47fabb1ccb2297ccbcfa7f5","modified":1654394160816},{"_id":"themes/next_8.8/docs/ru/README.md","hash":"e1d6bf38cf34972ca2ee5331a727787fe14082a3","modified":1654394160825},{"_id":"themes/next_8.8/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7befb4325b107dd668d9eae3d7e86a34910ce3f2","modified":1654394160827},{"_id":"themes/next_8.8/docs/zh-CN/CONTRIBUTING.md","hash":"a09ceb82b45dd8b7da76c227f3d0bb7eebe7d5d1","modified":1654394160828},{"_id":"themes/next_8.8/docs/zh-CN/README.md","hash":"354b0b0a24cbe97cccf2ec8bd97eb7d624fa0dea","modified":1654394160829},{"_id":"themes/next_8.8/layout/_macro/post-collapse.njk","hash":"d9d8e6d7a6a8c80009dd5334cc17fd3e4977a008","modified":1654394160887},{"_id":"themes/next_8.8/layout/_macro/post.njk","hash":"367cafd3acc1c6a045d8a72de0479aabbf4a3559","modified":1654394160887},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/bug-report.md","hash":"032194e7975564176f2109aa8b7c020fa6d5e6b1","modified":1654394160777},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/feature-request.md","hash":"4a7885fe2c8b25be02ab57c345cd862aeeeeacaf","modified":1654394160782},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/other.md","hash":"618d07b49f4774cd79613d4001984a19d954a6ad","modified":1654394160797},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk","hash":"f3c1fc4b3333cb09a40b6b3b9042e5ab277fe885","modified":1654394160888},{"_id":"themes/next_8.8/layout/_partials/footer.njk","hash":"0347cb6077a969136aac26ebdc205a7817010ee7","modified":1654394160890},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk_backup","hash":"eec74e135d01948361020140c3798769e1e7363b","modified":1654394160889},{"_id":"themes/next_8.8/layout/_partials/pagination.njk","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1654394160900},{"_id":"themes/next_8.8/layout/_partials/comments.njk","hash":"d6b7bb7764e3b471ed6b4e5715f6cbe2dd453f59","modified":1654394160890},{"_id":"themes/next_8.8/layout/_scripts/index.njk","hash":"4eb65641b47ea9b23ed2ddfd69b18f21d7d8f214","modified":1654394160910},{"_id":"themes/next_8.8/layout/_scripts/vendors.njk","hash":"0a1470440f11362df2b1cd6b6228e273d9f999d6","modified":1654394160910},{"_id":"themes/next_8.8/layout/_third-party/fancybox.njk","hash":"53ad3c31762b74e5d29787b37d5e494cc4fded9b","modified":1654394160922},{"_id":"themes/next_8.8/layout/_third-party/pace.njk","hash":"13b2a77b4858a127f458ea092b6f713b052befac","modified":1654394160926},{"_id":"themes/next_8.8/layout/_third-party/index.njk","hash":"33a4a3275474bd3bb2e8d1b0ea01b42dda9ea608","modified":1654394160924},{"_id":"themes/next_8.8/layout/_third-party/quicklink.njk","hash":"73bc15a9c3c5c239ab90efa19a1e721f41f3cb93","modified":1654394160927},{"_id":"themes/next_8.8/scripts/events/index.js","hash":"8bca7ae3cebb3857866d718a562c5d8820fcfbe5","modified":1654394160941},{"_id":"themes/next_8.8/layout/_third-party/rating.njk","hash":"d0444179fec512760ab1d4f76928d795b971c884","modified":1654394160928},{"_id":"themes/next_8.8/scripts/filters/default-injects.js","hash":"0c9a1fe9906672724dbf274154a37bac1915ca2c","modified":1654394160960},{"_id":"themes/next_8.8/scripts/filters/locals.js","hash":"8499b9c8c6cdae8aa7e4f5ec5b4b76037969db76","modified":1654394160961},{"_id":"themes/next_8.8/scripts/filters/minify.js","hash":"9789307212d729c8cb65e3541348938a1965ff6f","modified":1654394160962},{"_id":"themes/next_8.8/scripts/filters/post.js","hash":"5a132b7f9280a40b3d5fb40928c8cbbe071fe6f6","modified":1654394161388},{"_id":"themes/next_8.8/scripts/filters/number.js","hash":"e709fc433b8834e8f5141758c818210cc052eeee","modified":1654394160963},{"_id":"themes/next_8.8/scripts/helpers/engine.js","hash":"18cc82558e7a9f3b6086c41ce9de0c46e807a66c","modified":1654394161392},{"_id":"themes/next_8.8/scripts/helpers/font.js","hash":"0a6fa582a0890ecaf5f03f758a730936e48aeca1","modified":1654394161394},{"_id":"themes/next_8.8/scripts/helpers/next-config.js","hash":"e73f43f1bcb46965e317285d6831e129a40ea59b","modified":1654394161396},{"_id":"themes/next_8.8/scripts/helpers/next-url.js","hash":"98fc68cf3fcd6253bbb94068ab1d86578a4ef9ea","modified":1654394161398},{"_id":"themes/next_8.8/scripts/helpers/next-vendors.js","hash":"52acbc74c1ead8a77cd3bbcba4e033053683f7d0","modified":1654394161402},{"_id":"themes/next_8.8/scripts/tags/caniuse.js","hash":"8e912c715702addaf0cefe63e580e45b97ae8c3f","modified":1654394161407},{"_id":"themes/next_8.8/scripts/tags/button.js","hash":"86c71c73a63744efbbbb367612871fede0d69529","modified":1654394161405},{"_id":"themes/next_8.8/scripts/tags/center-quote.js","hash":"b4d12e6fe29089be0f43bafc9eea736602cd16bf","modified":1654394161409},{"_id":"themes/next_8.8/scripts/tags/group-pictures.js","hash":"1c609312a71d47f838226346aad5c2e1c35f15dd","modified":1654394161411},{"_id":"themes/next_8.8/scripts/tags/index.js","hash":"255dd1090e8319b557eeca43571f0e4f8aab013b","modified":1654394161414},{"_id":"themes/next_8.8/scripts/tags/label.js","hash":"c18b0e619a779ed40be7f014db92af18f45fbd5c","modified":1654394161416},{"_id":"themes/next_8.8/scripts/tags/link-grid.js","hash":"3f358bb78c5c6fdf45de287f3ead553e3a6a93c2","modified":1654394161418},{"_id":"themes/next_8.8/scripts/tags/note.js","hash":"a12fd53e421400836a3722ae69130969558d6ac0","modified":1654394161422},{"_id":"themes/next_8.8/scripts/tags/mermaid.js","hash":"b3844e168b51a99d495ca05562ffac47677f5728","modified":1654394161420},{"_id":"themes/next_8.8/scripts/tags/pdf.js","hash":"317ba4611020cc840854386dde098dbbe452777e","modified":1654394161423},{"_id":"themes/next_8.8/scripts/tags/tabs.js","hash":"e0ed5fe1bc9d2957952a1aacdf3252d6ef3f9743","modified":1654394161424},{"_id":"themes/next_8.8/scripts/tags/video.js","hash":"f6ad3f52779f0636251238d3cbdc5b6f91cc5aba","modified":1654394161424},{"_id":"themes/next_8.8/layout/_partials/languages.njk","hash":"537026fc120adeef9148c98ebf074207e3810538","modified":1654394160896},{"_id":"themes/next_8.8/layout/_partials/widgets.njk","hash":"967594ee64805e27b7ff9d957e23ab3f5c948600","modified":1654394160909},{"_id":"themes/next_8.8/source/css/_colors.styl","hash":"a88430865c99f47ce1d8240f8895819b8b7b0c06","modified":1654394161426},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1654394161502},{"_id":"themes/next_8.8/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1654394161503},{"_id":"themes/next_8.8/source/images/avatar_100.jpg","hash":"01b495adc2782bb5b371c693c6d8c33a0caacb53","modified":1654394161505},{"_id":"themes/next_8.8/source/images/avatar_200.jpg","hash":"4a76d1527e511350c9112e899046a34be59ad278","modified":1654394161506},{"_id":"themes/next_8.8/source/css/_mixins.styl","hash":"2ca820b221fb7458e6ef4fbcff826e1d1cf4b473","modified":1654394161479},{"_id":"themes/next_8.8/source/css/main.styl","hash":"38b8a12681a3a04bed02aa1659054912ed6def11","modified":1654394161500},{"_id":"themes/next_8.8/source/css/noscript.styl","hash":"7dc97674c232f6ca71e48b95e3f66472cd8e9c05","modified":1654394161501},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1654394161510},{"_id":"themes/next_8.8/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1654394161512},{"_id":"themes/next_8.8/source/images/avatar_80.jpg","hash":"465e53598964df3852f08794cb73c5e459855e92","modified":1654394161509},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1654394161511},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1654394161511},{"_id":"themes/next_8.8/source/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1654394161512},{"_id":"themes/next_8.8/source/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1654394161513},{"_id":"themes/next_8.8/source/js/comments.js","hash":"0b4daf0ce610760bd52e95d423f61f3e1c72442a","modified":1654394161515},{"_id":"themes/next_8.8/source/js/bookmark.js","hash":"1457291a7244b7786ec35b949d97183e4fbd181d","modified":1654394161514},{"_id":"themes/next_8.8/source/js/comments-buttons.js","hash":"81ea6cbcdf0357094753d7523919c1eafa38e79f","modified":1654394161515},{"_id":"themes/next_8.8/source/js/config.js","hash":"211a9ab35205ccfa6b7c74394bade84da0d00af7","modified":1654394161516},{"_id":"themes/next_8.8/source/js/motion.js","hash":"ad4a3ddc154840fe5c5ca7727da830004a53e7be","modified":1654394161517},{"_id":"themes/next_8.8/source/js/next-boot.js","hash":"b0bdb542a809932182cfbb8772328115142a0b77","modified":1654394161518},{"_id":"themes/next_8.8/source/js/schedule.js","hash":"6dade4388aa6579576a35758075134f573985d57","modified":1654394161519},{"_id":"themes/next_8.8/source/js/pjax.js","hash":"85293c253e0f43540572c4e4615c712325a732e2","modified":1654394161519},{"_id":"themes/next_8.8/test/helpers/index.js","hash":"2fb58dca3df2fe53116ee2b1232fa26ebe7b2ce5","modified":1654394161542},{"_id":"themes/next_8.8/source/js/utils.js","hash":"c13fa66aae52f59f88881738c00ebdcaf0209496","modified":1654394161540},{"_id":"themes/next_8.8/test/helpers/font.js","hash":"6f5076bd3f2724e47b46ca69028393a9b6275cd1","modified":1654394161542},{"_id":"themes/next_8.8/test/tags/center-quote.js","hash":"2ac4b5a358681691a17e736de06fce0b640a7023","modified":1654394161546},{"_id":"themes/next_8.8/test/helpers/next-url.js","hash":"08e84781f1cd54e5634b86877ad9cefae4a78e95","modified":1654394161543},{"_id":"themes/next_8.8/test/tags/button.js","hash":"a50ca44eaec3d91c2958e3157d624cd3e68828c7","modified":1654394161544},{"_id":"themes/next_8.8/test/tags/caniuse.js","hash":"2852be850d9103c25114253a45e6c62e32517de4","modified":1654394161545},{"_id":"themes/next_8.8/test/tags/index.js","hash":"5cad001936a694bf32d59751cc2b68a66199f976","modified":1654394161548},{"_id":"themes/next_8.8/test/tags/mermaid.js","hash":"f718a3d0e303d842e2ca5a3b162539a49e45a520","modified":1654394161550},{"_id":"themes/next_8.8/test/tags/label.js","hash":"6cad7d84c42511459a89cda3971e8ea5cdee0125","modified":1654394161549},{"_id":"themes/next_8.8/test/tags/group-pictures.js","hash":"8f66d3c6f03fb11d85aa2ab05c9b3c9aa2b4e994","modified":1654394161547},{"_id":"themes/next_8.8/test/tags/link-grid.js","hash":"41730266306c02362258384cd73659223928361f","modified":1654394161550},{"_id":"themes/next_8.8/test/tags/pdf.js","hash":"2d114596a8a180b2f3cd2a9c6528a328961f12d4","modified":1654394161551},{"_id":"themes/next_8.8/test/tags/note.js","hash":"161a81ce749e239d2403681372d48ecc1b51d7b9","modified":1654394161551},{"_id":"themes/next_8.8/test/tags/video.js","hash":"88db9a3a26cd35525c43c0339fcd1c5965ec9518","modified":1654394161553},{"_id":"themes/next_8.8/layout/_partials/head/head-unique.njk","hash":"bd87e3a877ebab4508fc2b48b41c96b45c4dd970","modified":1654394160891},{"_id":"themes/next_8.8/test/validate/index.js","hash":"560862194991c5963da5a411629d8e6c71d20ee2","modified":1654394161553},{"_id":"themes/next_8.8/test/tags/tabs.js","hash":"b19d2592347eae5d6a7a97ca7e8cec03e8f25b51","modified":1654394161552},{"_id":"themes/next_8.8/layout/_partials/head/head.njk","hash":"abcc550cb14374fb7452d6edee63967ad9583d1c","modified":1654394160892},{"_id":"themes/next_8.8/layout/_partials/header/index.njk","hash":"1b2ae17f3c394ce310fe2d9ed5f4d07d8cc74ae7","modified":1654394160893},{"_id":"themes/next_8.8/layout/_partials/header/brand.njk","hash":"8e08c19e1bd92f3179907b0ff3743d6e2371d7ae","modified":1654394160893},{"_id":"themes/next_8.8/layout/_partials/header/menu-item.njk","hash":"f066390762faf6684a523e2eb943420023aac2b1","modified":1654394160894},{"_id":"themes/next_8.8/layout/_partials/page/categories.njk","hash":"b352346dd2cb42f7eeaec5e39d9a2a353b029775","modified":1654394160898},{"_id":"themes/next_8.8/layout/_partials/header/menu.njk","hash":"67372599fe025ebe442b73151e5bb56415758356","modified":1654394160894},{"_id":"themes/next_8.8/layout/_partials/page/page-header.njk","hash":"92553feb26f30f7fc9147bc4ef122908a9da06be","modified":1654394160898},{"_id":"themes/next_8.8/layout/_partials/header/sub-menu.njk","hash":"940cad08a67e6c361214045096bd3cdffdf44fcf","modified":1654394160895},{"_id":"themes/next_8.8/layout/_partials/page/schedule.njk","hash":"130e776575d634201d4f8ef3d78dc12624f19fde","modified":1654394160899},{"_id":"themes/next_8.8/layout/_partials/page/tags.njk","hash":"752df7d12360a077c51a25609916a3ecc1763bb3","modified":1654394160900},{"_id":"themes/next_8.8/layout/_partials/page/breadcrumb.njk","hash":"9c136edd2248e2d50c1f6110b75e2b75c299bbd7","modified":1654394160897},{"_id":"themes/next_8.8/layout/_partials/post/post-footer.njk","hash":"e3502059bcc443ce932946a9891fcbe8b2bb362d","modified":1654394160903},{"_id":"themes/next_8.8/layout/_partials/post/post-copyright.njk","hash":"0ebc0142abebbeef4278e32abb543c7d7fa75d88","modified":1654394160901},{"_id":"themes/next_8.8/layout/_partials/post/post-reward.njk","hash":"58b3f657a47bae406e5fcf19cd5e42680785ac71","modified":1654394160905},{"_id":"themes/next_8.8/layout/_partials/post/post-followme.njk","hash":"ebf83083856f8bd81ad47ffb985d44e338b4e6bb","modified":1654394160902},{"_id":"themes/next_8.8/layout/_partials/search/algolia-search.njk","hash":"93fbb449fbd599cb4315d7eb0daeb239811b233f","modified":1654394160906},{"_id":"themes/next_8.8/layout/_partials/post/post-meta.njk","hash":"9a9c4fb7e7c4fe4b7d474bdfdb4ed2b0a5423df2","modified":1654394160904},{"_id":"themes/next_8.8/layout/_partials/post/post-related.njk","hash":"80d3dac42740d2aef677e25165e31c05eb048887","modified":1654394160905},{"_id":"themes/next_8.8/layout/_partials/search/index.njk","hash":"9766852e72c1809d8c1eea71ac6116b4cc0886d2","modified":1654394160907},{"_id":"themes/next_8.8/layout/_third-party/analytics/growingio.njk","hash":"9ff9ec05c2037beea229a6bb698f9e3546973220","modified":1654394160914},{"_id":"themes/next_8.8/layout/_third-party/analytics/google-analytics.njk","hash":"52ad137450f7b3d6a330e16b3ed1c6174290f0eb","modified":1654394160913},{"_id":"themes/next_8.8/layout/_third-party/analytics/baidu-analytics.njk","hash":"3e80332f88b101141be69f2a07f54ed8c053eabb","modified":1654394160912},{"_id":"themes/next_8.8/layout/_third-party/analytics/cloudflare.njk","hash":"c7cea42f6db2137c11ca1d83e43fcb7ad7ccfb89","modified":1654394160912},{"_id":"themes/next_8.8/layout/_third-party/analytics/index.njk","hash":"465fcffd4216f8ca0ea2613fe9cf7308f71b9da5","modified":1654394160915},{"_id":"themes/next_8.8/layout/_third-party/chat/tidio.njk","hash":"3fbc72427c1211e5dcfd269af1a74852a7ba5c1a","modified":1654394160917},{"_id":"themes/next_8.8/layout/_third-party/chat/gitter.njk","hash":"375a86f0b19e130cfa7707007e3a53d9ae7c9b64","modified":1654394160916},{"_id":"themes/next_8.8/layout/_third-party/chat/chatra.njk","hash":"09d2c9487d75894d45a823e3237ae9f90fd6ee01","modified":1654394160915},{"_id":"themes/next_8.8/layout/_third-party/comments/changyan.njk","hash":"5f7967bd946060f4102263a552ddfbae9975e7ea","modified":1654394160917},{"_id":"themes/next_8.8/layout/_third-party/comments/disqus.njk","hash":"b0828dd1b1fd66ecd612d9e886a08e7579e9a4f7","modified":1654394160918},{"_id":"themes/next_8.8/layout/_third-party/comments/gitalk.njk","hash":"6fd4df5c21cfe530dbb0c012bc0b202f2c362b9c","modified":1654394160919},{"_id":"themes/next_8.8/layout/_third-party/comments/livere.njk","hash":"b8e0d5de584cece5e05b03db5b86145aa1e422b4","modified":1654394160920},{"_id":"themes/next_8.8/layout/_third-party/comments/utterances.njk","hash":"a7921be7328e1509d33b435175f5333a9aada66f","modified":1654394160921},{"_id":"themes/next_8.8/layout/_third-party/comments/disqusjs.njk","hash":"c5086b4c35f730f82c99c4a8317f2f153ebde869","modified":1654394160919},{"_id":"themes/next_8.8/layout/_third-party/comments/isso.njk","hash":"38badcc7624a13961381c2465478056b9602aee5","modified":1654394160920},{"_id":"themes/next_8.8/layout/_third-party/search/algolia-search.njk","hash":"67f67a77f27103177b9940446f43610229536d82","modified":1654394160928},{"_id":"themes/next_8.8/layout/_third-party/math/mathjax.njk","hash":"a62aa1ed4e35b8d0451d83f341bf0a97538bc9a4","modified":1654394160926},{"_id":"themes/next_8.8/layout/_third-party/math/katex.njk","hash":"a84db8bc8804335f95609a221ac1746433dcdc89","modified":1654394160925},{"_id":"themes/next_8.8/layout/_third-party/statistics/index.njk","hash":"866ffa15a3250678eb8a90aa6f609fa965db90fd","modified":1654394160931},{"_id":"themes/next_8.8/layout/_third-party/statistics/busuanzi-counter.njk","hash":"d97790e4b442a1e3ded7d7b4f84b8ee6cdb6e8ea","modified":1654394160930},{"_id":"themes/next_8.8/layout/_third-party/statistics/firestore.njk","hash":"af5336e8bbdc4638435971da115bb7443d374ade","modified":1654394160930},{"_id":"themes/next_8.8/layout/_third-party/search/localsearch.njk","hash":"210c32b654adae3d8076c4417d370b42af258cea","modified":1654394160929},{"_id":"themes/next_8.8/layout/_third-party/statistics/lean-analytics.njk","hash":"8703d1855bb8d251c9b7c2940b7e3be525e53000","modified":1654394160932},{"_id":"themes/next_8.8/layout/_third-party/tags/mermaid.njk","hash":"dd8f963acd5a3685be46fd5319c06df0308d99b2","modified":1654394160933},{"_id":"themes/next_8.8/scripts/events/lib/config.js","hash":"a912944cae0d864458d365867b8a9c89f348e68a","modified":1654394160942},{"_id":"themes/next_8.8/layout/_third-party/tags/pdf.njk","hash":"0386c708975cc5faea4f782611c5d2c6b8ac2850","modified":1654394160934},{"_id":"themes/next_8.8/scripts/events/lib/highlight.js","hash":"00cec6980cafd417def885f496371856cd524a25","modified":1654394160942},{"_id":"themes/next_8.8/scripts/events/lib/vendors.js","hash":"2f7057a8d3fce08aa7e2a17d7b7a1f03ac3d8ed6","modified":1654394160945},{"_id":"themes/next_8.8/scripts/events/lib/utils.js","hash":"8508e96a5f883a5a57d8c1b8b5ea438fa29aafd3","modified":1654394160944},{"_id":"themes/next_8.8/scripts/events/lib/injects.js","hash":"1f1ea7b579a49f17574c31d78d663c54896133eb","modified":1654394160943},{"_id":"themes/next_8.8/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1654394160947},{"_id":"themes/next_8.8/scripts/filters/comment/changyan.js","hash":"cfff8331fdaa2ede4ab08c58cfc6d98c7d2374d9","modified":1654394160946},{"_id":"themes/next_8.8/scripts/filters/comment/disqus.js","hash":"3283bdd6e5ac7d10376df8ddd5faaec5dc1bd667","modified":1654394160948},{"_id":"themes/next_8.8/scripts/filters/comment/default-config.js","hash":"1cb58aa6b88f7461c3c3f9605273686adcc30979","modified":1654394160948},{"_id":"themes/next_8.8/scripts/filters/comment/gitalk.js","hash":"96e58efba0dc76af409cc7d2db225f0fe4526ea8","modified":1654394160957},{"_id":"themes/next_8.8/scripts/filters/comment/isso.js","hash":"c22cbccd7d514947e084eeac6a3af1aa41ec857a","modified":1654394160958},{"_id":"themes/next_8.8/scripts/filters/comment/disqusjs.js","hash":"70eb507ef7f1a4fc3ca71a3814cc57afe7f3f60c","modified":1654394160949},{"_id":"themes/next_8.8/scripts/filters/comment/livere.js","hash":"bb8ebb541c40362c0cbbd8e83d3b777302bb6c40","modified":1654394160959},{"_id":"themes/next_8.8/scripts/filters/comment/utterances.js","hash":"a50718c081685fd35ff8ea9ca13682c284399ed8","modified":1654394160960},{"_id":"themes/next_8.8/source/css/_variables/Gemini.styl","hash":"c4537fa2de33d98baff2c87a73801770414e0b69","modified":1654394161496},{"_id":"themes/next_8.8/source/css/_variables/Mist.styl","hash":"ee5024be8e39605f0c6d71db038e15e0693d0f41","modified":1654394161497},{"_id":"themes/next_8.8/layout/_partials/sidebar/site-overview.njk","hash":"c5c38b4fb137cc799a6ec31f391d1efc12234c8c","modified":1654394160908},{"_id":"themes/next_8.8/source/css/_variables/Muse.styl","hash":"d3a8f6e71c86926d0c2a247a31d7446d829736d5","modified":1654394161498},{"_id":"themes/next_8.8/source/css/_variables/Pisces.styl","hash":"58014a2d087c4126058a99b5b1cb7d8a2eb6224d","modified":1654394161499},{"_id":"themes/next_8.8/source/js/third-party/pace.js","hash":"0ebee77b2307bf4b260afb06c060171ef42b7141","modified":1654394161533},{"_id":"themes/next_8.8/source/css/_variables/base.styl","hash":"0876b50a58f114bc0b7982b85c5e5011730253b8","modified":1654394161499},{"_id":"themes/next_8.8/layout/_partials/search/localsearch.njk","hash":"f73d25a8ccfdd5d4ca2953dc434ff8ce36034c57","modified":1654394160907},{"_id":"themes/next_8.8/source/js/schemes/muse.js","hash":"e1b4bf9aa47d14c790a0920d7dbb3e9812d4358b","modified":1654394161520},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","hash":"8a847a7bbdbc0086dd1de12b82107a854b43f5e5","modified":1654394161530},{"_id":"themes/next_8.8/source/css/_common/components/index.styl","hash":"991c1f80995cec418dc00d3d6b13e2d911ac9894","modified":1654394161428},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","hash":"539c5bb51244f7f4aa98884f3229d128c1cefc40","modified":1654394161534},{"_id":"themes/next_8.8/source/css/_common/components/back-to-top.styl","hash":"2bbf9046ef2a8f99ef3668bbb8be4e52e9d97bb7","modified":1654394161427},{"_id":"themes/next_8.8/source/css/_common/outline/index.styl","hash":"7782dfae7a0f8cd61b936fa8ac980440a7bbd3bb","modified":1654394161455},{"_id":"themes/next_8.8/source/js/third-party/rating.js","hash":"a1f44247c18ac00ee3e0026560398429e4c77dd7","modified":1654394161534},{"_id":"themes/next_8.8/source/css/_common/outline/mobile.styl","hash":"2db4462e9cb87b8aef3f50f850fed407de16da3e","modified":1654394161456},{"_id":"themes/next_8.8/source/css/_common/components/reading-progress.styl","hash":"f3defd56be33dba4866a695396d96c767ce63182","modified":1654394161441},{"_id":"themes/next_8.8/source/css/_common/scaffolding/comments.styl","hash":"cf8446f4378dcab27b55ede1635c608ae6b8a5c8","modified":1654394161466},{"_id":"themes/next_8.8/source/css/_common/scaffolding/index.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1654394161469},{"_id":"themes/next_8.8/source/css/_common/scaffolding/buttons.styl","hash":"f768ecb2fe3e9384777c1c115cd7409e9155edd7","modified":1654394161465},{"_id":"themes/next_8.8/layout/_third-party/math/index.njk","hash":"1856c4b035c5b8e64300a11af0461b519dfc4cf4","modified":1654394160925},{"_id":"themes/next_8.8/source/css/_common/scaffolding/toggles.styl","hash":"90f7d3baab061e860172b536c9edc38c7fd2ef5c","modified":1654394161479},{"_id":"themes/next_8.8/source/css/_schemes/Gemini/index.styl","hash":"f51b6a4f06359ed56b2d10caa6f15362d3b3751d","modified":1654394161481},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_header.styl","hash":"b1054313ca9419e76fea0451417c881616f50a38","modified":1654394161481},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_layout.styl","hash":"00366a6bd1a66f99f845c5ebfc9e8cf56651b815","modified":1654394161482},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tables.styl","hash":"b9388016f8d9274703e77e306a1feaad1b7b9d6c","modified":1654394161471},{"_id":"themes/next_8.8/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1654394161470},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_header.styl","hash":"fd89988442f380cba907752fe3f608e3498f8c93","modified":1654394161486},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_posts-expand.styl","hash":"c9a9e07b721bb2376e24753ae0a9452431439114","modified":1654394161485},{"_id":"themes/next_8.8/source/css/_schemes/Mist/index.styl","hash":"89bf3f6b82cb0fafbbd483431df8f450857c5a0b","modified":1654394161485},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl","hash":"e2da25ff86d2be5ff0a0cee33c7d4c5e11046736","modified":1654394161463},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_layout.styl","hash":"018b6a761e197086174c9f06b4d5ea21cc230951","modified":1654394161487},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl_backup","hash":"1239f1b432a6932b2bb9ebcfbaabf724b8f4e59a","modified":1654394161464},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1654394161490},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_header.styl","hash":"9b2cba0c9aa5a64957294f7548c199db1f63f0f4","modified":1654394161491},{"_id":"themes/next_8.8/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1654394161490},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_menu.styl","hash":"28030c61288cc0e1321b18373a5c79029fd76a53","modified":1654394161487},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sidebar.styl","hash":"134272cb8096156c9e32fbbe085394633c7509cd","modified":1654394161489},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5c3dd08c520a16ee49f85fa12b4935e725ef261","modified":1654394161494},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/index.styl","hash":"7905f428b46d100ac5928875cb1e2b99fa86fc0b","modified":1654394161495},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_layout.styl","hash":"9f60d501808f67d151af437221d0dfacc27c180c","modified":1654394161492},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","hash":"f9579a02599de063ccff336177ba964a2931a6e9","modified":1654394161521},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","hash":"d77d4934d959e7125128754b568f1d041c3fbfff","modified":1654394161522},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","hash":"72e0766752b78a723fb30e92d533a8b353104e2d","modified":1654394161523},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","hash":"14b024c920a8b359777d79dd8e1a849387f8f3ad","modified":1654394161524},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","hash":"f755e8537ccbbb0bd84c26923f320d4e206e7428","modified":1654394161523},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","hash":"77c231bcd64f1c09bd9989909e9fee703b65f47f","modified":1654394161525},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_menu.styl","hash":"1d29eca70fa686d895f8e98a283e4a159e40905a","modified":1654394161492},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_menu.styl","hash":"f337981f8f20944ed366694aea88146c7b0a13ab","modified":1654394161483},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","hash":"b1dd519dc3b1153c9d2ba2d35f68ca8f73f33bae","modified":1654394161525},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","hash":"1c282d6c2151346d1f0aa95055d17abe77054ec9","modified":1654394161527},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sidebar.styl","hash":"42bf453def88da82c842dca84e8f47087091f08e","modified":1654394161494},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","hash":"b9b9fd2f0e098a123b34a4932da912a9485ffe6c","modified":1654394161528},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","hash":"5460de247c038d6cfbe774d7f8747f0a958d9017","modified":1654394161526},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","hash":"68892d74ef5fc308c6e7e6b4f190826d79f3055d","modified":1654394161528},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","hash":"ec44d7f1c8b51b0aa3cccba099a78f3575ac828c","modified":1654394161529},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","hash":"1e8509356fb027d948d118ab220d9631f4d482fa","modified":1654394161527},{"_id":"themes/next_8.8/source/css/_common/scaffolding/pagination.styl","hash":"34416a5792d0235caa8c0c7e59725f2df0fa614c","modified":1654394161470},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","hash":"5c63ec71458b4fe0cd98fd4a04e11c3746764f11","modified":1654394161531},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","hash":"d93556184b2c0aa1dbc4a6fb892d2f77b80d7d9f","modified":1654394161532},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","hash":"dc2b0e89aa32afc7f7a7e2d7a277dadb7f96e06d","modified":1654394161536},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","hash":"ea94731438d8c518d946601f8f46a65b92381fac","modified":1654394161536},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","hash":"e109c2d6828f527f0289d5fa3bb02fce63ee6d93","modified":1654394161540},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","hash":"6abdc209f4503d4efd676e18bc30ddea813b6ff9","modified":1654394161538},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","hash":"d0829fe41d2fe86b8499e2a896556c1275ea0066","modified":1654394161537},{"_id":"themes/next_8.8/source/css/_common/components/pages/breadcrumb.styl","hash":"fde10ce94e9ae21a03b60d41d532835b54abdcb1","modified":1654394161429},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","hash":"2618135cbcee6bf228f6734767de1995e5eaaac6","modified":1654394161539},{"_id":"themes/next_8.8/source/css/_common/components/pages/index.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1654394161431},{"_id":"themes/next_8.8/source/css/_common/components/pages/tag-cloud.styl","hash":"56d719bcdcba3d725141c55bbd4b168f3942f912","modified":1654394161433},{"_id":"themes/next_8.8/source/css/_common/components/pages/schedule.styl","hash":"091b8c763e43447d087c122a86538f290f83136a","modified":1654394161432},{"_id":"themes/next_8.8/source/css/_common/components/pages/categories.styl","hash":"80595d274f593b321c0b644a06f3165fe07b16f5","modified":1654394161430},{"_id":"themes/next_8.8/source/css/_common/components/post/index.styl","hash":"df2fbd0ada00f37439b0de965c6f1c29d3c97429","modified":1654394161434},{"_id":"themes/next_8.8/source/css/_common/components/post/post-body.styl","hash":"7a34d020877273dcf11c25fa481409300efb8659","modified":1654394161435},{"_id":"themes/next_8.8/source/css/_common/components/post/post-followme.styl","hash":"791bc9befb0d4d06e3e517eccfe0bc3551a02a60","modified":1654394161437},{"_id":"themes/next_8.8/source/css/_common/components/post/post-collapse.styl","hash":"eebe3013a9a976011570dce2d04dfeae4c31d790","modified":1654394161436},{"_id":"themes/next_8.8/source/css/_common/components/post/post-gallery.styl","hash":"c34936a17c3d8af6c0988ac6746d7509dc0b50eb","modified":1654394161438},{"_id":"themes/next_8.8/source/css/_common/components/post/post-nav.styl","hash":"69dff7cf231d01f85671758455726dd666664a73","modified":1654394161439},{"_id":"themes/next_8.8/source/css/_common/components/post/post-widgets.styl","hash":"0a779f955a0e25df0852e0731517dadb234aa181","modified":1654394161440},{"_id":"themes/next_8.8/source/css/_common/components/post/post-header.styl","hash":"4d29b6ae7ed3dc44b10df851a4128b6441efa8be","modified":1654394161439},{"_id":"themes/next_8.8/source/css/_common/components/post/post-footer.styl","hash":"e53a5eb1d1771e284044bdb0bc0ed2de27923669","modified":1654394161437},{"_id":"themes/next_8.8/source/css/_common/components/post/post-reward.styl","hash":"9043d9bc2db35ca000c79258ef89fdb161dc43fb","modified":1654394161440},{"_id":"themes/next_8.8/source/css/_common/components/third-party/gitalk.styl","hash":"fb165c1a0d990c5cf98b87773e0dc50410229b96","modified":1654394161443},{"_id":"themes/next_8.8/source/css/_common/components/third-party/index.styl","hash":"25ea9a0af888355b3a046db1100b5cb0e2d6ef6e","modified":1654394161444},{"_id":"themes/next_8.8/source/css/_common/components/third-party/math.styl","hash":"1e5776ad4c5c8bcf7596ac74dcabc30704b3f5a0","modified":1654394161445},{"_id":"themes/next_8.8/source/css/_common/components/third-party/related-posts.styl","hash":"0527153aa821bdbdb84c7b47f60e3cefd95a742f","modified":1654394161446},{"_id":"themes/next_8.8/source/css/_common/outline/header/bookmark.styl","hash":"c8648c8ea3105556be0068d9fb2735261d0d94bc","modified":1654394161449},{"_id":"themes/next_8.8/source/css/_common/components/third-party/utterances.styl","hash":"d28856f365a9373c4ae6fe1e5673d63df2dfd65f","modified":1654394161447},{"_id":"themes/next_8.8/source/css/_common/components/third-party/search.styl","hash":"49c26184580fde8a732899a4de5aae8662e289b8","modified":1654394161446},{"_id":"themes/next_8.8/source/css/_common/components/third-party/disqusjs.styl","hash":"c1e9edbfd1c3696b35d5452ae2e6d766f3fe91aa","modified":1654394161442},{"_id":"themes/next_8.8/source/css/_common/outline/footer/index.styl","hash":"02b6d1a53f7a02c6b0929b11f3ab904b5b873a0e","modified":1654394161448},{"_id":"themes/next_8.8/source/css/_common/outline/header/index.styl","hash":"67fc7a1eb59c8451eec34e572cbb2fd1424757bc","modified":1654394161450},{"_id":"themes/next_8.8/source/css/_common/outline/header/github-banner.styl","hash":"05af22f3edc2383a3d97ec4c05e9ac43b014bead","modified":1654394161450},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-meta.styl","hash":"86b0925e968f35bbc76b473a861e8f9797f7580e","modified":1654394161452},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-nav.styl","hash":"d9bc2b520636b9df7f946295cd430593df4118ff","modified":1654394161454},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2c2bfbc34b6f19d262ae7c041474985e12f4f4ad","modified":1654394161458},{"_id":"themes/next_8.8/source/css/_common/outline/header/menu.styl","hash":"2db695204d39e4c7daa7b91585a0ea4b06b49f11","modified":1654394161452},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"63d8f5f169c2b1c969928fc79244c5fe89ee484e","modified":1654394161460},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"1c324d56ae83e96db2c4c6d63edd7ee51c936fc1","modified":1654394161459},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"57ed6770535ecb2e6485a0c87d4de6d6476368b9","modified":1654394161460},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/index.styl","hash":"9964a96f9a647cfb16b97679eced79d07e084e6d","modified":1654394161457},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"6681ffe283f8a7e3c86310ef4f6ca1e499c1a19f","modified":1654394161462},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"d8a028f532d562e6a86bb3b9c7b992e4b6dbbb51","modified":1654394161458},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/site-state.styl","hash":"2de038def2cb91da143b14696366c14a66e0e569","modified":1654394161462},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"a4003e1408844568cb5102a5a111046cb19b2d31","modified":1654394161467},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"db4f3263b2b6551dd56bfdf33cceaf81661a3611","modified":1654394161461},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"081345490271840855d1238b969dbf2e0a2bba8f","modified":1654394161461},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"8d9218980e185210ce034e9769ab639b9630fd88","modified":1654394161473},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/index.styl","hash":"e22fde6f1657d311d46f64d868c4491d535c8caa","modified":1654394161474},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/mermaid.styl","hash":"c7754dc6c866928b538f0863a05b96ec44b5e986","modified":1654394161476},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/label.styl","hash":"531daf2612c6217950677a2d03924459ce57c291","modified":1654394161475},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/pdf.styl","hash":"77122986509a6b4968bae2729417b7016137534c","modified":1654394161478},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/index.styl","hash":"5f706f3382652835379cf9b9fec24ccd4513ab65","modified":1654394161468},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"6b3680e0dbea8e14c1cec24ef63b7fae5e37f7ef","modified":1654394161472},{"_id":"themes/next_8.8/source/images/avatar.jpg","hash":"ed5dde31684ebfd99e9965da1140fb919496f1d3","modified":1654394161504},{"_id":"themes/next_8.8/source/images/avatar_300.jpg","hash":"13b3ca4593cd3c96d4f973b26ec9dbe51e28ed29","modified":1654394161507},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7075dd32dd70da1e161e4bd14b46f1e8be62fa3c","modified":1654394161476},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/note.styl","hash":"2e9dc3b3546e19e9de18050ad04b1741841116bc","modified":1654394161477},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/tabs.styl","hash":"40a38f2129617ffd4e8d5cd78e982fdfc9941acf","modified":1654394161478},{"_id":"themes/next_8.8/source/images/avatar_500.jpg","hash":"fa083b4b1260c2ee43810fc4819e958196ff7d49","modified":1654394161509},{"_id":"source/_posts/GNN/GraphMatrix.jpg","hash":"4b0b74c378c512b604a7289f838d049f372f248a","modified":1654394131541},{"_id":"public/sitemap.xml","hash":"9ead3f949eae09c6d6adb720e7216ef60ecd8774","modified":1659495662189},{"_id":"public/about/index.html","hash":"c387ffa2b65743891223e2afa3cf1370e9e8ac85","modified":1656571633417},{"_id":"public/categories/index.html","hash":"c08f942bf64229e949b53551fd11a4cf105eb5fa","modified":1656571633417},{"_id":"public/tags/index.html","hash":"83e90cdc4b228ca4f48efb3c7c8ce2349eaa2503","modified":1656571633417},{"_id":"public/2022/05/11/English-Writing/index.html","hash":"7f6e5797c18346329af049bb95bd21b51612cbce","modified":1656571633417},{"_id":"public/2022/05/11/Monocular-Vision/index.html","hash":"8f53be97c0c5c83e1cdc1a9edb16016af5c6990f","modified":1656571633417},{"_id":"public/2022/04/12/GNN/index.html","hash":"95d34500ae3776aaea04bc5088a25bad78c9ce43","modified":1656571633417},{"_id":"public/archives/index.html","hash":"faa8eb69f9e6c52313a1585a54ad427b2177fdbe","modified":1656571633417},{"_id":"public/archives/2022/index.html","hash":"c469c11a3a32d5d18831fefb4d26ca54d7985916","modified":1656571633417},{"_id":"public/archives/2022/01/index.html","hash":"eca3774097a87c0da77ee5d1fa1e7601d099f0c6","modified":1656571633417},{"_id":"public/archives/2022/04/index.html","hash":"e883b17214c829fa03ad724ae53edec0a3ae6e21","modified":1656571633417},{"_id":"public/archives/2022/05/index.html","hash":"8026a2967db8bb97f28fd84f3a49c7fc6ad60ad7","modified":1656571633417},{"_id":"public/categories/GNN/index.html","hash":"07f1d8bf1075d194bffa2cbbe2a3fe8e96882c64","modified":1656571633417},{"_id":"public/categories/English-Paper-Writing/index.html","hash":"0d75f718bd85d1288cd54e94d490711ad115ef9b","modified":1656571633417},{"_id":"public/categories/Monocular-Vision/index.html","hash":"826cbc5cbc19f03b723cf6dcdeb098e50da355d9","modified":1656571633417},{"_id":"public/categories/Papers/index.html","hash":"3107037118fda0010a002c5ba4c293a5761d075b","modified":1656571633417},{"_id":"public/tags/GNN/index.html","hash":"069f2903d0ca8ba2bcafd24fc5fb12bdcfb44a29","modified":1656571633417},{"_id":"public/tags/English-Paper-Writing/index.html","hash":"4650fe78b8e34cd6219f3ae346c11d6e8506df85","modified":1656571633417},{"_id":"public/tags/Monocular-Vision/index.html","hash":"b2eb63ba02ff8263277be128cfec58e238bbbcbc","modified":1656571633417},{"_id":"public/2022/01/05/Paper-Reading/index.html","hash":"984785bb128c460964c64878e08cc85652dcaca7","modified":1656571633417},{"_id":"public/index.html","hash":"0924ae74a3bfd361cdd4d7149d625f1e974fe01d","modified":1659495662189},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1654394281331},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1654394281331},{"_id":"public/images/avatar_100.jpg","hash":"01b495adc2782bb5b371c693c6d8c33a0caacb53","modified":1654394281331},{"_id":"public/images/avatar_200.jpg","hash":"4a76d1527e511350c9112e899046a34be59ad278","modified":1654394281331},{"_id":"public/images/avatar_80.jpg","hash":"465e53598964df3852f08794cb73c5e459855e92","modified":1654394281331},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1654394281331},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1654394281331},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1654394281331},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1654394281331},{"_id":"public/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1654394281331},{"_id":"public/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1654394281331},{"_id":"public/css/hbe.style.css","hash":"f80457bac5f4bc67ae41733ee1ffb65d7b6ba1e8","modified":1654394281331},{"_id":"public/lib/hbe.js","hash":"2a6e18b7c37578f582a0acdc1ebc54d1e846b1a7","modified":1654394281331},{"_id":"public/images/avatar.jpg","hash":"ed5dde31684ebfd99e9965da1140fb919496f1d3","modified":1654394281331},{"_id":"public/images/avatar_300.jpg","hash":"13b3ca4593cd3c96d4f973b26ec9dbe51e28ed29","modified":1654394281331},{"_id":"public/css/noscript.css","hash":"54d14cd43dc297950a4a8d39ec9644dd5fc3499f","modified":1654394281331},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1654394281331},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1654394281331},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1654394281331},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1654394281331},{"_id":"public/js/motion.js","hash":"9c4c861dfb080b6244d4d9eba33ac686735754f3","modified":1654394281331},{"_id":"public/js/next-boot.js","hash":"48497e2156a10155dc42311633a110c9685692c9","modified":1654394281331},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1654394281331},{"_id":"public/js/schedule.js","hash":"2b43e2d576a308289880befc27580dbb2aa34439","modified":1654394281331},{"_id":"public/js/utils.js","hash":"e447160d342b1f93df5214b6a733441039ced439","modified":1654394281331},{"_id":"public/js/schemes/muse.js","hash":"9a836d2bcc3defe4bd1ee51f5f4eb7006ebdd41b","modified":1654394281331},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1654394281331},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1654394281331},{"_id":"public/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":1654394281331},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1654394281331},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1654394281331},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1654394281331},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1654394281331},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1654394281331},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1654394281331},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1654394281331},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1654394281331},{"_id":"public/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":1654394281331},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1654394281331},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1654394281331},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1654394281331},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1654394281331},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1654394281331},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1654394281331},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1654394281331},{"_id":"public/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":1654394281331},{"_id":"public/js/third-party/search/local-search.js","hash":"45c485f82258d246f37deb66884bd2643323ef3a","modified":1654394281331},{"_id":"public/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":1654394281331},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1654394281331},{"_id":"public/js/third-party/tags/mermaid.js","hash":"3dc4628efa6debd6490fc0ebddff2424a7b319d8","modified":1654394281331},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1654394281331},{"_id":"public/css/main.css","hash":"2b9e134ea77c81001b2471bf42117e5cacdd104c","modified":1654394281331},{"_id":"public/images/avatar_500.jpg","hash":"fa083b4b1260c2ee43810fc4819e958196ff7d49","modified":1654394281331},{"_id":"public/2022/04/12/GNN/GraphMatrix.jpg","hash":"4b0b74c378c512b604a7289f838d049f372f248a","modified":1654394281331},{"_id":"source/_posts/CNN-Regularization.md","hash":"b5de34fddf30936ef3a7e07989933e4b0816b250","modified":1659690867227},{"_id":"source/_posts/CNN-Regularization/1.png","hash":"6c903496ef5c611fab20e53123f9a846ff70690a","modified":1655261223155},{"_id":"source/_posts/CNN-Regularization/2.png","hash":"e528298bc7b7789e0814254d1f3240080fd2c18b","modified":1655261231147},{"_id":"source/_posts/CNN-Regularization/3.png","hash":"1daa34dfe04e610c41f3fa08254dc9aca2b4b520","modified":1655261215210},{"_id":"source/_posts/CNN-Regularization/1.jpg.png","hash":"6c903496ef5c611fab20e53123f9a846ff70690a","modified":1655261223155},{"_id":"source/_posts/CNN-Regularization/2.jpg.png","hash":"e528298bc7b7789e0814254d1f3240080fd2c18b","modified":1655261231147},{"_id":"source/_posts/CNN-Regularization/3.jpg.png","hash":"1daa34dfe04e610c41f3fa08254dc9aca2b4b520","modified":1655261215210},{"_id":"source/_posts/CNN-Regularization/4.png","hash":"765e8c267e66222a81cc348e69bd245889bd4444","modified":1655273225868},{"_id":"source/_posts/CNN-Regularization/D2.png","hash":"f9bd73382fc5c3a192e4b0ac8408c7774608f356","modified":1655264617536},{"_id":"source/_posts/CNN-Regularization/t1.png","hash":"ae6b405f297ba53760a88da4bd9c36bfaea2bf82","modified":1655277374542},{"_id":"source/_posts/CNN-Regularization/t2.png","hash":"11875b8dd065491e28d9ce4a5d306d09127dc109","modified":1655277768203},{"_id":"source/_posts/CNN-Regularization/t3.png","hash":"e90eb774149b95294292d14354ce304d114f7ac3","modified":1655282681176},{"_id":"source/_posts/CNN-Regularization/t3_1.png","hash":"5ef99fb033cde1d8ed3b3e6729ca13d57f7dd131","modified":1655282873985},{"_id":"source/_posts/CNN-Regularization/t3_2.png","hash":"81799f6864888ebdb0d39cdd6d7a7f5a3dc7cb08","modified":1655282952370},{"_id":"source/_posts/CNN-Regularization/5.png","hash":"676ebe9cb36af4c18b9ca83e922cecb70c6d858f","modified":1655305219383},{"_id":"source/_posts/CNN-Regularization/e1.png","hash":"1d543ebfa1ba5e1245359bf9743fdddc6d6f052e","modified":1655305877814},{"_id":"source/_posts/CNN-Regularization/t4_3.png","hash":"4bb230850242f4b8964d4b4d04f431ad47dcdc9d","modified":1655306237936},{"_id":"source/_posts/CNN-Regularization/t4_1.png","hash":"be55b791d96d029200e783d22302b6987a2dfee4","modified":1655306137769},{"_id":"source/_posts/CNN-Regularization/t4_2.png","hash":"6c86f244241fc87003c230fa25831ed8b6474007","modified":1655306206331},{"_id":"source/_posts/CNN-Regularization/7.png","hash":"5fb117d45a251e532056039a1fab2e41d2ed0d9a","modified":1655307219428},{"_id":"source/_posts/Poem.md","hash":"cadeaa15379a3dcc21233640ae0a54c19c248f4c","modified":1657678054508},{"_id":"source/_posts/CNN-Regularization/h1.png","hash":"9d540e8635e2c9d5fcf1f8008ddc2543b5031594","modified":1655880859105},{"_id":"source/_posts/CNN-Regularization/dropout_f7.png","hash":"0bebfd6b86649af9300a2b1e4408bac3b56cb385","modified":1655885612376},{"_id":"source/_posts/CNN-Regularization/heuristic_a1.png","hash":"6e83a28e36e2e24a1a02a3b4b82aca811dc61ff4","modified":1655886825512},{"_id":"source/_posts/CNN-Regularization/heuristic_a2.png","hash":"447c256bf43f6352ca09e9291de96f67fa150fe1","modified":1655887800340},{"_id":"source/_posts/CNN-Regularization/heuristic_e1.png","hash":"1edeb3cfe8ccf50e175a2f6218c1b82c5e7d5d04","modified":1655957315350},{"_id":"source/_posts/CNN-Regularization/heuristic_t1.png","hash":"37bd4af7a245a338b938c5748be0be57134f641a","modified":1655958619595},{"_id":"source/_posts/CNN-Regularization/heuristic_f1.png","hash":"8ad096b2a06b5e6903e7f21a476a9375925cc339","modified":1655958575853},{"_id":"source/_posts/CNN-Regularization/heuristic_f3.png","hash":"50e3395b23cb5723f16d40c931f458ed0e3aed7a","modified":1655959840170},{"_id":"source/_posts/CNN-Regularization/heuristic_f4.png","hash":"93362193440a1ca75cae800c516d702dd7f4311d","modified":1655960698870},{"_id":"source/_posts/CNN-Regularization/heuristic_f2.png","hash":"dffdf51cc407b366272d9d32fb81e95220dc4570","modified":1655959882756},{"_id":"public/2022/06/18/Poem/index.html","hash":"e489a02b0e062c7d2684e72f270b62402887dcec","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/index.html","hash":"0b7bbe05c4489d2c559b1ee3fdf52339cd99243c","modified":1659495662189},{"_id":"public/archives/2022/06/index.html","hash":"44887abb073159ee74341688a8ea0b4505c27c5a","modified":1656571633417},{"_id":"public/categories/CNN-Regularization/index.html","hash":"68cb02a5b44857c4f418ac7d3ca44c9a71cbb236","modified":1656571633417},{"_id":"public/categories/Poem/index.html","hash":"8e74119f9323f448732d7a2cce981ed3bd279f11","modified":1656571633417},{"_id":"public/tags/CNN-Regularization/index.html","hash":"295f695cd0699fe8d520e7ab7c889455d0b7b6f4","modified":1656571633417},{"_id":"public/tags/Poem/index.html","hash":"9fd99b6d80e034d81a11c225fd15c001c91bb838","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/1.png","hash":"6c903496ef5c611fab20e53123f9a846ff70690a","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/2.png","hash":"e528298bc7b7789e0814254d1f3240080fd2c18b","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/3.png","hash":"1daa34dfe04e610c41f3fa08254dc9aca2b4b520","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/D2.png","hash":"f9bd73382fc5c3a192e4b0ac8408c7774608f356","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/5.png","hash":"676ebe9cb36af4c18b9ca83e922cecb70c6d858f","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/4.png","hash":"765e8c267e66222a81cc348e69bd245889bd4444","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/h1.png","hash":"9d540e8635e2c9d5fcf1f8008ddc2543b5031594","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t4_3.png","hash":"4bb230850242f4b8964d4b4d04f431ad47dcdc9d","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_a2.png","hash":"447c256bf43f6352ca09e9291de96f67fa150fe1","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_t1.png","hash":"37bd4af7a245a338b938c5748be0be57134f641a","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_f1.png","hash":"8ad096b2a06b5e6903e7f21a476a9375925cc339","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/7.png","hash":"5fb117d45a251e532056039a1fab2e41d2ed0d9a","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_e1.png","hash":"1edeb3cfe8ccf50e175a2f6218c1b82c5e7d5d04","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/e1.png","hash":"1d543ebfa1ba5e1245359bf9743fdddc6d6f052e","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_f3.png","hash":"50e3395b23cb5723f16d40c931f458ed0e3aed7a","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t1.png","hash":"ae6b405f297ba53760a88da4bd9c36bfaea2bf82","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_a1.png","hash":"6e83a28e36e2e24a1a02a3b4b82aca811dc61ff4","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_f4.png","hash":"93362193440a1ca75cae800c516d702dd7f4311d","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t2.png","hash":"11875b8dd065491e28d9ce4a5d306d09127dc109","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t4_1.png","hash":"be55b791d96d029200e783d22302b6987a2dfee4","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t3_1.png","hash":"5ef99fb033cde1d8ed3b3e6729ca13d57f7dd131","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t4_2.png","hash":"6c86f244241fc87003c230fa25831ed8b6474007","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/t3_2.png","hash":"81799f6864888ebdb0d39cdd6d7a7f5a3dc7cb08","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/heuristic_f2.png","hash":"dffdf51cc407b366272d9d32fb81e95220dc4570","modified":1656571633417},{"_id":"public/2022/06/10/CNN-Regularization/dropout_f7.png","hash":"0bebfd6b86649af9300a2b1e4408bac3b56cb385","modified":1656571633417},{"_id":"source/_posts/CNN-Regularization/NSDropout_f1.png","hash":"a7cd56d25cfdd7ddacb12a75f99d7ce14c443478","modified":1656641966753},{"_id":"source/_posts/CNN-Regularization/NSDropout_e1.png","hash":"2692c1b83114fff3a66c17c0aee8ec9e396c6d8d","modified":1656643806012},{"_id":"source/_posts/CNN-Regularization/NSDropout_f2.png","hash":"99da90208be00c3b71446299f6f17f49e2b36b20","modified":1656646392646},{"_id":"source/_posts/CNN-Regularization/StructuralDropout_f1.png","hash":"56f7036f3cef2348fe6f5c7b391e4b7bea35683e","modified":1656922493465},{"_id":"public/2022/06/10/CNN-Regularization/NSDropout_e1.png","hash":"2692c1b83114fff3a66c17c0aee8ec9e396c6d8d","modified":1657288324087},{"_id":"public/2022/06/10/CNN-Regularization/StructuralDropout_f1.png","hash":"56f7036f3cef2348fe6f5c7b391e4b7bea35683e","modified":1657288324087},{"_id":"public/2022/06/10/CNN-Regularization/NSDropout_f1.png","hash":"a7cd56d25cfdd7ddacb12a75f99d7ce14c443478","modified":1657288324087},{"_id":"public/2022/06/10/CNN-Regularization/NSDropout_f2.png","hash":"99da90208be00c3b71446299f6f17f49e2b36b20","modified":1657288324087},{"_id":"source/_posts/CNN-Regularization/Clustering-basedAdaptiveDropout.png","hash":"ff6cdefbb530d37568b0d1c2057591dda26380e2","modified":1657352383550},{"_id":"source/_posts/CNN-Regularization/Clustering_basedAdaptiveDropout.png","hash":"ff6cdefbb530d37568b0d1c2057591dda26380e2","modified":1657352383550},{"_id":"source/_posts/CNN-Regularization/Clustering-basedAdaptiveDropout_f1.png","hash":"ff6cdefbb530d37568b0d1c2057591dda26380e2","modified":1657352383550},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f1.png","hash":"3d6c1006071f3087e734eae8ccd63430f918409b","modified":1657523410504},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e1.png","hash":"7f8051596443444423d7c145aa980413ae579e10","modified":1657527935105},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e4.png","hash":"a409663ba94bf472789129cb8157c1f027ef73a2","modified":1657528934463},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e5.png","hash":"5e24c48efba6f582a13e8ba38cf6f34356ef51a1","modified":1657529484020},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e6.png","hash":"07218837eadec3d88ad795eabbc186aac25fee80","modified":1657529957242},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f2.png","hash":"223d1377bf9bd4494d9d8767adcf50bbeda495b2","modified":1658325177268},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f3.png","hash":"2130e4a40f664a25d09cbc4c105a57b6fa07e43f","modified":1657530364497},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e10.png","hash":"a33e44aaf485aec3dec1abb43cdeec483351c9aa","modified":1657542029295},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e11.png","hash":"65dc9cbfc857fc4881100219c86462060c03f28e","modified":1657542229332},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e14.png","hash":"6df82bb53bd1660efe7a1a9e5ff6d7ad726fe90b","modified":1657544985646},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e13.png","hash":"29bc0b8325879aef1d6ed30694f77fcfe1cd7c23","modified":1657544804828},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e9.png","hash":"15808514a96e00da24100761a4c34c824e5e4beb","modified":1657537325584},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e12.png","hash":"54d8cbb2be2193570c1e54f2e03ed91d2c1c73f9","modified":1657543335228},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e1.png","hash":"7f8051596443444423d7c145aa980413ae579e10","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e5.png","hash":"5e24c48efba6f582a13e8ba38cf6f34356ef51a1","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e6.png","hash":"07218837eadec3d88ad795eabbc186aac25fee80","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e4.png","hash":"a409663ba94bf472789129cb8157c1f027ef73a2","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e10.png","hash":"a33e44aaf485aec3dec1abb43cdeec483351c9aa","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e12.png","hash":"54d8cbb2be2193570c1e54f2e03ed91d2c1c73f9","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e14.png","hash":"6df82bb53bd1660efe7a1a9e5ff6d7ad726fe90b","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e9.png","hash":"15808514a96e00da24100761a4c34c824e5e4beb","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e11.png","hash":"65dc9cbfc857fc4881100219c86462060c03f28e","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_e13.png","hash":"29bc0b8325879aef1d6ed30694f77fcfe1cd7c23","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_f2.png","hash":"223d1377bf9bd4494d9d8767adcf50bbeda495b2","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_f3.png","hash":"2130e4a40f664a25d09cbc4c105a57b6fa07e43f","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/Clustering-basedAdaptiveDropout_f1.png","hash":"ff6cdefbb530d37568b0d1c2057591dda26380e2","modified":1658646841850},{"_id":"public/2022/06/10/CNN-Regularization/CorrDrop_f1.png","hash":"3d6c1006071f3087e734eae8ccd63430f918409b","modified":1658646841850},{"_id":"source/_posts/CNN-Regularization/CDB_f1.png","hash":"cbb6b6b2f2502cb40d8ada635dbb88f2004f2970","modified":1658650646619},{"_id":"source/_posts/CNN-Regularization/CDB_a1.png","hash":"3e1f8c37c45020e64d71a57aac41cf6d6495e860","modified":1658651835693},{"_id":"public/2022/06/10/CNN-Regularization/CDB_f1.png","hash":"cbb6b6b2f2502cb40d8ada635dbb88f2004f2970","modified":1659495662189},{"_id":"public/2022/06/10/CNN-Regularization/CDB_a1.png","hash":"3e1f8c37c45020e64d71a57aac41cf6d6495e860","modified":1659495662189},{"_id":"source/_posts/CNN-Regularization/CDB_f2.png","hash":"9a3daaf1d7916aa81596e6dbe24e76e6fa50686b","modified":1659668435257},{"_id":"source/_posts/CNN-Regularization/CDB_e1.png","hash":"a9d7d0579161d68e83431ed024125756b771bc60","modified":1659688656987},{"_id":"source/_posts/CNN-Regularization/CDB_e2.png","hash":"501d44030b0ee95b29ed024415595ce456d6ff3b","modified":1659688678687},{"_id":"source/_posts/CNN-Regularization/CDB_e3.png","hash":"38aa84ae2b535da63ac18084b3f30def5d219c99","modified":1659690273837}],"Category":[{"name":"GNN","_id":"cl40nn32t0003mculfa5jbacm"},{"name":"English Paper Writing","_id":"cl40nn332000amculfluz2pqy"},{"name":"Monocular Vision","_id":"cl40nn334000dmcula3e9ezux"},{"name":"Papers","_id":"cl40nn335000hmcul5n3ncmf5"},{"name":"CNN Regularization","_id":"cl49vqqs300011ould6nyhwjv"},{"name":"Poem","_id":"cl4m00ue60001zoulfpu80wcl"}],"Data":[],"Page":[{"title":"about","date":"2021-12-12T13:52:49.000Z","_content":"\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n> ​     *There is a pleasure in the pathless woods;*\n> ​     *there is a rapture on the lonely shore;*\n> ​     *there is society, where none intrudes,*\n> ​     *by the deep sea, and music in its roar;*\n> ​     *I love not man the less, but nature more...*\n> ​                          *by George Gordon Byron* \n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-12-12 21:52:49\n---\n\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n> ​     *There is a pleasure in the pathless woods;*\n> ​     *there is a rapture on the lonely shore;*\n> ​     *there is society, where none intrudes,*\n> ​     *by the deep sea, and music in its roar;*\n> ​     *I love not man the less, but nature more...*\n> ​                          *by George Gordon Byron* \n\n","updated":"2022-06-05T01:55:31.545Z","path":"about/index.html","comments":1,"layout":"page","_id":"cl40nn31t0000mcul1b2vgc85","content":"<html><head></head><body><p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br>\n<br>\n<hr>\n<br>\n<br>\n<blockquote>\n<p>​     <em>There is a pleasure in the pathless woods;</em>\n​     <em>there is a rapture on the lonely shore;</em>\n​     <em>there is society, where none intrudes,</em>\n​     <em>by the deep sea, and music in its roar;</em>\n​     <em>I love not man the less, but nature more…</em>\n​                          <em>by George Gordon Byron</em></p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br/>\n<br/>\n<hr />\n<br/>\n<br/>\n<blockquote>\n<p>​     <em>There is a pleasure in the pathless woods;</em>\n​     <em>there is a rapture on the lonely shore;</em>\n​     <em>there is society, where none intrudes,</em>\n​     <em>by the deep sea, and music in its roar;</em>\n​     <em>I love not man the less, but nature more…</em>\n​                          <em>by George Gordon Byron</em></p>\n</blockquote>\n"},{"title":"categories","date":"2021-11-24T11:46:36.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-11-24 19:46:36\ntype: \"categories\"\n---\n","updated":"2022-06-05T01:55:31.546Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl40nn32r0002mculaqmi2amg","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""},{"title":"tags","date":"2021-12-08T06:48:06.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-12-08 14:48:06\ntype: \"tags\"\n---\n","updated":"2022-06-05T01:55:31.547Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cl40nn32w0005mculbstpevf8","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""}],"Post":[{"title":"GNN","top":false,"cover":false,"toc":true,"mathjax":false,"date":"2022-04-12T01:00:41.000Z","password":null,"summary":null,"description":null,"_content":"\n# 从 CNN 到 GNN\n\n现实生活中，很多 graph（图数据）是从非欧氏空间中生成的，graph 不再只是从欧氏空间中生成的规则的栅格结构。\n\n> CNN --> 用来提取欧氏空间数据的特征，它针对规则的 2D 栅格结构的 image（传统数据，具有规则的空间结构），其像素点的排列顺序有明显的上下左右的位置关系。\n>\n> CNN 中卷积核的形状是固定的，其卷积操作具有序列有序性和维数不变性的限制。\n\n> GCN --> 针对不规则的 graph（图数据，非矩阵结构数据）， graph 的节点之间无空间上的位置关系，中心节点的邻域节点是不确定的，且邻域节点没有顺序。因此传统的卷积核就无法适应图卷积。\n\n## graph 的定义\n\n> 在数学中，图是由顶点（Vertex）和连接顶点的边（Edge）构成的。顶点表示研究的对象，边表示两个对象之间的特定关系。\n>\n> 图表示顶点和边的集合，记为 $G = (V, E)$ ，其中，$V$是顶点集合，$E$ 是边集合。        设图 $G$ 的顶点数为 $N$，边数为 $M$。                                                                                           一条连接顶点 $v_i, v_j \\in V$ 的边记为$(v_i, v_j)$或者$e_{ij}$ 。\n\n> <font color =  green>**邻居和度**</font>                                                                                                                                 如果存在一条边连接顶点 $v_i$ 和 $v_j$ ，则称 $v_j$ 是 $v_i$ 的邻居，反之亦然。                                                                              $v_i$ 的所有邻居为集合 $N(v_i)$，即: \n>\n> $$N(v_i) = \\{v_j | \\exists e_{ij} \\in E  \\ or \\ e_{ji} \\in E\\}$$\n>\n> 以 $v_i$为端点的边的数目称为 $v_i$ 的度（Degree），记为 $deg(v_i)$,  $deg(v_i) = | N(v_i) |$.\n\n> <font color =  green>**邻接矩阵 & 关联矩阵 & 度矩阵 & 拉普拉斯矩阵**</font>\n>\n> 图 $G = (V, E)$， <font color =  green>**邻接矩阵**</font> *A* 描述图中顶点之间的关系，$A \\in R^{N \\times N}$,其定义为：\n>\n> $$ A_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if \\  (v_i, v_j) \\subseteq E}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> <font color =  green>**关联矩阵**</font> B 描述节点与边之间的关联，$B \\in R^{N \\times M}$，其定义为：\n>\n> $$ B_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if \\  v_i 与 e_j 相连}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> <font color =  green>**度矩阵**</font> D 是一个对角矩阵，$D \\in R^{N \\times N}$，其定义为：\n>\n> $$ D_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> deg(v_i)     &        & {if  \\  i = j}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> 示例：\n>\n> <!-- ![avatar](GraphMatrix.jpg ) -->\n>\n> <img src=GraphMatrix.jpg width=70% />\n>\n> <font color =  green>**拉普拉斯矩阵**</font> L 用来表示 graph 的结构信息，$L = D - A \\in R^{N \\times N}$，定义为：\n>\n> $$ L_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> deg(v_i)     &        & {if  \\  i = j}\\\\\n>\n> -1       &         &{if   \\   e_{ij} \\in  E}\\\\\n>\n> 0     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n>\n> --> 由此定义可知：<font color =  green>**拉普拉斯矩阵是一个实对称矩阵！！！**</font>\n>\n> --> <font color =  red>**拉普拉斯矩阵用来表示 graph 的结构信息，目前GCN 都是将结构信息和属性信息剥离开来，能否找到一种方式融合结构信息和属性信息来表示 graph 呢？？？**</font>\n>\n> <font color =  green>**归一化拉普拉斯矩阵**</font> $L^{sym}$，对拉普拉斯矩阵的归一化操作，是通过对拉普拉斯矩阵两边乘以节点的度开方然后取逆得到，即：$L^{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = D^{-\\frac{1}{2}}(D-A))D^{-\\frac{1}{2}} = I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$，其中每一个节点对(i,j)定义为：\n>\n> $$ L^{sym}_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if  \\  i = j 且 deg(v_i) \\not= 0}\\\\\n>\n> -\\frac{1}{deg(v_i)deg(v_j)}&   &{if   \\   i \\not= j 且 v_i 与 v_j 相邻}\\\\\n>\n> 0     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n\n\n\n\n\n\n\n\n\n\n\n# GNN\n\n## GCN\n\n> GCN：将卷积运算从传统 image 推广到 graph 图数据。核心思想：学习一个函数映射 $f(.)$，通过映射图中的节点 $v_{i}$ 可聚合自身特征 $x_{i}$ 与它的邻居特征 $x_{j} (j \\in N(v_{i}))$ 来生成节点 $v_{i}$ 的新表示。\n>\n> GCN --> 在非欧几里得结构数据中做卷积。\n\n> <font color =  green>**&图卷积神经网络与图傅里叶变换和拉普拉斯矩阵之间有什么关系？**</font> \n>\n> 传统卷积：\n>\n> $$(f * g)(t) = \\int_{R}f(x) \\cdot g(t-x)dx$$\n>\n> --> 传统的卷积是针对具有规则空间结构，其像素点的排列顺序具有明确的上下左右位置关系的 image 的.\n>\n> --> 针对空间结构不规则，其节点之间没有空间上的位置关系的 graph，如何实现卷积操作呢？\n>\n> --> 根据时域卷积定理：时域内的卷积对应频域内的乘积。\n>\n> --> 那么<font color =  green>** 只需定义 graph 上的傅里叶变换，即可定义 graph 上的卷积**</font>，即：\n>\n> $$ f * g = \\mathcal{F}^{-1}\\{ \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\} $$\n>\n> --> 图傅里叶变换是离散的，类似于离散傅里叶变换\n>\n> --> 离散傅里叶正变换：\n>\n> $$F(w) = \\sum_{t=1}^{n}f(t) \\cdot e^{-i \\frac{2\\pi}{n}wt}$$\n>\n> n：傅里叶变换的点数；w：傅里叶变换的第 k 个频谱。\n>\n> --> <font color =  green>** 离散正变换的本质是：求线性组合的系数**</font>，由原函数和基函数的共轭的内积求得。\n>\n> --> 离散傅里叶反变换：\n>\n> $$f(t) = \\frac{1}{n}\\sum_{w=1}^{n}F(w) \\cdot e^{i \\frac{2\\pi}{n}wt}$$\n>\n> F(w)：分量的振幅；w：频率；$e^{i \\frac{2\\pi}{n}wt}$：基\n>\n> --> <font color =  green>** 离散傅里叶反变换的本质：把任意一个函数表示成了若干个正交基函数的线性组合**</font>\n>\n> --> 对应 graph 上的信号 $x = [x_1, x_2, \\cdots, x_n]^{T} \\in R^n$，如果要进行傅里叶变换，那么：\n>\n> <font color =  green>** 我们需要找到一组正交基，通过这组正交基的线性组合来表达$x \\in R^n$**</font>\n>\n> --> <font color =  green>** 图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基**</font>\n>\n> --> <font color =  blue>** 为什么拉普拉斯矩阵的特征向量能作为图傅里叶变换的正交基？**</font>\n>\n> 假设图的拉普拉斯矩阵为 $L \\in R^{N \\times N}$，$L = D - A$ 是一个实对称矩阵，因为<font color =  green>** 实对称矩阵都可以被正交对角化**</font>，所以对 L 进行特征分解（谱分解），将拉普拉斯矩阵分解为由其特征值和特征向量表示的矩阵之积，即：$L = V \\Lambda V^{T}$，$V = [v_1, v_2, \\cdots, v_N]$ 表示 L 的 N 个特征向量， $\\Lambda = diag[\\lambda_1, \\lambda_2, \\cdots, \\lambda_N]$ 表示 L 的 N 个特征值，而 $\\lambda_k$ 则表示第 $k$ 个特征向量对应的特征值。\n>\n> <font color =  green>** 对称矩阵性质： n 阶对称矩阵一定有 n 个线性无关的特征向量**</font>\n>\n> <font color =  green>** 矩阵论：n 维线性空间中的 n 个线性无关的向量都可以构成线性空间中的一组基**</font>，基可表示 n 维线性空间中的所有向量。\n>\n> --> <font color =  green>** 拉普拉斯矩阵的 n 个特征向量是线性无关的，它们是 n 维空间中的一组基**</font>\n>\n> <font color =  green>** 对称矩阵性质：对称矩阵的不同特征值对应的特征向量相互正交，这些正交的特征向量构成的矩阵为正交矩阵**</font>\n>\n> --> $VV^{T} = I$, $L = V \\Lambda V^{T} = V \\Lambda V^{-1}$，$V^{T} = V^{-1}$  <-- 正交矩阵\n>\n> --><font color =  green>** 拉普拉斯矩阵特征分解后的特征向量不但是 n 维空间中的一组基，而且还是正交的（相乘为0）**</font>\n>\n> 即 <font color =  blue>** 拉普拉斯矩阵的 n 个特征向量是 n 维空间中的一组标准正交基**</font>\n>\n> 因此，拉普拉斯矩阵的特征向量可以作为图傅里叶变换的正交基，满足充分条件。<font color =  red>**那么，正交基那么多，为什么 GCN 采用拉普拉斯矩阵的特征向量呢？也就是采用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基的必要性体现在哪里呢？**</font>\n>\n> > <font color =  red>**暂时还没能彻底搞清楚，下面是我认为的原因：**</font>\n> >\n> > 拉普拉斯矩阵的定义来源于拉普拉斯算子。拉普拉斯算子是 n 维欧氏空间中的一个二阶微分算子，定义为梯度的散度，即：$\\triangle f = \\sum_{i=1}^{n} \\frac{\\sigma^2f}{\\sigma{x_i}^2}$，将拉普拉斯算子推广到离散的二维图像空间，即变成了边缘检测算子 --> 将拉普拉斯算子表示成模板的形式：\n> > $$\n> > \\left[\n> > \\begin{matrix}\n> > \n> > 0 & 1  & 0 \\\\\n> > \n> > 1 & -4 & 1 \\\\\n> > \n> > 0 & 1  & 0\n> > \n> > \\end{matrix}\n> > \\right]\n> > $$\n> > --> 由图可知，<font color =  green>** 拉普拉斯算子描述了中心像素与局部上下左右四个邻居像素的差异 --> 被用来当作 image 上的边缘检测算子。**</font>\n> >\n> > --> <font color =  blue>** 将其推广到 graph 上，也用拉普拉斯算子来描述中心节点与邻居节点之间的信号的差异**</font>\n> >\n> > --> <font color =  green>**拉普拉斯矩阵其实就是离散化拉普拉斯算子的一种表示方式，也就是说拉普拉斯矩阵是图上的拉普拉斯算子。**</font>\n> >\n> > --> 因为拉普拉斯矩阵是图上的拉普拉斯算子，能描述中心节点与邻居节点之间的信号差异，所以将拉普拉斯矩阵应用到图卷积中。\n>\n> --> 综上，<font color =  green>**图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的基函数**</font>，即：<font color =  green>**$\\mathbf{V = (v_1, v_2, \\cdots, v_n)}$--> 傅里叶基**</font>\n>\n> 图傅里叶变换：$\\tilde{x}_k$表示$x$在第$k$个傅里叶基上的傅里叶系数；$V_k$表示第$k$个傅里叶基。公式如下：\n>\n> $$\\tilde{x}_k = \\sum_{i=1}^{N}V_{ki}^{T}x_i = <V_k, x>$$\n>\n> 图傅里叶逆变换：$V_{ki}$表示傅里叶基； $\\tilde{x}_i$表示傅里叶系数。公式如下：\n>\n> $$x_k = \\sum_{i=1}^{N}V_{ki} \\cdot \\tilde{x}_i$$\n>\n> 图信号的总变差 --> 刻画图信号的整体平滑度\n>\n> $$TV(x) = x^{T}Lx = x^{T}(V \\Lambda V^{T})x = (V \\tilde{x})^{T}(V \\Lambda V^{T})(V \\tilde{x})\\\\\n>\n> = \\tilde{x}^{T}V^{T}V \\Lambda V^{T}V \\tilde{x} = \\tilde{x}^{T} \\Lambda \\tilde{x}= \\sum_k^N \\lambda_k \\tilde{x}_k^2$$\n>\n> --> <font color =  green>**$\\lambda_k$：特征值，等价于频率；$\\tilde{x}_k$：傅里叶系数，等价于幅值。**</font>\n\n\n\n> GCN 的两种理解方式（从空域和频域两个不同的角度来理解图卷积）：\n>\n> 1）Spectral-based GCN --> 从图信号处理角度引入滤波器来定义卷积。\n>\n> 2）Spatial-based GIN --> 从邻域聚合特征信息。利用边的信息对节点信息进行聚合从而生成新的节点表示。\n\n### Spectral-based Graph Convolutional Networks\n\n\n\n### Spatial-based Graph Convolutional Networks\n\n\n\n\n\n邻居节点度数越小，分配到的权重越大。\n\n空间意义为：\n\n每个节点通过边对边发送相同量的信息，边越多的节点，每条边发送出去的信息量就越小。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/GNN.md","raw":"---\ntitle: GNN\ntop: false\ncover: false\ntoc: true\nmathjax: false\ndate: 2022-04-12 09:00:41\npassword:\nsummary:\ndescription:\ncategories:\n- GNN\ntags:\n- GNN\n---\n\n# 从 CNN 到 GNN\n\n现实生活中，很多 graph（图数据）是从非欧氏空间中生成的，graph 不再只是从欧氏空间中生成的规则的栅格结构。\n\n> CNN --> 用来提取欧氏空间数据的特征，它针对规则的 2D 栅格结构的 image（传统数据，具有规则的空间结构），其像素点的排列顺序有明显的上下左右的位置关系。\n>\n> CNN 中卷积核的形状是固定的，其卷积操作具有序列有序性和维数不变性的限制。\n\n> GCN --> 针对不规则的 graph（图数据，非矩阵结构数据）， graph 的节点之间无空间上的位置关系，中心节点的邻域节点是不确定的，且邻域节点没有顺序。因此传统的卷积核就无法适应图卷积。\n\n## graph 的定义\n\n> 在数学中，图是由顶点（Vertex）和连接顶点的边（Edge）构成的。顶点表示研究的对象，边表示两个对象之间的特定关系。\n>\n> 图表示顶点和边的集合，记为 $G = (V, E)$ ，其中，$V$是顶点集合，$E$ 是边集合。        设图 $G$ 的顶点数为 $N$，边数为 $M$。                                                                                           一条连接顶点 $v_i, v_j \\in V$ 的边记为$(v_i, v_j)$或者$e_{ij}$ 。\n\n> <font color =  green>**邻居和度**</font>                                                                                                                                 如果存在一条边连接顶点 $v_i$ 和 $v_j$ ，则称 $v_j$ 是 $v_i$ 的邻居，反之亦然。                                                                              $v_i$ 的所有邻居为集合 $N(v_i)$，即: \n>\n> $$N(v_i) = \\{v_j | \\exists e_{ij} \\in E  \\ or \\ e_{ji} \\in E\\}$$\n>\n> 以 $v_i$为端点的边的数目称为 $v_i$ 的度（Degree），记为 $deg(v_i)$,  $deg(v_i) = | N(v_i) |$.\n\n> <font color =  green>**邻接矩阵 & 关联矩阵 & 度矩阵 & 拉普拉斯矩阵**</font>\n>\n> 图 $G = (V, E)$， <font color =  green>**邻接矩阵**</font> *A* 描述图中顶点之间的关系，$A \\in R^{N \\times N}$,其定义为：\n>\n> $$ A_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if \\  (v_i, v_j) \\subseteq E}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> <font color =  green>**关联矩阵**</font> B 描述节点与边之间的关联，$B \\in R^{N \\times M}$，其定义为：\n>\n> $$ B_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if \\  v_i 与 e_j 相连}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> <font color =  green>**度矩阵**</font> D 是一个对角矩阵，$D \\in R^{N \\times N}$，其定义为：\n>\n> $$ D_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> deg(v_i)     &        & {if  \\  i = j}\\\\\n>\n> 0     &        &{else}\n>\n> \\end{array} \\right. $$\n>\n> 示例：\n>\n> <!-- ![avatar](GraphMatrix.jpg ) -->\n>\n> <img src=GraphMatrix.jpg width=70% />\n>\n> <font color =  green>**拉普拉斯矩阵**</font> L 用来表示 graph 的结构信息，$L = D - A \\in R^{N \\times N}$，定义为：\n>\n> $$ L_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> deg(v_i)     &        & {if  \\  i = j}\\\\\n>\n> -1       &         &{if   \\   e_{ij} \\in  E}\\\\\n>\n> 0     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n>\n> --> 由此定义可知：<font color =  green>**拉普拉斯矩阵是一个实对称矩阵！！！**</font>\n>\n> --> <font color =  red>**拉普拉斯矩阵用来表示 graph 的结构信息，目前GCN 都是将结构信息和属性信息剥离开来，能否找到一种方式融合结构信息和属性信息来表示 graph 呢？？？**</font>\n>\n> <font color =  green>**归一化拉普拉斯矩阵**</font> $L^{sym}$，对拉普拉斯矩阵的归一化操作，是通过对拉普拉斯矩阵两边乘以节点的度开方然后取逆得到，即：$L^{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = D^{-\\frac{1}{2}}(D-A))D^{-\\frac{1}{2}} = I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$，其中每一个节点对(i,j)定义为：\n>\n> $$ L^{sym}_{ij} = \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 1     &        & {if  \\  i = j 且 deg(v_i) \\not= 0}\\\\\n>\n> -\\frac{1}{deg(v_i)deg(v_j)}&   &{if   \\   i \\not= j 且 v_i 与 v_j 相邻}\\\\\n>\n> 0     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n\n\n\n\n\n\n\n\n\n\n\n# GNN\n\n## GCN\n\n> GCN：将卷积运算从传统 image 推广到 graph 图数据。核心思想：学习一个函数映射 $f(.)$，通过映射图中的节点 $v_{i}$ 可聚合自身特征 $x_{i}$ 与它的邻居特征 $x_{j} (j \\in N(v_{i}))$ 来生成节点 $v_{i}$ 的新表示。\n>\n> GCN --> 在非欧几里得结构数据中做卷积。\n\n> <font color =  green>**&图卷积神经网络与图傅里叶变换和拉普拉斯矩阵之间有什么关系？**</font> \n>\n> 传统卷积：\n>\n> $$(f * g)(t) = \\int_{R}f(x) \\cdot g(t-x)dx$$\n>\n> --> 传统的卷积是针对具有规则空间结构，其像素点的排列顺序具有明确的上下左右位置关系的 image 的.\n>\n> --> 针对空间结构不规则，其节点之间没有空间上的位置关系的 graph，如何实现卷积操作呢？\n>\n> --> 根据时域卷积定理：时域内的卷积对应频域内的乘积。\n>\n> --> 那么<font color =  green>** 只需定义 graph 上的傅里叶变换，即可定义 graph 上的卷积**</font>，即：\n>\n> $$ f * g = \\mathcal{F}^{-1}\\{ \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\} $$\n>\n> --> 图傅里叶变换是离散的，类似于离散傅里叶变换\n>\n> --> 离散傅里叶正变换：\n>\n> $$F(w) = \\sum_{t=1}^{n}f(t) \\cdot e^{-i \\frac{2\\pi}{n}wt}$$\n>\n> n：傅里叶变换的点数；w：傅里叶变换的第 k 个频谱。\n>\n> --> <font color =  green>** 离散正变换的本质是：求线性组合的系数**</font>，由原函数和基函数的共轭的内积求得。\n>\n> --> 离散傅里叶反变换：\n>\n> $$f(t) = \\frac{1}{n}\\sum_{w=1}^{n}F(w) \\cdot e^{i \\frac{2\\pi}{n}wt}$$\n>\n> F(w)：分量的振幅；w：频率；$e^{i \\frac{2\\pi}{n}wt}$：基\n>\n> --> <font color =  green>** 离散傅里叶反变换的本质：把任意一个函数表示成了若干个正交基函数的线性组合**</font>\n>\n> --> 对应 graph 上的信号 $x = [x_1, x_2, \\cdots, x_n]^{T} \\in R^n$，如果要进行傅里叶变换，那么：\n>\n> <font color =  green>** 我们需要找到一组正交基，通过这组正交基的线性组合来表达$x \\in R^n$**</font>\n>\n> --> <font color =  green>** 图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基**</font>\n>\n> --> <font color =  blue>** 为什么拉普拉斯矩阵的特征向量能作为图傅里叶变换的正交基？**</font>\n>\n> 假设图的拉普拉斯矩阵为 $L \\in R^{N \\times N}$，$L = D - A$ 是一个实对称矩阵，因为<font color =  green>** 实对称矩阵都可以被正交对角化**</font>，所以对 L 进行特征分解（谱分解），将拉普拉斯矩阵分解为由其特征值和特征向量表示的矩阵之积，即：$L = V \\Lambda V^{T}$，$V = [v_1, v_2, \\cdots, v_N]$ 表示 L 的 N 个特征向量， $\\Lambda = diag[\\lambda_1, \\lambda_2, \\cdots, \\lambda_N]$ 表示 L 的 N 个特征值，而 $\\lambda_k$ 则表示第 $k$ 个特征向量对应的特征值。\n>\n> <font color =  green>** 对称矩阵性质： n 阶对称矩阵一定有 n 个线性无关的特征向量**</font>\n>\n> <font color =  green>** 矩阵论：n 维线性空间中的 n 个线性无关的向量都可以构成线性空间中的一组基**</font>，基可表示 n 维线性空间中的所有向量。\n>\n> --> <font color =  green>** 拉普拉斯矩阵的 n 个特征向量是线性无关的，它们是 n 维空间中的一组基**</font>\n>\n> <font color =  green>** 对称矩阵性质：对称矩阵的不同特征值对应的特征向量相互正交，这些正交的特征向量构成的矩阵为正交矩阵**</font>\n>\n> --> $VV^{T} = I$, $L = V \\Lambda V^{T} = V \\Lambda V^{-1}$，$V^{T} = V^{-1}$  <-- 正交矩阵\n>\n> --><font color =  green>** 拉普拉斯矩阵特征分解后的特征向量不但是 n 维空间中的一组基，而且还是正交的（相乘为0）**</font>\n>\n> 即 <font color =  blue>** 拉普拉斯矩阵的 n 个特征向量是 n 维空间中的一组标准正交基**</font>\n>\n> 因此，拉普拉斯矩阵的特征向量可以作为图傅里叶变换的正交基，满足充分条件。<font color =  red>**那么，正交基那么多，为什么 GCN 采用拉普拉斯矩阵的特征向量呢？也就是采用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基的必要性体现在哪里呢？**</font>\n>\n> > <font color =  red>**暂时还没能彻底搞清楚，下面是我认为的原因：**</font>\n> >\n> > 拉普拉斯矩阵的定义来源于拉普拉斯算子。拉普拉斯算子是 n 维欧氏空间中的一个二阶微分算子，定义为梯度的散度，即：$\\triangle f = \\sum_{i=1}^{n} \\frac{\\sigma^2f}{\\sigma{x_i}^2}$，将拉普拉斯算子推广到离散的二维图像空间，即变成了边缘检测算子 --> 将拉普拉斯算子表示成模板的形式：\n> > $$\n> > \\left[\n> > \\begin{matrix}\n> > \n> > 0 & 1  & 0 \\\\\n> > \n> > 1 & -4 & 1 \\\\\n> > \n> > 0 & 1  & 0\n> > \n> > \\end{matrix}\n> > \\right]\n> > $$\n> > --> 由图可知，<font color =  green>** 拉普拉斯算子描述了中心像素与局部上下左右四个邻居像素的差异 --> 被用来当作 image 上的边缘检测算子。**</font>\n> >\n> > --> <font color =  blue>** 将其推广到 graph 上，也用拉普拉斯算子来描述中心节点与邻居节点之间的信号的差异**</font>\n> >\n> > --> <font color =  green>**拉普拉斯矩阵其实就是离散化拉普拉斯算子的一种表示方式，也就是说拉普拉斯矩阵是图上的拉普拉斯算子。**</font>\n> >\n> > --> 因为拉普拉斯矩阵是图上的拉普拉斯算子，能描述中心节点与邻居节点之间的信号差异，所以将拉普拉斯矩阵应用到图卷积中。\n>\n> --> 综上，<font color =  green>**图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的基函数**</font>，即：<font color =  green>**$\\mathbf{V = (v_1, v_2, \\cdots, v_n)}$--> 傅里叶基**</font>\n>\n> 图傅里叶变换：$\\tilde{x}_k$表示$x$在第$k$个傅里叶基上的傅里叶系数；$V_k$表示第$k$个傅里叶基。公式如下：\n>\n> $$\\tilde{x}_k = \\sum_{i=1}^{N}V_{ki}^{T}x_i = <V_k, x>$$\n>\n> 图傅里叶逆变换：$V_{ki}$表示傅里叶基； $\\tilde{x}_i$表示傅里叶系数。公式如下：\n>\n> $$x_k = \\sum_{i=1}^{N}V_{ki} \\cdot \\tilde{x}_i$$\n>\n> 图信号的总变差 --> 刻画图信号的整体平滑度\n>\n> $$TV(x) = x^{T}Lx = x^{T}(V \\Lambda V^{T})x = (V \\tilde{x})^{T}(V \\Lambda V^{T})(V \\tilde{x})\\\\\n>\n> = \\tilde{x}^{T}V^{T}V \\Lambda V^{T}V \\tilde{x} = \\tilde{x}^{T} \\Lambda \\tilde{x}= \\sum_k^N \\lambda_k \\tilde{x}_k^2$$\n>\n> --> <font color =  green>**$\\lambda_k$：特征值，等价于频率；$\\tilde{x}_k$：傅里叶系数，等价于幅值。**</font>\n\n\n\n> GCN 的两种理解方式（从空域和频域两个不同的角度来理解图卷积）：\n>\n> 1）Spectral-based GCN --> 从图信号处理角度引入滤波器来定义卷积。\n>\n> 2）Spatial-based GIN --> 从邻域聚合特征信息。利用边的信息对节点信息进行聚合从而生成新的节点表示。\n\n### Spectral-based Graph Convolutional Networks\n\n\n\n### Spatial-based Graph Convolutional Networks\n\n\n\n\n\n邻居节点度数越小，分配到的权重越大。\n\n空间意义为：\n\n每个节点通过边对边发送相同量的信息，边越多的节点，每条边发送出去的信息量就越小。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"GNN","published":1,"updated":"2022-06-05T01:55:31.526Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl40nn32n0001mculdth9b62w","content":"<html><head></head><body><h1 id=\"从-cnn-到-gnn\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#从-cnn-到-gnn\"></a> 从 CNN 到 GNN</h1>\n<p>现实生活中，很多 graph（图数据）是从非欧氏空间中生成的，graph 不再只是从欧氏空间中生成的规则的栅格结构。</p>\n<blockquote>\n<p>CNN --&gt; 用来提取欧氏空间数据的特征，它针对规则的 2D 栅格结构的 image（传统数据，具有规则的空间结构），其像素点的排列顺序有明显的上下左右的位置关系。</p>\n<p>CNN 中卷积核的形状是固定的，其卷积操作具有序列有序性和维数不变性的限制。</p>\n</blockquote>\n<blockquote>\n<p>GCN --&gt; 针对不规则的 graph（图数据，非矩阵结构数据）， graph 的节点之间无空间上的位置关系，中心节点的邻域节点是不确定的，且邻域节点没有顺序。因此传统的卷积核就无法适应图卷积。</p>\n</blockquote>\n<h2 id=\"graph-的定义\"><span class=\"post-title-index\">1.1. </span><a class=\"markdownIt-Anchor\" href=\"#graph-的定义\"></a> graph 的定义</h2>\n<blockquote>\n<p>在数学中，图是由顶点（Vertex）和连接顶点的边（Edge）构成的。顶点表示研究的对象，边表示两个对象之间的特定关系。</p>\n<p>图表示顶点和边的集合，记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span> ，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>是顶点集合，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 是边集合。        设图 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">G</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span></span></span></span> 的顶点数为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>，边数为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>。                                                                                           一条连接顶点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">v_i, v_j \\in V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8252079999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> 的边记为<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(v_i, v_j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>或者<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">e_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> 。</p>\n</blockquote>\n<blockquote>\n<p><font color=\"green\"><strong>邻居和度</strong></font>                                                                                                                                 如果存在一条边连接顶点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> ，则称 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的邻居，反之亦然。                                                                              <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的所有邻居为集合 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(v_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>，即:</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∃</mi><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mi>E</mi><mtext>&nbsp;</mtext><mi>o</mi><mi>r</mi><mtext>&nbsp;</mtext><msub><mi>e</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>∈</mo><mi>E</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">N(v_i) = \\{v_j | \\exists e_{ij} \\in E  \\ or \\ e_{ji} \\in E\\}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\">∃</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\">&nbsp;</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\">&nbsp;</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">}</span></span></span></span></span></p>\n<p>以 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>为端点的边的数目称为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的度（Degree），记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">deg(v_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>,  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"normal\">∣</mi><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">deg(v_i) = | N(v_i) |</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">∣</span></span></span></span>.</p>\n</blockquote>\n<blockquote>\n<p><font color=\"green\"><strong>邻接矩阵 &amp; 关联矩阵 &amp; 度矩阵 &amp; 拉普拉斯矩阵</strong></font></p>\n<p>图 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span>， <font color=\"green\"><strong>邻接矩阵</strong></font> <em>A</em> 描述图中顶点之间的关系，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>,其定义为：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mo>⊆</mo><mi>E</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> A_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if \\  (v_i, v_j) \\subseteq E}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">⊆</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><font color=\"green\"><strong>关联矩阵</strong></font> B 描述节点与边之间的关联，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">B \\in R^{N \\times M}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span></span></span></span></span></span></span></span>，其定义为：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><msub><mi>v</mi><mi>i</mi></msub><mi mathvariant=\"normal\">与</mi><msub><mi>e</mi><mi>j</mi></msub><mi mathvariant=\"normal\">相</mi><mi mathvariant=\"normal\">连</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> B_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if \\  v_i 与 e_j 相连}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">与</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">相</span><span class=\"mord cjk_fallback\">连</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><font color=\"green\"><strong>度矩阵</strong></font> D 是一个对角矩阵，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，其定义为：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> D_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\ndeg(v_i)     &amp;        &amp; {if  \\  i = j}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>示例：</p>\n<!-- ![avatar](GraphMatrix.jpg ) -->\n<img src=\"GraphMatrix.jpg\" width=\"70%\">\n<p><font color=\"green\"><strong>拉普拉斯矩阵</strong></font> L 用来表示 graph 的结构信息，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>D</mi><mo>−</mo><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L = D - A \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，定义为：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mi>E</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> L_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\ndeg(v_i)     &amp;        &amp; {if  \\  i = j}\\\\\n\n-1       &amp;         &amp;{if   \\   e_{ij} \\in  E}\\\\\n\n0     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.60004em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.49999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-3.15001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.30002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.05em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.849999999999999em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-1.6499999999999992em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>–&gt; 由此定义可知：<font color=\"green\"><strong>拉普拉斯矩阵是一个实对称矩阵！！！</strong></font></p>\n<p>–&gt; <font color=\"red\"><strong>拉普拉斯矩阵用来表示 graph 的结构信息，目前GCN 都是将结构信息和属性信息剥离开来，能否找到一种方式融合结构信息和属性信息来表示 graph 呢？？？</strong></font></p>\n<p><font color=\"green\"><strong>归一化拉普拉斯矩阵</strong></font> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>L</mi><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L^{sym}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span></span></span></span></span></span></span></span>，对拉普拉斯矩阵的归一化操作，是通过对拉普拉斯矩阵两边乘以节点的度开方然后取逆得到，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>L</mi><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msup><mo>=</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mi>L</mi><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>=</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo stretchy=\"false\">(</mo><mi>D</mi><mo>−</mo><mi>A</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>=</mo><msub><mi>I</mi><mi>n</mi></msub><mo>−</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mi>A</mi><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L^{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = D^{-\\frac{1}{2}}(D-A))D^{-\\frac{1}{2}} = I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">L</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.20402em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.20402em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">A</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span></span></span></span>，其中每一个节点对(i,j)定义为：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>L</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msubsup><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mi>i</mi><mo>=</mo><mi>j</mi><mi mathvariant=\"normal\">且</mi><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>≠</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mi>i</mi><mo>≠</mo><mi>j</mi><mi mathvariant=\"normal\">且</mi><msub><mi>v</mi><mi>i</mi></msub><mi mathvariant=\"normal\">与</mi><msub><mi>v</mi><mi>j</mi></msub><mi mathvariant=\"normal\">相</mi><mi mathvariant=\"normal\">邻</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> L^{sym}_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if  \\  i = j 且 deg(v_i) \\not= 0}\\\\\n\n-\\frac{1}{deg(v_i)deg(v_j)}&amp;   &amp;{if   \\   i \\not= j 且 v_i 与 v_j 相邻}\\\\\n\n0     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1952720000000001em;vertical-align:-0.412972em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7823em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.1809080000000005em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.412972em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.7874280000000002em;vertical-align:-1.6437140000000001em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.49999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-3.15001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.30002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.303714em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-3.098606em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2818857142857143em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.5423199999999999em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span><span style=\"top:-1.716286em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.148822em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.943714em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span><span style=\"top:-1.5613939999999997em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.303714em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord cjk_fallback\">且</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">0</span></span></span></span><span style=\"top:-3.098606em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\">&nbsp;</span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord cjk_fallback\">且</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">与</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">相</span><span class=\"mord cjk_fallback\">邻</span></span></span></span><span style=\"top:-1.716286em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n</blockquote>\n<h1 id=\"gnn\"><span class=\"post-title-index\">2. </span><a class=\"markdownIt-Anchor\" href=\"#gnn\"></a> GNN</h1>\n<h2 id=\"gcn\"><span class=\"post-title-index\">2.1. </span><a class=\"markdownIt-Anchor\" href=\"#gcn\"></a> GCN</h2>\n<blockquote>\n<p>GCN：将卷积运算从传统 image 推广到 graph 图数据。核心思想：学习一个函数映射 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span>，通过映射图中的节点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 可聚合自身特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 与它的邻居特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi>j</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">x_{j} (j \\in N(v_{i}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 来生成节点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的新表示。</p>\n<p>GCN --&gt; 在非欧几里得结构数据中做卷积。</p>\n</blockquote>\n<blockquote>\n<p><font color=\"green\"><strong>&amp;图卷积神经网络与图傅里叶变换和拉普拉斯矩阵之间有什么关系？</strong></font></p>\n<p>传统卷积：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo>∗</mo><mi>g</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mo>∫</mo><mi>R</mi></msub><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">(f * g)(t) = \\int_{R}f(x) \\cdot g(t-x)dx\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">)</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.27195em;vertical-align:-0.9119499999999999em;\"></span><span class=\"mop\"><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:-0.433619em;\"><span style=\"top:-1.7880500000000001em;margin-left:-0.44445em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.00773em;\">R</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9119499999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">x</span></span></span></span></span></p>\n<p>–&gt; 传统的卷积是针对具有规则空间结构，其像素点的排列顺序具有明确的上下左右位置关系的 image 的.</p>\n<p>–&gt; 针对空间结构不规则，其节点之间没有空间上的位置关系的 graph，如何实现卷积操作呢？</p>\n<p>–&gt; 根据时域卷积定理：时域内的卷积对应频域内的乘积。</p>\n<p>–&gt; 那么<font color=\"green\">** 只需定义 graph 上的傅里叶变换，即可定义 graph 上的卷积**</font>，即：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>∗</mo><mi>g</mi><mo>=</mo><msup><mi mathvariant=\"script\">F</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">{</mo><mi mathvariant=\"script\">F</mi><mo stretchy=\"false\">{</mo><mi>f</mi><mo stretchy=\"false\">}</mo><mo>⋅</mo><mi mathvariant=\"script\">F</mi><mo stretchy=\"false\">{</mo><mi>g</mi><mo stretchy=\"false\">}</mo><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">f * g = \\mathcal{F}^{-1}\\{ \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\} \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">}</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">}</span><span class=\"mclose\">}</span></span></span></span></span></p>\n<p>–&gt; 图傅里叶变换是离散的，类似于离散傅里叶变换</p>\n<p>–&gt; 离散傅里叶正变换：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F(w) = \\sum_{t=1}^{n}f(t) \\cdot e^{-i \\frac{2\\pi}{n}wt}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9185100000000004em;vertical-align:-1.267113em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0040200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0040200000000001em;\"><span style=\"top:-3.4130000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>n：傅里叶变换的点数；w：傅里叶变换的第 k 个频谱。</p>\n<p>–&gt; <font color=\"green\">** 离散正变换的本质是：求线性组合的系数**</font>，由原函数和基函数的共轭的内积求得。</p>\n<p>–&gt; 离散傅里叶反变换：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>F</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><msup><mi>e</mi><mrow><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">f(t) = \\frac{1}{n}\\sum_{w=1}^{n}F(w) \\cdot e^{i \\frac{2\\pi}{n}wt}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9185100000000004em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">n</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0040200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0040200000000001em;\"><span style=\"top:-3.4130000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>F(w)：分量的振幅；w：频率；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">e^{i \\frac{2\\pi}{n}wt}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span>：基</p>\n<p>–&gt; <font color=\"green\">** 离散傅里叶反变换的本质：把任意一个函数表示成了若干个正交基函数的线性组合**</font></p>\n<p>–&gt; 对应 graph 上的信号 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>n</mi></msub><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">x = [x_1, x_2, \\cdots, x_n]^{T} \\in R^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span>，如果要进行傅里叶变换，那么：</p>\n<p><font color=\"green\">** 我们需要找到一组正交基，通过这组正交基的线性组合来表达<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">x \\in R^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span>**</font></p>\n<p>–&gt; <font color=\"green\">** 图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基**</font></p>\n<p>–&gt; <font color=\"blue\">** 为什么拉普拉斯矩阵的特征向量能作为图傅里叶变换的正交基？**</font></p>\n<p>假设图的拉普拉斯矩阵为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>D</mi><mo>−</mo><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">L = D - A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span> 是一个实对称矩阵，因为<font color=\"green\">** 实对称矩阵都可以被正交对角化**</font>，所以对 L 进行特征分解（谱分解），将拉普拉斯矩阵分解为由其特征值和特征向量表示的矩阵之积，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">L = V \\Lambda V^{T}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>v</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">V = [v_1, v_2, \\cdots, v_N]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span> 表示 L 的 N 个特征向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Λ</mi><mo>=</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy=\"false\">[</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>λ</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\Lambda = diag[\\lambda_1, \\lambda_2, \\cdots, \\lambda_N]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Λ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span> 表示 L 的 N 个特征值，而 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>λ</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 则表示第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 个特征向量对应的特征值。</p>\n<p><font color=\"green\">** 对称矩阵性质： n 阶对称矩阵一定有 n 个线性无关的特征向量**</font></p>\n<p><font color=\"green\">** 矩阵论：n 维线性空间中的 n 个线性无关的向量都可以构成线性空间中的一组基**</font>，基可表示 n 维线性空间中的所有向量。</p>\n<p>–&gt; <font color=\"green\">** 拉普拉斯矩阵的 n 个特征向量是线性无关的，它们是 n 维空间中的一组基**</font></p>\n<p><font color=\"green\">** 对称矩阵性质：对称矩阵的不同特征值对应的特征向量相互正交，这些正交的特征向量构成的矩阵为正交矩阵**</font></p>\n<p>–&gt; <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">VV^{T} = I</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L = V \\Lambda V^{T} = V \\Lambda V^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{T} = V^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span>  &lt;-- 正交矩阵</p>\n<p>–&gt;<font color=\"green\">** 拉普拉斯矩阵特征分解后的特征向量不但是 n 维空间中的一组基，而且还是正交的（相乘为0）**</font></p>\n<p>即&nbsp;<font color=\"blue\">** 拉普拉斯矩阵的 n 个特征向量是 n 维空间中的一组标准正交基**</font></p>\n<p>因此，拉普拉斯矩阵的特征向量可以作为图傅里叶变换的正交基，满足充分条件。<font color=\"red\"><strong>那么，正交基那么多，为什么 GCN 采用拉普拉斯矩阵的特征向量呢？也就是采用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基的必要性体现在哪里呢？</strong></font></p>\n<blockquote>\n<p><font color=\"red\"><strong>暂时还没能彻底搞清楚，下面是我认为的原因：</strong></font></p>\n<p>拉普拉斯矩阵的定义来源于拉普拉斯算子。拉普拉斯算子是 n 维欧氏空间中的一个二阶微分算子，定义为梯度的散度，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">△</mi><mi>f</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><msup><mi>σ</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>σ</mi><msup><msub><mi>x</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\triangle f = \\sum_{i=1}^{n} \\frac{\\sigma^2f}{\\sigma{x_i}^2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">△</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.5151279999999998em;vertical-align:-0.44509999999999994em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.070028em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7463142857142857em;\"><span style=\"top:-2.786em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.446108em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913142857142857em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.44509999999999994em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>，将拉普拉斯算子推广到离散的二维图像空间，即变成了边缘检测算子 --&gt; 将拉普拉斯算子表示成模板的形式：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mn>4</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\n\\begin{matrix}\n\n0 &amp; 1  &amp; 0 \\\\\n\n1 &amp; -4 &amp; 1 \\\\\n\n0 &amp; 1  &amp; 0\n\n\\end{matrix}\n\\right]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.60004em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎣</span></span></span><span style=\"top:-4.05002em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎡</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\">4</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎦</span></span></span><span style=\"top:-4.05002em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎤</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>–&gt; 由图可知，<font color=\"green\">** 拉普拉斯算子描述了中心像素与局部上下左右四个邻居像素的差异 --&gt; 被用来当作 image 上的边缘检测算子。**</font></p>\n<p>–&gt; <font color=\"blue\">** 将其推广到 graph 上，也用拉普拉斯算子来描述中心节点与邻居节点之间的信号的差异**</font></p>\n<p>–&gt; <font color=\"green\"><strong>拉普拉斯矩阵其实就是离散化拉普拉斯算子的一种表示方式，也就是说拉普拉斯矩阵是图上的拉普拉斯算子。</strong></font></p>\n<p>–&gt; 因为拉普拉斯矩阵是图上的拉普拉斯算子，能描述中心节点与邻居节点之间的信号差异，所以将拉普拉斯矩阵应用到图卷积中。</p>\n</blockquote>\n<p>–&gt; 综上，<font color=\"green\"><strong>图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的基函数</strong></font>，即：<font color=\"green\"><strong><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold\">V</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">v</mi><mn mathvariant=\"bold\">1</mn></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">v</mi><mn mathvariant=\"bold\">2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">v</mi><mi mathvariant=\"bold\">n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{V = (v_1, v_2, \\cdots, v_n)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.161108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>–&gt; 傅里叶基</strong></font></p>\n<p>图傅里叶变换：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>在第<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>个傅里叶基上的傅里叶系数；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>V</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">V_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示第<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>个傅里叶基。公式如下：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>&lt;</mo><msub><mi>V</mi><mi>k</mi></msub><mo separator=\"true\">,</mo><mi>x</mi><mo>&gt;</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k = \\sum_{i=1}^{N}V_{ki}^{T}x_i = &lt;V_k, x&gt;\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span></span></span></span></span></p>\n<p>图傅里叶逆变换：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">V_{ki}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示傅里叶基； <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示傅里叶系数。公式如下：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_k = \\sum_{i=1}^{N}V_{ki} \\cdot \\tilde{x}_i\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>图信号的总变差 --&gt; 刻画图信号的整体平滑度</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mi>V</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>L</mi><mi>x</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo stretchy=\"false\">)</mo><mi>x</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><msup><mo stretchy=\"false\">)</mo><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mo>=</mo><msup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>T</mi></msup><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><msup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>T</mi></msup><mi mathvariant=\"normal\">Λ</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><munderover><mo>∑</mo><mi>k</mi><mi>N</mi></munderover><msub><mi>λ</mi><mi>k</mi></msub><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">TV(x) = x^{T}Lx = x^{T}(V \\Lambda V^{T})x = (V \\tilde{x})^{T}(V \\Lambda V^{T})(V \\tilde{x})\\\\\n\n= \\tilde{x}^{T}V^{T}V \\Lambda V^{T}V \\tilde{x} = \\tilde{x}^{T} \\Lambda \\tilde{x}= \\sum_k^N \\lambda_k \\tilde{x}_k^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.36687em;vertical-align:0em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord\">Λ</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.1304490000000005em;vertical-align:-1.302113em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.8478869999999998em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-3.0500049999999996em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.300005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.302113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>–&gt; <font color=\"green\"><strong><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>λ</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：特征值，等价于频率；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：傅里叶系数，等价于幅值。</strong></font></p>\n</blockquote>\n<blockquote>\n<p>GCN 的两种理解方式（从空域和频域两个不同的角度来理解图卷积）：</p>\n<p>1）Spectral-based GCN --&gt; 从图信号处理角度引入滤波器来定义卷积。</p>\n<p>2）Spatial-based GIN --&gt; 从邻域聚合特征信息。利用边的信息对节点信息进行聚合从而生成新的节点表示。</p>\n</blockquote>\n<h3 id=\"spectral-based-graph-convolutional-networks\"><span class=\"post-title-index\">2.1.1. </span><a class=\"markdownIt-Anchor\" href=\"#spectral-based-graph-convolutional-networks\"></a> Spectral-based Graph Convolutional Networks</h3>\n<h3 id=\"spatial-based-graph-convolutional-networks\"><span class=\"post-title-index\">2.1.2. </span><a class=\"markdownIt-Anchor\" href=\"#spatial-based-graph-convolutional-networks\"></a> Spatial-based Graph Convolutional Networks</h3>\n<p>邻居节点度数越小，分配到的权重越大。</p>\n<p>空间意义为：</p>\n<p>每个节点通过边对边发送相同量的信息，边越多的节点，每条边发送出去的信息量就越小。</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"从-cnn-到-gnn\"><a class=\"markdownIt-Anchor\" href=\"#从-cnn-到-gnn\"></a> 从 CNN 到 GNN</h1>\n<p>现实生活中，很多 graph（图数据）是从非欧氏空间中生成的，graph 不再只是从欧氏空间中生成的规则的栅格结构。</p>\n<blockquote>\n<p>CNN --&gt; 用来提取欧氏空间数据的特征，它针对规则的 2D 栅格结构的 image（传统数据，具有规则的空间结构），其像素点的排列顺序有明显的上下左右的位置关系。</p>\n<p>CNN 中卷积核的形状是固定的，其卷积操作具有序列有序性和维数不变性的限制。</p>\n</blockquote>\n<blockquote>\n<p>GCN --&gt; 针对不规则的 graph（图数据，非矩阵结构数据）， graph 的节点之间无空间上的位置关系，中心节点的邻域节点是不确定的，且邻域节点没有顺序。因此传统的卷积核就无法适应图卷积。</p>\n</blockquote>\n<h2 id=\"graph-的定义\"><a class=\"markdownIt-Anchor\" href=\"#graph-的定义\"></a> graph 的定义</h2>\n<blockquote>\n<p>在数学中，图是由顶点（Vertex）和连接顶点的边（Edge）构成的。顶点表示研究的对象，边表示两个对象之间的特定关系。</p>\n<p>图表示顶点和边的集合，记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span> ，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>是顶点集合，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 是边集合。        设图 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">G</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span></span></span></span> 的顶点数为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>，边数为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>。                                                                                           一条连接顶点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">v_i, v_j \\in V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8252079999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> 的边记为<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(v_i, v_j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>或者<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">e_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> 。</p>\n</blockquote>\n<blockquote>\n<p><font color =  green><strong>邻居和度</strong></font>                                                                                                                                 如果存在一条边连接顶点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> ，则称 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的邻居，反之亦然。                                                                              <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的所有邻居为集合 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(v_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>，即:</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∃</mi><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mi>E</mi><mtext> </mtext><mi>o</mi><mi>r</mi><mtext> </mtext><msub><mi>e</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>∈</mo><mi>E</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">N(v_i) = \\{v_j | \\exists e_{ij} \\in E  \\ or \\ e_{ji} \\in E\\}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\">∃</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\"> </span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">}</span></span></span></span></span></p>\n<p>以 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>为端点的边的数目称为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的度（Degree），记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">deg(v_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>,  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"normal\">∣</mi><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">deg(v_i) = | N(v_i) |</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">∣</span></span></span></span>.</p>\n</blockquote>\n<blockquote>\n<p><font color =  green><strong>邻接矩阵 &amp; 关联矩阵 &amp; 度矩阵 &amp; 拉普拉斯矩阵</strong></font></p>\n<p>图 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G = (V, E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span>， <font color =  green><strong>邻接矩阵</strong></font> <em>A</em> 描述图中顶点之间的关系，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>,其定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mo>⊆</mo><mi>E</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> A_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if \\  (v_i, v_j) \\subseteq E}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">⊆</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><font color =  green><strong>关联矩阵</strong></font> B 描述节点与边之间的关联，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">B \\in R^{N \\times M}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span></span></span></span></span></span></span></span>，其定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>B</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><msub><mi>v</mi><mi>i</mi></msub><mi mathvariant=\"normal\">与</mi><msub><mi>e</mi><mi>j</mi></msub><mi mathvariant=\"normal\">相</mi><mi mathvariant=\"normal\">连</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> B_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if \\  v_i 与 e_j 相连}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">与</span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">相</span><span class=\"mord cjk_fallback\">连</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p><font color =  green><strong>度矩阵</strong></font> D 是一个对角矩阵，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，其定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>e</mi><mi>l</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> D_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\ndeg(v_i)     &amp;        &amp; {if  \\  i = j}\\\\\n\n0     &amp;        &amp;{else}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>示例：</p>\n<!-- ![avatar](GraphMatrix.jpg ) -->\n<img src=GraphMatrix.jpg width=70% />\n<p><font color =  green><strong>拉普拉斯矩阵</strong></font> L 用来表示 graph 的结构信息，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>D</mi><mo>−</mo><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L = D - A \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mi>E</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> L_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\ndeg(v_i)     &amp;        &amp; {if  \\  i = j}\\\\\n\n-1       &amp;         &amp;{if   \\   e_{ij} \\in  E}\\\\\n\n0     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.60004em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.49999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-3.15001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.30002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.05em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.849999999999999em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-1.6499999999999992em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>–&gt; 由此定义可知：<font color =  green><strong>拉普拉斯矩阵是一个实对称矩阵！！！</strong></font></p>\n<p>–&gt; <font color =  red><strong>拉普拉斯矩阵用来表示 graph 的结构信息，目前GCN 都是将结构信息和属性信息剥离开来，能否找到一种方式融合结构信息和属性信息来表示 graph 呢？？？</strong></font></p>\n<p><font color =  green><strong>归一化拉普拉斯矩阵</strong></font> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>L</mi><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L^{sym}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span></span></span></span></span></span></span></span>，对拉普拉斯矩阵的归一化操作，是通过对拉普拉斯矩阵两边乘以节点的度开方然后取逆得到，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>L</mi><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msup><mo>=</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mi>L</mi><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>=</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo stretchy=\"false\">(</mo><mi>D</mi><mo>−</mo><mi>A</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo>=</mo><msub><mi>I</mi><mi>n</mi></msub><mo>−</mo><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mi>A</mi><msup><mi>D</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L^{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = D^{-\\frac{1}{2}}(D-A))D^{-\\frac{1}{2}} = I_n - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">L</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.20402em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.20402em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">A</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span></span></span></span></span></span></span></span></span>，其中每一个节点对(i,j)定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>L</mi><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mi>s</mi><mi>y</mi><mi>m</mi></mrow></msubsup><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>i</mi><mo>=</mo><mi>j</mi><mi mathvariant=\"normal\">且</mi><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>≠</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>i</mi><mo>≠</mo><mi>j</mi><mi mathvariant=\"normal\">且</mi><msub><mi>v</mi><mi>i</mi></msub><mi mathvariant=\"normal\">与</mi><msub><mi>v</mi><mi>j</mi></msub><mi mathvariant=\"normal\">相</mi><mi mathvariant=\"normal\">邻</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> L^{sym}_{ij} = \\left\\{\n\n\\begin{array}{rcl}\n\n1     &amp;        &amp; {if  \\  i = j 且 deg(v_i) \\not= 0}\\\\\n\n-\\frac{1}{deg(v_i)deg(v_j)}&amp;   &amp;{if   \\   i \\not= j 且 v_i 与 v_j 相邻}\\\\\n\n0     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1952720000000001em;vertical-align:-0.412972em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7823em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span style=\"top:-3.1809080000000005em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.412972em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.7874280000000002em;vertical-align:-1.6437140000000001em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.49999em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-3.15001em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.30002em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.303714em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-3.098606em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2818857142857143em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.5423199999999999em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span><span style=\"top:-1.716286em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.148822em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.943714em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span><span style=\"top:-1.5613939999999997em;\"><span class=\"pstrut\" style=\"height:2.8451079999999997em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.143714em;\"><span style=\"top:-4.303714em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord cjk_fallback\">且</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">0</span></span></span></span><span style=\"top:-3.098606em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\"> </span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord cjk_fallback\">且</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">与</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">相</span><span class=\"mord cjk_fallback\">邻</span></span></span></span><span style=\"top:-1.716286em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6437140000000001em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n</blockquote>\n<h1 id=\"gnn\"><a class=\"markdownIt-Anchor\" href=\"#gnn\"></a> GNN</h1>\n<h2 id=\"gcn\"><a class=\"markdownIt-Anchor\" href=\"#gcn\"></a> GCN</h2>\n<blockquote>\n<p>GCN：将卷积运算从传统 image 推广到 graph 图数据。核心思想：学习一个函数映射 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span>，通过映射图中的节点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 可聚合自身特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 与它的邻居特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi>j</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">x_{j} (j \\in N(v_{i}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 来生成节点 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的新表示。</p>\n<p>GCN --&gt; 在非欧几里得结构数据中做卷积。</p>\n</blockquote>\n<blockquote>\n<p><font color =  green><strong>&amp;图卷积神经网络与图傅里叶变换和拉普拉斯矩阵之间有什么关系？</strong></font></p>\n<p>传统卷积：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo>∗</mo><mi>g</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mo>∫</mo><mi>R</mi></msub><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">(f * g)(t) = \\int_{R}f(x) \\cdot g(t-x)dx\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">)</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.27195em;vertical-align:-0.9119499999999999em;\"></span><span class=\"mop\"><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:-0.433619em;\"><span style=\"top:-1.7880500000000001em;margin-left:-0.44445em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.00773em;\">R</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9119499999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">x</span></span></span></span></span></p>\n<p>–&gt; 传统的卷积是针对具有规则空间结构，其像素点的排列顺序具有明确的上下左右位置关系的 image 的.</p>\n<p>–&gt; 针对空间结构不规则，其节点之间没有空间上的位置关系的 graph，如何实现卷积操作呢？</p>\n<p>–&gt; 根据时域卷积定理：时域内的卷积对应频域内的乘积。</p>\n<p>–&gt; 那么<font color =  green>** 只需定义 graph 上的傅里叶变换，即可定义 graph 上的卷积**</font>，即：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>∗</mo><mi>g</mi><mo>=</mo><msup><mi mathvariant=\"script\">F</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">{</mo><mi mathvariant=\"script\">F</mi><mo stretchy=\"false\">{</mo><mi>f</mi><mo stretchy=\"false\">}</mo><mo>⋅</mo><mi mathvariant=\"script\">F</mi><mo stretchy=\"false\">{</mo><mi>g</mi><mo stretchy=\"false\">}</mo><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">f * g = \\mathcal{F}^{-1}\\{ \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\} \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">}</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mclose\">}</span><span class=\"mclose\">}</span></span></span></span></span></p>\n<p>–&gt; 图傅里叶变换是离散的，类似于离散傅里叶变换</p>\n<p>–&gt; 离散傅里叶正变换：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F(w) = \\sum_{t=1}^{n}f(t) \\cdot e^{-i \\frac{2\\pi}{n}wt}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9185100000000004em;vertical-align:-1.267113em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0040200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0040200000000001em;\"><span style=\"top:-3.4130000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>n：傅里叶变换的点数；w：傅里叶变换的第 k 个频谱。</p>\n<p>–&gt; <font color =  green>** 离散正变换的本质是：求线性组合的系数**</font>，由原函数和基函数的共轭的内积求得。</p>\n<p>–&gt; 离散傅里叶反变换：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>F</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><msup><mi>e</mi><mrow><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">f(t) = \\frac{1}{n}\\sum_{w=1}^{n}F(w) \\cdot e^{i \\frac{2\\pi}{n}wt}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9185100000000004em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">n</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6513970000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0040200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0040200000000001em;\"><span style=\"top:-3.4130000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>F(w)：分量的振幅；w：频率；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mfrac><mrow><mn>2</mn><mi>π</mi></mrow><mi>n</mi></mfrac><mi>w</mi><mi>t</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">e^{i \\frac{2\\pi}{n}wt}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9540200000000001em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9540200000000001em;\"><span style=\"top:-3.363em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span></span></span></span></span>：基</p>\n<p>–&gt; <font color =  green>** 离散傅里叶反变换的本质：把任意一个函数表示成了若干个正交基函数的线性组合**</font></p>\n<p>–&gt; 对应 graph 上的信号 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>n</mi></msub><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">x = [x_1, x_2, \\cdots, x_n]^{T} \\in R^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span>，如果要进行傅里叶变换，那么：</p>\n<p><font color =  green>** 我们需要找到一组正交基，通过这组正交基的线性组合来表达<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">x \\in R^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span>**</font></p>\n<p>–&gt; <font color =  green>** 图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基**</font></p>\n<p>–&gt; <font color =  blue>** 为什么拉普拉斯矩阵的特征向量能作为图傅里叶变换的正交基？**</font></p>\n<p>假设图的拉普拉斯矩阵为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L \\in R^{N \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>D</mi><mo>−</mo><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">L = D - A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span> 是一个实对称矩阵，因为<font color =  green>** 实对称矩阵都可以被正交对角化**</font>，所以对 L 进行特征分解（谱分解），将拉普拉斯矩阵分解为由其特征值和特征向量表示的矩阵之积，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">L = V \\Lambda V^{T}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>v</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>v</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">V = [v_1, v_2, \\cdots, v_N]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span> 表示 L 的 N 个特征向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Λ</mi><mo>=</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy=\"false\">[</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>λ</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\Lambda = diag[\\lambda_1, \\lambda_2, \\cdots, \\lambda_N]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Λ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span> 表示 L 的 N 个特征值，而 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>λ</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 则表示第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 个特征向量对应的特征值。</p>\n<p><font color =  green>** 对称矩阵性质： n 阶对称矩阵一定有 n 个线性无关的特征向量**</font></p>\n<p><font color =  green>** 矩阵论：n 维线性空间中的 n 个线性无关的向量都可以构成线性空间中的一组基**</font>，基可表示 n 维线性空间中的所有向量。</p>\n<p>–&gt; <font color =  green>** 拉普拉斯矩阵的 n 个特征向量是线性无关的，它们是 n 维空间中的一组基**</font></p>\n<p><font color =  green>** 对称矩阵性质：对称矩阵的不同特征值对应的特征向量相互正交，这些正交的特征向量构成的矩阵为正交矩阵**</font></p>\n<p>–&gt; <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">VV^{T} = I</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">L = V \\Lambda V^{T} = V \\Lambda V^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{T} = V^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span>  &lt;-- 正交矩阵</p>\n<p>–&gt;<font color =  green>** 拉普拉斯矩阵特征分解后的特征向量不但是 n 维空间中的一组基，而且还是正交的（相乘为0）**</font></p>\n<p>即 <font color =  blue>** 拉普拉斯矩阵的 n 个特征向量是 n 维空间中的一组标准正交基**</font></p>\n<p>因此，拉普拉斯矩阵的特征向量可以作为图傅里叶变换的正交基，满足充分条件。<font color =  red><strong>那么，正交基那么多，为什么 GCN 采用拉普拉斯矩阵的特征向量呢？也就是采用拉普拉斯矩阵的特征向量作为图傅里叶变换的正交基的必要性体现在哪里呢？</strong></font></p>\n<blockquote>\n<p><font color =  red><strong>暂时还没能彻底搞清楚，下面是我认为的原因：</strong></font></p>\n<p>拉普拉斯矩阵的定义来源于拉普拉斯算子。拉普拉斯算子是 n 维欧氏空间中的一个二阶微分算子，定义为梯度的散度，即：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">△</mi><mi>f</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><msup><mi>σ</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>σ</mi><msup><msub><mi>x</mi><mi>i</mi></msub><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\triangle f = \\sum_{i=1}^{n} \\frac{\\sigma^2f}{\\sigma{x_i}^2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">△</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.5151279999999998em;vertical-align:-0.44509999999999994em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.804292em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.070028em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7463142857142857em;\"><span style=\"top:-2.786em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.446108em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913142857142857em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.44509999999999994em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>，将拉普拉斯算子推广到离散的二维图像空间，即变成了边缘检测算子 --&gt; 将拉普拉斯算子表示成模板的形式：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mn>4</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\n\\begin{matrix}\n\n0 &amp; 1  &amp; 0 \\\\\n\n1 &amp; -4 &amp; 1 \\\\\n\n0 &amp; 1  &amp; 0\n\n\\end{matrix}\n\\right]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.60004em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎣</span></span></span><span style=\"top:-4.05002em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎡</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord\">4</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05002em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎦</span></span></span><span style=\"top:-4.05002em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎤</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>–&gt; 由图可知，<font color =  green>** 拉普拉斯算子描述了中心像素与局部上下左右四个邻居像素的差异 --&gt; 被用来当作 image 上的边缘检测算子。**</font></p>\n<p>–&gt; <font color =  blue>** 将其推广到 graph 上，也用拉普拉斯算子来描述中心节点与邻居节点之间的信号的差异**</font></p>\n<p>–&gt; <font color =  green><strong>拉普拉斯矩阵其实就是离散化拉普拉斯算子的一种表示方式，也就是说拉普拉斯矩阵是图上的拉普拉斯算子。</strong></font></p>\n<p>–&gt; 因为拉普拉斯矩阵是图上的拉普拉斯算子，能描述中心节点与邻居节点之间的信号差异，所以将拉普拉斯矩阵应用到图卷积中。</p>\n</blockquote>\n<p>–&gt; 综上，<font color =  green><strong>图傅里叶变换使用拉普拉斯矩阵的特征向量作为图傅里叶变换的基函数</strong></font>，即：<font color =  green><strong><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold\">V</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold\">v</mi><mn mathvariant=\"bold\">1</mn></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">v</mi><mn mathvariant=\"bold\">2</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold\">v</mi><mi mathvariant=\"bold\">n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{V = (v_1, v_2, \\cdots, v_n)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.161108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01597em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathbf mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>–&gt; 傅里叶基</strong></font></p>\n<p>图傅里叶变换：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>在第<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>个傅里叶基上的傅里叶系数；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>V</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">V_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示第<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>个傅里叶基。公式如下：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>&lt;</mo><msub><mi>V</mi><mi>k</mi></msub><mo separator=\"true\">,</mo><mi>x</mi><mo>&gt;</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k = \\sum_{i=1}^{N}V_{ki}^{T}x_i = &lt;V_k, x&gt;\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span></span></span></span></span></p>\n<p>图傅里叶逆变换：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">V_{ki}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示傅里叶基； <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>表示傅里叶系数。公式如下：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>V</mi><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_k = \\sum_{i=1}^{N}V_{ki} \\cdot \\tilde{x}_i\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>图信号的总变差 --&gt; 刻画图信号的整体平滑度</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mi>V</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>L</mi><mi>x</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo stretchy=\"false\">)</mo><mi>x</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><msup><mo stretchy=\"false\">)</mo><mi>T</mi></msup><mo stretchy=\"false\">(</mo><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mo>=</mo><msup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>T</mi></msup><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mi mathvariant=\"normal\">Λ</mi><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><msup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>T</mi></msup><mi mathvariant=\"normal\">Λ</mi><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><munderover><mo>∑</mo><mi>k</mi><mi>N</mi></munderover><msub><mi>λ</mi><mi>k</mi></msub><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">TV(x) = x^{T}Lx = x^{T}(V \\Lambda V^{T})x = (V \\tilde{x})^{T}(V \\Lambda V^{T})(V \\tilde{x})\\\\\n\n= \\tilde{x}^{T}V^{T}V \\Lambda V^{T}V \\tilde{x} = \\tilde{x}^{T} \\Lambda \\tilde{x}= \\sum_k^N \\lambda_k \\tilde{x}_k^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.36687em;vertical-align:0em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Λ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span><span class=\"mord\">Λ</span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.1304490000000005em;vertical-align:-1.302113em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.8478869999999998em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-3.0500049999999996em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.300005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.302113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>–&gt; <font color =  green><strong><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>λ</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：特征值，等价于频率；<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\">~</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>：傅里叶系数，等价于幅值。</strong></font></p>\n</blockquote>\n<blockquote>\n<p>GCN 的两种理解方式（从空域和频域两个不同的角度来理解图卷积）：</p>\n<p>1）Spectral-based GCN --&gt; 从图信号处理角度引入滤波器来定义卷积。</p>\n<p>2）Spatial-based GIN --&gt; 从邻域聚合特征信息。利用边的信息对节点信息进行聚合从而生成新的节点表示。</p>\n</blockquote>\n<h3 id=\"spectral-based-graph-convolutional-networks\"><a class=\"markdownIt-Anchor\" href=\"#spectral-based-graph-convolutional-networks\"></a> Spectral-based Graph Convolutional Networks</h3>\n<h3 id=\"spatial-based-graph-convolutional-networks\"><a class=\"markdownIt-Anchor\" href=\"#spatial-based-graph-convolutional-networks\"></a> Spatial-based Graph Convolutional Networks</h3>\n<p>邻居节点度数越小，分配到的权重越大。</p>\n<p>空间意义为：</p>\n<p>每个节点通过边对边发送相同量的信息，边越多的节点，每条边发送出去的信息量就越小。</p>\n"},{"title":"English-Writing","top":false,"cover":false,"toc":true,"date":"2022-05-11T12:17:31.000Z","password":null,"summary":null,"description":null,"_content":"\n# 摘抄论文佳句\n\n- xxx <font color=green>have experienced significant attention and have become the de facto methods for</font> xxx. --> xxx 受到了广泛的关注，并已成为xxx的实际方法。\n- <font color=green>building upon [cite]</font>, xxx propose to ... --> 在[引文]的基础上，xxx提出...\n- xxx <font color=green>derive inspiration primarily from</font> xxx --> xxx 主要从xxx中获得灵感。\n- <font color=green>including but not limited to</font> ... --> 包括但不限于...\n- <font color=green>In particular, SGC rivals, if not surpasses,</font> GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. --> 特别是，SGC 在文本分类，用户地理定位，关系提取和 zero-shot 图像分类任务方面可以与基于 GCN 的方法竞争，甚至超越。\n- More <font color=green>compactly</font>, --> 更简洁地，\n- features in the second layer <font color=green>build on top of the</font> features of the first layer. --> 第二层特征是建立在第一层特征之上的。\n- We hypothesize that the nonlinearity between GCN layers <font color=green>is not critical - but that the majority of the benefit arises from</font> the local averaging. --> 我们假设 GCN 层之间的非线性不是至关重要的——但是大部分的好处来自于局部平均。\n- <font color=green>As one may observe, </font>--> 可以看出，\n- <font color=green>In a follow-up work, </font> --> 在后续工作中，\n- <font color=green>While dating back to at least </font> xxx, --> 至少可以追溯到xxx，\n- a k-nearest neighbor(kNN) graph <font color=green>is constructed on the fly in the</font> feature space of the neural network. --> 一个 k 近邻（kNN）图是在神经网络的特征空间中动态构造的。\n- The DGM <font color=green>is tasked with building</font> the (weighted) graph representing the input space. --> DGM 的任务是构建表示输入空间的（加权）图。\n- <font color=green>The advantage provided by the proposed methods is confirmed by the results achieved on</font> four real world datasets: <font color=green>an increase of up to 12 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> vertex-based semi-supervised classification <font color=green> and up to 2 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> graph-based wupervised classification. --> 在四个真实世界数据集上取得的结果证实了所提出的方法提供的优势：基于顶点的半监督分类的准确率和 F1得分提高了12个百分点，基于图监督分类的准确率和 F1 得分提高了2个百分点。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/English-Writing.md","raw":"---\ntitle: English-Writing\ntop: false\ncover: false\ntoc: true\ndate: 2022-05-11 20:17:31\npassword:\nsummary:\ndescription:\ncategories:\n- English Paper Writing\ntags:\n- English Paper Writing\n---\n\n# 摘抄论文佳句\n\n- xxx <font color=green>have experienced significant attention and have become the de facto methods for</font> xxx. --> xxx 受到了广泛的关注，并已成为xxx的实际方法。\n- <font color=green>building upon [cite]</font>, xxx propose to ... --> 在[引文]的基础上，xxx提出...\n- xxx <font color=green>derive inspiration primarily from</font> xxx --> xxx 主要从xxx中获得灵感。\n- <font color=green>including but not limited to</font> ... --> 包括但不限于...\n- <font color=green>In particular, SGC rivals, if not surpasses,</font> GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. --> 特别是，SGC 在文本分类，用户地理定位，关系提取和 zero-shot 图像分类任务方面可以与基于 GCN 的方法竞争，甚至超越。\n- More <font color=green>compactly</font>, --> 更简洁地，\n- features in the second layer <font color=green>build on top of the</font> features of the first layer. --> 第二层特征是建立在第一层特征之上的。\n- We hypothesize that the nonlinearity between GCN layers <font color=green>is not critical - but that the majority of the benefit arises from</font> the local averaging. --> 我们假设 GCN 层之间的非线性不是至关重要的——但是大部分的好处来自于局部平均。\n- <font color=green>As one may observe, </font>--> 可以看出，\n- <font color=green>In a follow-up work, </font> --> 在后续工作中，\n- <font color=green>While dating back to at least </font> xxx, --> 至少可以追溯到xxx，\n- a k-nearest neighbor(kNN) graph <font color=green>is constructed on the fly in the</font> feature space of the neural network. --> 一个 k 近邻（kNN）图是在神经网络的特征空间中动态构造的。\n- The DGM <font color=green>is tasked with building</font> the (weighted) graph representing the input space. --> DGM 的任务是构建表示输入空间的（加权）图。\n- <font color=green>The advantage provided by the proposed methods is confirmed by the results achieved on</font> four real world datasets: <font color=green>an increase of up to 12 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> vertex-based semi-supervised classification <font color=green> and up to 2 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> graph-based wupervised classification. --> 在四个真实世界数据集上取得的结果证实了所提出的方法提供的优势：基于顶点的半监督分类的准确率和 F1得分提高了12个百分点，基于图监督分类的准确率和 F1 得分提高了2个百分点。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"English-Writing","published":1,"updated":"2022-06-05T01:55:31.525Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl40nn32z0008mculcmu852qe","content":"<html><head></head><body><h1 id=\"摘抄论文佳句\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#摘抄论文佳句\"></a> 摘抄论文佳句</h1>\n<ul>\n<li>xxx <font color=\"green\">have experienced significant attention and have become the de facto methods for</font> xxx. --&gt; xxx 受到了广泛的关注，并已成为xxx的实际方法。</li>\n<li><font color=\"green\">building upon [cite]</font>, xxx propose to … --&gt; 在[引文]的基础上，xxx提出…</li>\n<li>xxx <font color=\"green\">derive inspiration primarily from</font> xxx --&gt; xxx 主要从xxx中获得灵感。</li>\n<li><font color=\"green\">including but not limited to</font> … --&gt; 包括但不限于…</li>\n<li><font color=\"green\">In particular, SGC rivals, if not surpasses,</font> GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. --&gt; 特别是，SGC 在文本分类，用户地理定位，关系提取和 zero-shot 图像分类任务方面可以与基于 GCN 的方法竞争，甚至超越。</li>\n<li>More <font color=\"green\">compactly</font>, --&gt; 更简洁地，</li>\n<li>features in the second layer <font color=\"green\">build on top of the</font> features of the first layer. --&gt; 第二层特征是建立在第一层特征之上的。</li>\n<li>We hypothesize that the nonlinearity between GCN layers <font color=\"green\">is not critical - but that the majority of the benefit arises from</font> the local averaging. --&gt; 我们假设 GCN 层之间的非线性不是至关重要的——但是大部分的好处来自于局部平均。</li>\n<li><font color=\"green\">As one may observe, </font>–&gt; 可以看出，</li>\n<li><font color=\"green\">In a follow-up work, </font> --&gt; 在后续工作中，</li>\n<li><font color=\"green\">While dating back to at least </font> xxx, --&gt; 至少可以追溯到xxx，</li>\n<li>a k-nearest neighbor(kNN) graph <font color=\"green\">is constructed on the fly in the</font> feature space of the neural network. --&gt; 一个 k 近邻（kNN）图是在神经网络的特征空间中动态构造的。</li>\n<li>The DGM <font color=\"green\">is tasked with building</font> the (weighted) graph representing the input space. --&gt; DGM 的任务是构建表示输入空间的（加权）图。</li>\n<li><font color=\"green\">The advantage provided by the proposed methods is confirmed by the results achieved on</font> four real world datasets: <font color=\"green\">an increase of up to 12 percentage points in</font> Accuracy and F1 scores <font color=\"green\">for</font> vertex-based semi-supervised classification <font color=\"green\"> and up to 2 percentage points in</font> Accuracy and F1 scores <font color=\"green\">for</font> graph-based wupervised classification. --&gt; 在四个真实世界数据集上取得的结果证实了所提出的方法提供的优势：基于顶点的半监督分类的准确率和 F1得分提高了12个百分点，基于图监督分类的准确率和 F1 得分提高了2个百分点。</li>\n</ul>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"摘抄论文佳句\"><a class=\"markdownIt-Anchor\" href=\"#摘抄论文佳句\"></a> 摘抄论文佳句</h1>\n<ul>\n<li>xxx <font color=green>have experienced significant attention and have become the de facto methods for</font> xxx. --&gt; xxx 受到了广泛的关注，并已成为xxx的实际方法。</li>\n<li><font color=green>building upon [cite]</font>, xxx propose to … --&gt; 在[引文]的基础上，xxx提出…</li>\n<li>xxx <font color=green>derive inspiration primarily from</font> xxx --&gt; xxx 主要从xxx中获得灵感。</li>\n<li><font color=green>including but not limited to</font> … --&gt; 包括但不限于…</li>\n<li><font color=green>In particular, SGC rivals, if not surpasses,</font> GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. --&gt; 特别是，SGC 在文本分类，用户地理定位，关系提取和 zero-shot 图像分类任务方面可以与基于 GCN 的方法竞争，甚至超越。</li>\n<li>More <font color=green>compactly</font>, --&gt; 更简洁地，</li>\n<li>features in the second layer <font color=green>build on top of the</font> features of the first layer. --&gt; 第二层特征是建立在第一层特征之上的。</li>\n<li>We hypothesize that the nonlinearity between GCN layers <font color=green>is not critical - but that the majority of the benefit arises from</font> the local averaging. --&gt; 我们假设 GCN 层之间的非线性不是至关重要的——但是大部分的好处来自于局部平均。</li>\n<li><font color=green>As one may observe, </font>–&gt; 可以看出，</li>\n<li><font color=green>In a follow-up work, </font> --&gt; 在后续工作中，</li>\n<li><font color=green>While dating back to at least </font> xxx, --&gt; 至少可以追溯到xxx，</li>\n<li>a k-nearest neighbor(kNN) graph <font color=green>is constructed on the fly in the</font> feature space of the neural network. --&gt; 一个 k 近邻（kNN）图是在神经网络的特征空间中动态构造的。</li>\n<li>The DGM <font color=green>is tasked with building</font> the (weighted) graph representing the input space. --&gt; DGM 的任务是构建表示输入空间的（加权）图。</li>\n<li><font color=green>The advantage provided by the proposed methods is confirmed by the results achieved on</font> four real world datasets: <font color=green>an increase of up to 12 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> vertex-based semi-supervised classification <font color=green> and up to 2 percentage points in</font> Accuracy and F1 scores <font color=green>for</font> graph-based wupervised classification. --&gt; 在四个真实世界数据集上取得的结果证实了所提出的方法提供的优势：基于顶点的半监督分类的准确率和 F1得分提高了12个百分点，基于图监督分类的准确率和 F1 得分提高了2个百分点。</li>\n</ul>\n"},{"title":"Monocular-Vision","top":false,"cover":false,"toc":true,"date":"2022-05-11T12:17:31.000Z","password":null,"summary":null,"description":null,"_content":"\n# 单目视觉研究方向\n\n单目视觉研究方向很多，包括但不限于：\n\n1）基于单目视觉的同时定位与地图构建方法；\n\n2）基于单目视觉的车距测量方法；\n\n3）基于单目视觉的定位方法；\n\n4）基于单目视觉的三维重建算法；\n\n5）基于单目视觉的深度估计方法；\n\n6）基于单目视觉的路面车辆检测及跟踪方法；\n\n7）基于单目视觉的道路图像理解……\n\n\n\n# 基于单目视觉的深度估计算法\n\n## 视觉系统\n\n单目视觉 --> 只是用一个视觉传感器（摄像机），其在成像过程中从三维客观世界投影到二维图像，损失了深度信息。单目视觉系统结构简单，算法成熟，计算量较小。\n\n双目立体视觉 --> 双目视觉系统由两个摄像机组成，利用 <font color=green>三角测量原理</font> 获得场景的深度信息，并可重建周围景物的三维形状和位置，类似于人眼功能。\n\n多目视觉 --> 多目视觉系统由三个及三个以上摄像机组成，主要用来解决双目立体视觉中匹配多义性问题，提高匹配精度。\n\n全景视觉 --> 具有较大水平视场的多方向成像系统，视场可达360度。全景视觉系统可通过图像拼接方式或者折反射光学元件实现。\n\n混合视觉系统 --> 由两种或两种以上视觉系统组成，\n\n## 深度估计\n\n\n\n> 深度估计可应用于机器人导航、增强现实、三维重建、自动驾驶等领域。\n>\n> <font color=green>目前，大部分深度估计都是基于二维RGB 图像到 RBG-D (RGB + depth map，depth map 类似于灰度图像，只是它的每个像素值是传感器距离物体的实际距离) 图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的 shape from X 方法，还有结合 SFM(structure from motion) 和 SLAM(simultaneous localization and mapping) 等方式预测相机位姿的算法。</font>\n\n\n\n> 直接用设备获取深度 --> 设备造假昂贵。\n>\n> 利用双目进行深度估计 --> 由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度较高，尤其对低纹理场景的匹配效果不好。\n>\n> 单目深度估计 --> 相对成本更低，更容易普及。\n\n\n\n> <font color=green>单目深度估计 </font>  --> 利用一张或者唯一视角下的 RGB 图像，估计图像中每个像素相对拍摄源的距离。\n\n\n\n> 单幅图像深度估计模型方法和数据集：\n>\n> 数据集 -->按场景类型可分为：室内数据集，室外数据集，虚拟场景数据集\n>\n> 模型方法 --> 按数学模型的不同分为：基于传统机器学习的方法，基于深度学习的方法。\n>\n> <font color = green>1. 基于传统机器学习的单目深度估计方法</font>\n>\n> 该方法一般使用马尔可夫随机场（MRF）或条件随机场（CRF）建模深度关系，在最大后验概率框架下，通过能量函数最小化求解深度。\n>\n> 基于传统机器学习的单目深度估计方法 --> 依据模型是否包含参数分为：参数学习方法，非参数学习方法。\n>\n> <font color = green>参数学习方法</font> --> 假定模型包含未知参数，训练过程是对未知参数求解的过程；\n>\n> <font color = green>非参数学习方法 </font>--> 使用现有数据集进行相似性检索推测深度，不需要通过学习获得参数。\n>\n> <font color = green>2. 基于深度学习的单目深度估计方法</font>\n>\n> 基于深度学习的单目深度估计算法可大致分为以下几类：\n>\n> 1）<font color = green>监督算法</font> --> 以 2 维图像作为输入，以深度图作为输出进行训练\n>\n> 2）<font color = green>无监督算法</font> --> 仅使用两个摄像机采集的双目图像数据进行联合训练。其双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或将双目图像中各个像素点的对应问题看作立体匹配问题进行训练。\n>\n> <font color = green>由于深度数据的获取难度较高，所以目前大量算法都是基于无监督模型的。</font>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Monocular-Vision.md","raw":"---\ntitle: Monocular-Vision\ntop: false\ncover: false\ntoc: true\ndate: 2022-05-11 20:17:31\npassword:\nsummary:\ndescription:\ncategories:\n- Monocular Vision\ntags:\n- Monocular Vision\n---\n\n# 单目视觉研究方向\n\n单目视觉研究方向很多，包括但不限于：\n\n1）基于单目视觉的同时定位与地图构建方法；\n\n2）基于单目视觉的车距测量方法；\n\n3）基于单目视觉的定位方法；\n\n4）基于单目视觉的三维重建算法；\n\n5）基于单目视觉的深度估计方法；\n\n6）基于单目视觉的路面车辆检测及跟踪方法；\n\n7）基于单目视觉的道路图像理解……\n\n\n\n# 基于单目视觉的深度估计算法\n\n## 视觉系统\n\n单目视觉 --> 只是用一个视觉传感器（摄像机），其在成像过程中从三维客观世界投影到二维图像，损失了深度信息。单目视觉系统结构简单，算法成熟，计算量较小。\n\n双目立体视觉 --> 双目视觉系统由两个摄像机组成，利用 <font color=green>三角测量原理</font> 获得场景的深度信息，并可重建周围景物的三维形状和位置，类似于人眼功能。\n\n多目视觉 --> 多目视觉系统由三个及三个以上摄像机组成，主要用来解决双目立体视觉中匹配多义性问题，提高匹配精度。\n\n全景视觉 --> 具有较大水平视场的多方向成像系统，视场可达360度。全景视觉系统可通过图像拼接方式或者折反射光学元件实现。\n\n混合视觉系统 --> 由两种或两种以上视觉系统组成，\n\n## 深度估计\n\n\n\n> 深度估计可应用于机器人导航、增强现实、三维重建、自动驾驶等领域。\n>\n> <font color=green>目前，大部分深度估计都是基于二维RGB 图像到 RBG-D (RGB + depth map，depth map 类似于灰度图像，只是它的每个像素值是传感器距离物体的实际距离) 图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的 shape from X 方法，还有结合 SFM(structure from motion) 和 SLAM(simultaneous localization and mapping) 等方式预测相机位姿的算法。</font>\n\n\n\n> 直接用设备获取深度 --> 设备造假昂贵。\n>\n> 利用双目进行深度估计 --> 由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度较高，尤其对低纹理场景的匹配效果不好。\n>\n> 单目深度估计 --> 相对成本更低，更容易普及。\n\n\n\n> <font color=green>单目深度估计 </font>  --> 利用一张或者唯一视角下的 RGB 图像，估计图像中每个像素相对拍摄源的距离。\n\n\n\n> 单幅图像深度估计模型方法和数据集：\n>\n> 数据集 -->按场景类型可分为：室内数据集，室外数据集，虚拟场景数据集\n>\n> 模型方法 --> 按数学模型的不同分为：基于传统机器学习的方法，基于深度学习的方法。\n>\n> <font color = green>1. 基于传统机器学习的单目深度估计方法</font>\n>\n> 该方法一般使用马尔可夫随机场（MRF）或条件随机场（CRF）建模深度关系，在最大后验概率框架下，通过能量函数最小化求解深度。\n>\n> 基于传统机器学习的单目深度估计方法 --> 依据模型是否包含参数分为：参数学习方法，非参数学习方法。\n>\n> <font color = green>参数学习方法</font> --> 假定模型包含未知参数，训练过程是对未知参数求解的过程；\n>\n> <font color = green>非参数学习方法 </font>--> 使用现有数据集进行相似性检索推测深度，不需要通过学习获得参数。\n>\n> <font color = green>2. 基于深度学习的单目深度估计方法</font>\n>\n> 基于深度学习的单目深度估计算法可大致分为以下几类：\n>\n> 1）<font color = green>监督算法</font> --> 以 2 维图像作为输入，以深度图作为输出进行训练\n>\n> 2）<font color = green>无监督算法</font> --> 仅使用两个摄像机采集的双目图像数据进行联合训练。其双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或将双目图像中各个像素点的对应问题看作立体匹配问题进行训练。\n>\n> <font color = green>由于深度数据的获取难度较高，所以目前大量算法都是基于无监督模型的。</font>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Monocular-Vision","published":1,"updated":"2022-06-09T12:50:43.865Z","_id":"cl40nn3300009mcularvafsm2","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h1 id=\"单目视觉研究方向\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#单目视觉研究方向\"></a> 单目视觉研究方向</h1>\n<p>单目视觉研究方向很多，包括但不限于：</p>\n<p>1）基于单目视觉的同时定位与地图构建方法；</p>\n<p>2）基于单目视觉的车距测量方法；</p>\n<p>3）基于单目视觉的定位方法；</p>\n<p>4）基于单目视觉的三维重建算法；</p>\n<p>5）基于单目视觉的深度估计方法；</p>\n<p>6）基于单目视觉的路面车辆检测及跟踪方法；</p>\n<p>7）基于单目视觉的道路图像理解……</p>\n<h1 id=\"基于单目视觉的深度估计算法\"><span class=\"post-title-index\">2. </span><a class=\"markdownIt-Anchor\" href=\"#基于单目视觉的深度估计算法\"></a> 基于单目视觉的深度估计算法</h1>\n<h2 id=\"视觉系统\"><span class=\"post-title-index\">2.1. </span><a class=\"markdownIt-Anchor\" href=\"#视觉系统\"></a> 视觉系统</h2>\n<p>单目视觉 --&gt; 只是用一个视觉传感器（摄像机），其在成像过程中从三维客观世界投影到二维图像，损失了深度信息。单目视觉系统结构简单，算法成熟，计算量较小。</p>\n<p>双目立体视觉 --&gt; 双目视觉系统由两个摄像机组成，利用 <font color=\"green\">三角测量原理</font> 获得场景的深度信息，并可重建周围景物的三维形状和位置，类似于人眼功能。</p>\n<p>多目视觉 --&gt; 多目视觉系统由三个及三个以上摄像机组成，主要用来解决双目立体视觉中匹配多义性问题，提高匹配精度。</p>\n<p>全景视觉 --&gt; 具有较大水平视场的多方向成像系统，视场可达360度。全景视觉系统可通过图像拼接方式或者折反射光学元件实现。</p>\n<p>混合视觉系统 --&gt; 由两种或两种以上视觉系统组成，</p>\n<h2 id=\"深度估计\"><span class=\"post-title-index\">2.2. </span><a class=\"markdownIt-Anchor\" href=\"#深度估计\"></a> 深度估计</h2>\n<blockquote>\n<p>深度估计可应用于机器人导航、增强现实、三维重建、自动驾驶等领域。</p>\n<p><font color=\"green\">目前，大部分深度估计都是基于二维RGB 图像到 RBG-D (RGB + depth map，depth map 类似于灰度图像，只是它的每个像素值是传感器距离物体的实际距离) 图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的 shape from X 方法，还有结合 SFM(structure from motion) 和 SLAM(simultaneous localization and mapping) 等方式预测相机位姿的算法。</font></p>\n</blockquote>\n<blockquote>\n<p>直接用设备获取深度 --&gt; 设备造假昂贵。</p>\n<p>利用双目进行深度估计 --&gt; 由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度较高，尤其对低纹理场景的匹配效果不好。</p>\n<p>单目深度估计 --&gt; 相对成本更低，更容易普及。</p>\n</blockquote>\n<blockquote>\n<p><font color=\"green\">单目深度估计 </font>  --&gt; 利用一张或者唯一视角下的 RGB 图像，估计图像中每个像素相对拍摄源的距离。</p>\n</blockquote>\n<blockquote>\n<p>单幅图像深度估计模型方法和数据集：</p>\n<p>数据集 --&gt;按场景类型可分为：室内数据集，室外数据集，虚拟场景数据集</p>\n<p>模型方法 --&gt; 按数学模型的不同分为：基于传统机器学习的方法，基于深度学习的方法。</p>\n<p><font color=\"green\">1. 基于传统机器学习的单目深度估计方法</font></p>\n<p>该方法一般使用马尔可夫随机场（MRF）或条件随机场（CRF）建模深度关系，在最大后验概率框架下，通过能量函数最小化求解深度。</p>\n<p>基于传统机器学习的单目深度估计方法 --&gt; 依据模型是否包含参数分为：参数学习方法，非参数学习方法。</p>\n<p><font color=\"green\">参数学习方法</font> --&gt; 假定模型包含未知参数，训练过程是对未知参数求解的过程；</p>\n<p><font color=\"green\">非参数学习方法 </font>–&gt; 使用现有数据集进行相似性检索推测深度，不需要通过学习获得参数。</p>\n<p><font color=\"green\">2. 基于深度学习的单目深度估计方法</font></p>\n<p>基于深度学习的单目深度估计算法可大致分为以下几类：</p>\n<p>1）<font color=\"green\">监督算法</font> --&gt; 以 2 维图像作为输入，以深度图作为输出进行训练</p>\n<p>2）<font color=\"green\">无监督算法</font> --&gt; 仅使用两个摄像机采集的双目图像数据进行联合训练。其双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或将双目图像中各个像素点的对应问题看作立体匹配问题进行训练。</p>\n<p><font color=\"green\">由于深度数据的获取难度较高，所以目前大量算法都是基于无监督模型的。</font></p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"单目视觉研究方向\"><a class=\"markdownIt-Anchor\" href=\"#单目视觉研究方向\"></a> 单目视觉研究方向</h1>\n<p>单目视觉研究方向很多，包括但不限于：</p>\n<p>1）基于单目视觉的同时定位与地图构建方法；</p>\n<p>2）基于单目视觉的车距测量方法；</p>\n<p>3）基于单目视觉的定位方法；</p>\n<p>4）基于单目视觉的三维重建算法；</p>\n<p>5）基于单目视觉的深度估计方法；</p>\n<p>6）基于单目视觉的路面车辆检测及跟踪方法；</p>\n<p>7）基于单目视觉的道路图像理解……</p>\n<h1 id=\"基于单目视觉的深度估计算法\"><a class=\"markdownIt-Anchor\" href=\"#基于单目视觉的深度估计算法\"></a> 基于单目视觉的深度估计算法</h1>\n<h2 id=\"视觉系统\"><a class=\"markdownIt-Anchor\" href=\"#视觉系统\"></a> 视觉系统</h2>\n<p>单目视觉 --&gt; 只是用一个视觉传感器（摄像机），其在成像过程中从三维客观世界投影到二维图像，损失了深度信息。单目视觉系统结构简单，算法成熟，计算量较小。</p>\n<p>双目立体视觉 --&gt; 双目视觉系统由两个摄像机组成，利用 <font color=green>三角测量原理</font> 获得场景的深度信息，并可重建周围景物的三维形状和位置，类似于人眼功能。</p>\n<p>多目视觉 --&gt; 多目视觉系统由三个及三个以上摄像机组成，主要用来解决双目立体视觉中匹配多义性问题，提高匹配精度。</p>\n<p>全景视觉 --&gt; 具有较大水平视场的多方向成像系统，视场可达360度。全景视觉系统可通过图像拼接方式或者折反射光学元件实现。</p>\n<p>混合视觉系统 --&gt; 由两种或两种以上视觉系统组成，</p>\n<h2 id=\"深度估计\"><a class=\"markdownIt-Anchor\" href=\"#深度估计\"></a> 深度估计</h2>\n<blockquote>\n<p>深度估计可应用于机器人导航、增强现实、三维重建、自动驾驶等领域。</p>\n<p><font color=green>目前，大部分深度估计都是基于二维RGB 图像到 RBG-D (RGB + depth map，depth map 类似于灰度图像，只是它的每个像素值是传感器距离物体的实际距离) 图像的转化估计，主要包括从图像明暗、不同视角、光度、纹理信息等获取场景深度形状的 shape from X 方法，还有结合 SFM(structure from motion) 和 SLAM(simultaneous localization and mapping) 等方式预测相机位姿的算法。</font></p>\n</blockquote>\n<blockquote>\n<p>直接用设备获取深度 --&gt; 设备造假昂贵。</p>\n<p>利用双目进行深度估计 --&gt; 由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度较高，尤其对低纹理场景的匹配效果不好。</p>\n<p>单目深度估计 --&gt; 相对成本更低，更容易普及。</p>\n</blockquote>\n<blockquote>\n<p><font color=green>单目深度估计 </font>  --&gt; 利用一张或者唯一视角下的 RGB 图像，估计图像中每个像素相对拍摄源的距离。</p>\n</blockquote>\n<blockquote>\n<p>单幅图像深度估计模型方法和数据集：</p>\n<p>数据集 --&gt;按场景类型可分为：室内数据集，室外数据集，虚拟场景数据集</p>\n<p>模型方法 --&gt; 按数学模型的不同分为：基于传统机器学习的方法，基于深度学习的方法。</p>\n<p><font color = green>1. 基于传统机器学习的单目深度估计方法</font></p>\n<p>该方法一般使用马尔可夫随机场（MRF）或条件随机场（CRF）建模深度关系，在最大后验概率框架下，通过能量函数最小化求解深度。</p>\n<p>基于传统机器学习的单目深度估计方法 --&gt; 依据模型是否包含参数分为：参数学习方法，非参数学习方法。</p>\n<p><font color = green>参数学习方法</font> --&gt; 假定模型包含未知参数，训练过程是对未知参数求解的过程；</p>\n<p><font color = green>非参数学习方法 </font>–&gt; 使用现有数据集进行相似性检索推测深度，不需要通过学习获得参数。</p>\n<p><font color = green>2. 基于深度学习的单目深度估计方法</font></p>\n<p>基于深度学习的单目深度估计算法可大致分为以下几类：</p>\n<p>1）<font color = green>监督算法</font> --&gt; 以 2 维图像作为输入，以深度图作为输出进行训练</p>\n<p>2）<font color = green>无监督算法</font> --&gt; 仅使用两个摄像机采集的双目图像数据进行联合训练。其双目数据可彼此预测对方，从而获得相应的视差数据，再根据视差与深度的关系进行演化。亦或将双目图像中各个像素点的对应问题看作立体匹配问题进行训练。</p>\n<p><font color = green>由于深度数据的获取难度较高，所以目前大量算法都是基于无监督模型的。</font></p>\n</blockquote>\n"},{"title":"Paper Reading","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2022-01-05T10:01:04.000Z","password":null,"summary":null,"description":null,"_content":"\n# Differentiable Graph Module(DGM)for Graph Convolutional Networks_PAMI-2022\n\n\n\n> 这篇文章基于每一层的输出特征来学习 graph，并在训练过程中优化网络参数。     所提出的架构包括 Differentiable Graph Module(DGM) 和 Diffusion Module 两个模块。\n\n> latent graph: \n>\n> In many problems, the data can be assumed to have some underlying graph structure, however, the graph itself might not be explicitly given, a setting we refer to as latent graph.\n\n## Differentiable Graph Module\n\n<font color=green>**DGM --> 构建表示输入空间的 weighted graph。**</font>即：\n\n> 输入：feature matrix X $\\in$ $\\mathcal{R}^{N \\times d}$ 或 initial graph $\\mathcal{G_0}$\n>\n> 输出：graph $\\mathcal{G}$\n\n> DGM 由两部分组成：\n>\n> ① 将输入特征转换为辅助特征 auxiliary features；\n>\n> ② 用辅助特征构造 graph。\n\n> ① 将输入特征转换为辅助特征：\n>\n> 输入特征 $X \\in \\mathcal{R}^{N \\times d}$\n>\n> 辅助特征 $\\hat{X}=\\mathcal{f}_\\theta(x) \\in \\mathcal{R}^{N \\times \\hat{d}}$ \n>\n> $$ \\hat{X}=\\mathcal{f}_\\theta(x) \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 对 \\mathcal{G_0} 进行 edge-/graph-convolution 得到     &        & {if \\mathcal{G_0}已知}\\\\\n>\n> \\mathcal{f_{\\theta}} 独立应用于每个节点特征，按行作用于矩阵 X     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Diffusion Module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Dynamic Graph Convolutional Networks_PR-2020\n\ndynamic graph --> 指每个 graph 的顶点 / 边随时间的变化而变化。\n\n面对很多不同的分类任务，首先需要对结构数据(structured data) 进行处理，通常的处理方式是 --> 将这些结构数据建模为 graphs。\n\n而针对那些顶点 / 边随时间的变化而变化的 dynamic graph 来说，目标则是 --> 利用现有的神经网络将这些数据集建模为随时间变化而变化的图结构(graph structures) 。--> 由于使用现有的架构不能解决上述目标，所以作者提出两种方法来实现这个目标，即：结合 Long Short-Term Memory networks 和 Graph Convolutional Networks 来学习长短期依赖关系和图结构。\n\n论文灵感：\n\n① GCNs 能有效解决图结构(graph-structured)信息，但缺乏处理随时间变化而变化的数据结构的能力，即：不能处理动态节点特征 + 不能处理动态边连接。\n\n② LSTMs 擅长发现长范围和短范围的序列依赖关系，但缺乏显示利用图结构信息的能力。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Paper-Reading.md","raw":"---\ntitle: Paper Reading\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2022-01-05 18:01:04\npassword:\nsummary:\ndescription:\ncategories:\n- Papers\ntags:\n---\n\n# Differentiable Graph Module(DGM)for Graph Convolutional Networks_PAMI-2022\n\n\n\n> 这篇文章基于每一层的输出特征来学习 graph，并在训练过程中优化网络参数。     所提出的架构包括 Differentiable Graph Module(DGM) 和 Diffusion Module 两个模块。\n\n> latent graph: \n>\n> In many problems, the data can be assumed to have some underlying graph structure, however, the graph itself might not be explicitly given, a setting we refer to as latent graph.\n\n## Differentiable Graph Module\n\n<font color=green>**DGM --> 构建表示输入空间的 weighted graph。**</font>即：\n\n> 输入：feature matrix X $\\in$ $\\mathcal{R}^{N \\times d}$ 或 initial graph $\\mathcal{G_0}$\n>\n> 输出：graph $\\mathcal{G}$\n\n> DGM 由两部分组成：\n>\n> ① 将输入特征转换为辅助特征 auxiliary features；\n>\n> ② 用辅助特征构造 graph。\n\n> ① 将输入特征转换为辅助特征：\n>\n> 输入特征 $X \\in \\mathcal{R}^{N \\times d}$\n>\n> 辅助特征 $\\hat{X}=\\mathcal{f}_\\theta(x) \\in \\mathcal{R}^{N \\times \\hat{d}}$ \n>\n> $$ \\hat{X}=\\mathcal{f}_\\theta(x) \\left\\{\n>\n> \\begin{array}{rcl}\n>\n> 对 \\mathcal{G_0} 进行 edge-/graph-convolution 得到     &        & {if \\mathcal{G_0}已知}\\\\\n>\n> \\mathcal{f_{\\theta}} 独立应用于每个节点特征，按行作用于矩阵 X     &        &{otherwise}\n>\n> \\end{array} \\right. $$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Diffusion Module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Dynamic Graph Convolutional Networks_PR-2020\n\ndynamic graph --> 指每个 graph 的顶点 / 边随时间的变化而变化。\n\n面对很多不同的分类任务，首先需要对结构数据(structured data) 进行处理，通常的处理方式是 --> 将这些结构数据建模为 graphs。\n\n而针对那些顶点 / 边随时间的变化而变化的 dynamic graph 来说，目标则是 --> 利用现有的神经网络将这些数据集建模为随时间变化而变化的图结构(graph structures) 。--> 由于使用现有的架构不能解决上述目标，所以作者提出两种方法来实现这个目标，即：结合 Long Short-Term Memory networks 和 Graph Convolutional Networks 来学习长短期依赖关系和图结构。\n\n论文灵感：\n\n① GCNs 能有效解决图结构(graph-structured)信息，但缺乏处理随时间变化而变化的数据结构的能力，即：不能处理动态节点特征 + 不能处理动态边连接。\n\n② LSTMs 擅长发现长范围和短范围的序列依赖关系，但缺乏显示利用图结构信息的能力。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Paper-Reading","published":1,"updated":"2022-06-05T01:55:31.544Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl40nn333000cmcul9bwk4dzs","content":"<html><head></head><body><h1 id=\"differentiable-graph-moduledgmfor-graph-convolutional-networks_pami-2022\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#differentiable-graph-moduledgmfor-graph-convolutional-networks_pami-2022\"></a> Differentiable Graph Module(DGM)for Graph Convolutional Networks_PAMI-2022</h1>\n<blockquote>\n<p>这篇文章基于每一层的输出特征来学习 graph，并在训练过程中优化网络参数。     所提出的架构包括 Differentiable Graph Module(DGM) 和 Diffusion Module 两个模块。</p>\n</blockquote>\n<blockquote>\n<p>latent graph:</p>\n<p>In many problems, the data can be assumed to have some underlying graph structure, however, the graph itself might not be explicitly given, a setting we refer to as latent graph.</p>\n</blockquote>\n<h2 id=\"differentiable-graph-module\"><span class=\"post-title-index\">1.1. </span><a class=\"markdownIt-Anchor\" href=\"#differentiable-graph-module\"></a> Differentiable Graph Module</h2>\n<p><font color=\"green\"><strong>DGM --&gt; 构建表示输入空间的 weighted graph。</strong></font>即：</p>\n<blockquote>\n<p>输入：feature matrix X <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>∈</mo></mrow><annotation encoding=\"application/x-tex\">\\in</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mrel\">∈</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{R}^{N \\times d}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8491079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\">d</span></span></span></span></span></span></span></span></span></span></span></span> 或 initial graph <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{G_0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>输出：graph <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.78055em;vertical-align:-0.09722em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span></span></span></span></span></p>\n</blockquote>\n<blockquote>\n<p>DGM 由两部分组成：</p>\n<p>① 将输入特征转换为辅助特征 auxiliary features；</p>\n<p>② 用辅助特征构造 graph。</p>\n</blockquote>\n<blockquote>\n<p>① 将输入特征转换为辅助特征：</p>\n<p>输入特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X \\in \\mathcal{R}^{N \\times d}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\">d</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>辅助特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\hat{X}=\\mathcal{f}_\\theta(x) \\in \\mathcal{R}^{N \\times \\hat{d}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0335159999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0335159999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord accent mtight\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-2.7em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span><span style=\"top:-2.9634400000000003em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\"><span class=\"mtight\">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi mathvariant=\"normal\">对</mi><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub><mi mathvariant=\"normal\">进</mi><mi mathvariant=\"normal\">行</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>e</mi><mo>−</mo><mi mathvariant=\"normal\">/</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>p</mi><mi>h</mi><mo>−</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>o</mi><mi>l</mi><mi>u</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant=\"normal\">得</mi><mi mathvariant=\"normal\">到</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub><mi mathvariant=\"normal\">已</mi><mi mathvariant=\"normal\">知</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><msub><mi>f</mi><mi>θ</mi></msub><mi mathvariant=\"normal\">独</mi><mi mathvariant=\"normal\">立</mi><mi mathvariant=\"normal\">应</mi><mi mathvariant=\"normal\">用</mi><mi mathvariant=\"normal\">于</mi><mi mathvariant=\"normal\">每</mi><mi mathvariant=\"normal\">个</mi><mi mathvariant=\"normal\">节</mi><mi mathvariant=\"normal\">点</mi><mi mathvariant=\"normal\">特</mi><mi mathvariant=\"normal\">征</mi><mi mathvariant=\"normal\">，</mi><mi mathvariant=\"normal\">按</mi><mi mathvariant=\"normal\">行</mi><mi mathvariant=\"normal\">作</mi><mi mathvariant=\"normal\">用</mi><mi mathvariant=\"normal\">于</mi><mi mathvariant=\"normal\">矩</mi><mi mathvariant=\"normal\">阵</mi><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> \\hat{X}=\\mathcal{f}_\\theta(x) \\left\\{\n\n\\begin{array}{rcl}\n\n对 \\mathcal{G_0} 进行 edge-/graph-convolution 得到     &amp;        &amp; {if \\mathcal{G_0}已知}\\\\\n\n\\mathcal{f_{\\theta}} 独立应用于每个节点特征，按行作用于矩阵 X     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord cjk_fallback\">对</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">进</span><span class=\"mord cjk_fallback\">行</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">e</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">/</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">h</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mord cjk_fallback\">得</span><span class=\"mord cjk_fallback\">到</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">独</span><span class=\"mord cjk_fallback\">立</span><span class=\"mord cjk_fallback\">应</span><span class=\"mord cjk_fallback\">用</span><span class=\"mord cjk_fallback\">于</span><span class=\"mord cjk_fallback\">每</span><span class=\"mord cjk_fallback\">个</span><span class=\"mord cjk_fallback\">节</span><span class=\"mord cjk_fallback\">点</span><span class=\"mord cjk_fallback\">特</span><span class=\"mord cjk_fallback\">征</span><span class=\"mord cjk_fallback\">，</span><span class=\"mord cjk_fallback\">按</span><span class=\"mord cjk_fallback\">行</span><span class=\"mord cjk_fallback\">作</span><span class=\"mord cjk_fallback\">用</span><span class=\"mord cjk_fallback\">于</span><span class=\"mord cjk_fallback\">矩</span><span class=\"mord cjk_fallback\">阵</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">已</span><span class=\"mord cjk_fallback\">知</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n</blockquote>\n<h2 id=\"diffusion-module\"><span class=\"post-title-index\">1.2. </span><a class=\"markdownIt-Anchor\" href=\"#diffusion-module\"></a> Diffusion Module</h2>\n<h1 id=\"dynamic-graph-convolutional-networks_pr-2020\"><span class=\"post-title-index\">2. </span><a class=\"markdownIt-Anchor\" href=\"#dynamic-graph-convolutional-networks_pr-2020\"></a> Dynamic Graph Convolutional Networks_PR-2020</h1>\n<p>dynamic graph --&gt; 指每个 graph 的顶点 / 边随时间的变化而变化。</p>\n<p>面对很多不同的分类任务，首先需要对结构数据(structured data) 进行处理，通常的处理方式是 --&gt; 将这些结构数据建模为 graphs。</p>\n<p>而针对那些顶点 / 边随时间的变化而变化的 dynamic graph 来说，目标则是 --&gt; 利用现有的神经网络将这些数据集建模为随时间变化而变化的图结构(graph structures) 。–&gt; 由于使用现有的架构不能解决上述目标，所以作者提出两种方法来实现这个目标，即：结合 Long Short-Term Memory networks 和 Graph Convolutional Networks 来学习长短期依赖关系和图结构。</p>\n<p>论文灵感：</p>\n<p>① GCNs 能有效解决图结构(graph-structured)信息，但缺乏处理随时间变化而变化的数据结构的能力，即：不能处理动态节点特征 + 不能处理动态边连接。</p>\n<p>② LSTMs 擅长发现长范围和短范围的序列依赖关系，但缺乏显示利用图结构信息的能力。</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"differentiable-graph-moduledgmfor-graph-convolutional-networks_pami-2022\"><a class=\"markdownIt-Anchor\" href=\"#differentiable-graph-moduledgmfor-graph-convolutional-networks_pami-2022\"></a> Differentiable Graph Module(DGM)for Graph Convolutional Networks_PAMI-2022</h1>\n<blockquote>\n<p>这篇文章基于每一层的输出特征来学习 graph，并在训练过程中优化网络参数。     所提出的架构包括 Differentiable Graph Module(DGM) 和 Diffusion Module 两个模块。</p>\n</blockquote>\n<blockquote>\n<p>latent graph:</p>\n<p>In many problems, the data can be assumed to have some underlying graph structure, however, the graph itself might not be explicitly given, a setting we refer to as latent graph.</p>\n</blockquote>\n<h2 id=\"differentiable-graph-module\"><a class=\"markdownIt-Anchor\" href=\"#differentiable-graph-module\"></a> Differentiable Graph Module</h2>\n<p><font color=green><strong>DGM --&gt; 构建表示输入空间的 weighted graph。</strong></font>即：</p>\n<blockquote>\n<p>输入：feature matrix X <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>∈</mo></mrow><annotation encoding=\"application/x-tex\">\\in</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mrel\">∈</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{R}^{N \\times d}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8491079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\">d</span></span></span></span></span></span></span></span></span></span></span></span> 或 initial graph <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{G_0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n<p>输出：graph <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">G</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.78055em;vertical-align:-0.09722em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span></span></span></span></span></p>\n</blockquote>\n<blockquote>\n<p>DGM 由两部分组成：</p>\n<p>① 将输入特征转换为辅助特征 auxiliary features；</p>\n<p>② 用辅助特征构造 graph。</p>\n</blockquote>\n<blockquote>\n<p>① 将输入特征转换为辅助特征：</p>\n<p>输入特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X \\in \\mathcal{R}^{N \\times d}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\">d</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>辅助特征 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mover accent=\"true\"><mi>d</mi><mo>^</mo></mover></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\hat{X}=\\mathcal{f}_\\theta(x) \\in \\mathcal{R}^{N \\times \\hat{d}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0335159999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0335159999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord accent mtight\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-2.7em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span><span style=\"top:-2.9634400000000003em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"accent-body\" style=\"left:-0.08332999999999999em;\"><span class=\"mtight\">^</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"right center left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi mathvariant=\"normal\">对</mi><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub><mi mathvariant=\"normal\">进</mi><mi mathvariant=\"normal\">行</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>e</mi><mo>−</mo><mi mathvariant=\"normal\">/</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>p</mi><mi>h</mi><mo>−</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>o</mi><mi>l</mi><mi>u</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant=\"normal\">得</mi><mi mathvariant=\"normal\">到</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mi>f</mi><msub><mi mathvariant=\"script\">G</mi><mn mathvariant=\"script\">0</mn></msub><mi mathvariant=\"normal\">已</mi><mi mathvariant=\"normal\">知</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><msub><mi>f</mi><mi>θ</mi></msub><mi mathvariant=\"normal\">独</mi><mi mathvariant=\"normal\">立</mi><mi mathvariant=\"normal\">应</mi><mi mathvariant=\"normal\">用</mi><mi mathvariant=\"normal\">于</mi><mi mathvariant=\"normal\">每</mi><mi mathvariant=\"normal\">个</mi><mi mathvariant=\"normal\">节</mi><mi mathvariant=\"normal\">点</mi><mi mathvariant=\"normal\">特</mi><mi mathvariant=\"normal\">征</mi><mi mathvariant=\"normal\">，</mi><mi mathvariant=\"normal\">按</mi><mi mathvariant=\"normal\">行</mi><mi mathvariant=\"normal\">作</mi><mi mathvariant=\"normal\">用</mi><mi mathvariant=\"normal\">于</mi><mi mathvariant=\"normal\">矩</mi><mi mathvariant=\"normal\">阵</mi><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\"> \\hat{X}=\\mathcal{f}_\\theta(x) \\left\\{\n\n\\begin{array}{rcl}\n\n对 \\mathcal{G_0} 进行 edge-/graph-convolution 得到     &amp;        &amp; {if \\mathcal{G_0}已知}\\\\\n\n\\mathcal{f_{\\theta}} 独立应用于每个节点特征，按行作用于矩阵 X     &amp;        &amp;{otherwise}\n\n\\end{array} \\right. </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord cjk_fallback\">对</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">进</span><span class=\"mord cjk_fallback\">行</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">e</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">/</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">h</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mord cjk_fallback\">得</span><span class=\"mord cjk_fallback\">到</span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">独</span><span class=\"mord cjk_fallback\">立</span><span class=\"mord cjk_fallback\">应</span><span class=\"mord cjk_fallback\">用</span><span class=\"mord cjk_fallback\">于</span><span class=\"mord cjk_fallback\">每</span><span class=\"mord cjk_fallback\">个</span><span class=\"mord cjk_fallback\">节</span><span class=\"mord cjk_fallback\">点</span><span class=\"mord cjk_fallback\">特</span><span class=\"mord cjk_fallback\">征</span><span class=\"mord cjk_fallback\">，</span><span class=\"mord cjk_fallback\">按</span><span class=\"mord cjk_fallback\">行</span><span class=\"mord cjk_fallback\">作</span><span class=\"mord cjk_fallback\">用</span><span class=\"mord cjk_fallback\">于</span><span class=\"mord cjk_fallback\">矩</span><span class=\"mord cjk_fallback\">阵</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.4499999999999997em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span><span style=\"top:-2.2499999999999996em;\"><span class=\"pstrut\" style=\"height:2.84em;\"></span><span class=\"mord\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.0593em;\">G</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0593em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathcal mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span><span class=\"mord cjk_fallback\">已</span><span class=\"mord cjk_fallback\">知</span></span></span></span><span style=\"top:-2.4099999999999997em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500000000000004em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n</blockquote>\n<h2 id=\"diffusion-module\"><a class=\"markdownIt-Anchor\" href=\"#diffusion-module\"></a> Diffusion Module</h2>\n<h1 id=\"dynamic-graph-convolutional-networks_pr-2020\"><a class=\"markdownIt-Anchor\" href=\"#dynamic-graph-convolutional-networks_pr-2020\"></a> Dynamic Graph Convolutional Networks_PR-2020</h1>\n<p>dynamic graph --&gt; 指每个 graph 的顶点 / 边随时间的变化而变化。</p>\n<p>面对很多不同的分类任务，首先需要对结构数据(structured data) 进行处理，通常的处理方式是 --&gt; 将这些结构数据建模为 graphs。</p>\n<p>而针对那些顶点 / 边随时间的变化而变化的 dynamic graph 来说，目标则是 --&gt; 利用现有的神经网络将这些数据集建模为随时间变化而变化的图结构(graph structures) 。–&gt; 由于使用现有的架构不能解决上述目标，所以作者提出两种方法来实现这个目标，即：结合 Long Short-Term Memory networks 和 Graph Convolutional Networks 来学习长短期依赖关系和图结构。</p>\n<p>论文灵感：</p>\n<p>① GCNs 能有效解决图结构(graph-structured)信息，但缺乏处理随时间变化而变化的数据结构的能力，即：不能处理动态节点特征 + 不能处理动态边连接。</p>\n<p>② LSTMs 擅长发现长范围和短范围的序列依赖关系，但缺乏显示利用图结构信息的能力。</p>\n"},{"title":"CNN-Regularization","top":false,"cover":false,"toc":true,"date":"2022-06-10T07:55:36.000Z","password":null,"summary":null,"description":null,"_content":"\n\n\n# 正则化\n\n> 过拟合 --> 在训练集上表现很好，但在测试集上效果不佳。（随着模型复杂度增加，训练误差减小，测试误差不减）\n>\n> 发生过拟合时，模型的偏差小而方差大。\n\n> 正则化 --> 解决模型过拟合问题！\n>\n> 正则化通过对学习算法进行微调，使得该模型具有更好的泛化能力，改善模型在未知数据上的表现。\n\n\n\n> **[过拟合](https://zh.m.wikipedia.org/zh-hans/%E9%81%8E%E9%81%A9 \"什么是过拟合？\")**\n>\n> **<font color = green>过拟合的本质是训练算法从统计噪声中获取了信息并表达在了模型结构的参数当中。</font>**\n>\n> 过拟合 --> 指过于紧密或精确地匹配训练集数据，以致于无法良好地拟合测试集数据 --> **<font color = green>过拟合一般可视为违反奥卡姆剃刀原理（简约法则，若无必要，勿增实体）</font>**\n>\n> **<font color = green>过拟合存在的原因 --> 选择模型的标准和评价模型的标准不一致导致的。</font>** 选择模型时往往选取在训练数据上表现最好的模型；而评价模型时则是观察模型在不可见数据上的表现。当模型尝试“记住”训练数据而非从训练数据中学习规律时，就可能发生过拟合。\n>\n> 在统计学系和机器学习中，为了避免或减轻过拟合，可使用以下技巧：\n>\n> <font color = green>$\\blacksquare$ 模型选择 Model Selection</font>  --> 给定数据的情况下，从一组模型中选出最优模型（或具有代表性的模型）的过程。\n>\n> <font color = green>$\\blacksquare$ 交叉验证 Cross-Validation</font>-->  一种预测模型拟合性能的方法。包括 Leave-one-out Cross-Validation 和 K-fold Cross Validation。\n>\n> <font color = green>$\\blacksquare$  提前停止 Early Stopping</font>  --> 当训练集上的 loss 不再减小（减小的程度小于某个阈值）时停止继续训练，即用于提前停止训练的回调函数callbacks。\n>\n> **<font color = green>$\\blacksquare$  正则化 Regularization</font>** -->  机器学习和逆问题领域中，**<font color = green>正则化</font>** 是指为解决适定性问题或过拟合而加入额外信息的过程。\n>\n> <font color = green>$\\blacksquare$  剪枝 Pruning</font> -->  机器学习和搜索算法中，通过移除决策树中分辨能力较弱的部分而减小决策树大小的方法，其降低了模型的复杂度，因此能够降低过拟合风险，从而降低泛化误差。\n>\n> <font color = green>$\\blacksquare$ 贝叶斯信息量准则 Bayesian Information Criterion / Schwarz Information Criterion</font>  -->  在有限集合中进行模型选择的准则。\n>\n> <font color = green>$\\blacksquare$  赤池信息量准则 Akaike Information Criterion</font> -->  基于信息熵，用于评估统计模型的复杂度和衡量统计模型拟合资料的优良性的一种标准。\n>\n> <font color = green>$\\blacksquare$  dropout</font>  -->  Hinton 提出的一种正则化方法，即在神经网络训练过程中，通过随机丢弃部分神经元，来减小神经元之间的协同适应性，从而降低网络过拟合风险。\n\n\n\n> **[深度学习中的正则化策略](https://zhuanlan.zhihu.com/p/37120298 \"正则化？\")**\n>\n> 正则化 --> 深度学习中，正则化是惩罚每个节点的权重矩阵。\n>\n> 用于深度学习的正则化技巧：\n>\n> <font color = green> L1 & L2 正则化 </font>  --> 均是在损失函数 cost function 中增加一个正则项，即：\n>\n> $$ Cost function = Loss(say, binary_{cross entropy}) + Regularization_{term} $$\n\n\n\n# Regularization For Deep Learning: A Taxonomy_2017\n\n> 正则化的定义很多，作者提出一个系统的，统一的分类方法将现有的正则化方法进行分类，并为开发人员提供了实用的正则化方法的建议。\n>\n> 作者将目前的正则化方法分类为 affect data 影响数据、network architectures 网络架构、error terms 错误项、regularization terms 正则化项、optimization procedures 优化过程 这几种方法。\n>\n> 在<font color = green>传统</font>意义上的优化和<font color = green>较老</font>的神经网络文献中，<font color = green>正则化只用于损失函数中的惩罚项</font>。\n>\n> 2016年 Goodfellow 等人 将正则化广泛定义为：<font color = green>为减少模型的测试误差，而非训练误差，对学习算法所作的任何修改。</font>即，正则化被定义为：\n>\n> **<font color = green>任何使模型能够更好地泛化的辅助技术，即在测试集上产生更好效果的技术都被称为正则化。</font>**--> 该定义更符合机器学习文献，而非逆问题文献。可包括<font color = green>损失函数的各种属性，损失优化算法或其他技术。</font>\n>\n>  \n\n> 为了为接下来提出的分类法的顶层提供一个证明，作者梳理了机器学习的理论框架。\n>\n> ## 理论框架\n>\n> 机器学习的中心任务是 <font color = green> 模型拟合：找到一个函数 $f$，它能很好地近似从输入 $x$ 到期望输出 $f(x)$ 的期望映射。</font>\n>\n> 很多应用中，<font color = green>神经网络已被证明是一个选择 $f$ 的很好的函数族。</font>\n>\n> 一个神经网络是一个具有可训练权值 $w \\in W$的函数 $f_w : x --> y$。\n>\n> <font color = green>训练网络意味着找到一个使损失函数 $L$ 最小的权重配置 $w^*$ ：\n>\n> <img src=1.png width=70% />\n>\n> 通常损失函数采用期望风险的形式：\n>\n> <img src=2.png width=70% />\n>\n> 其中包含两部分：<font color = green>误差函数$E$和正则化项$R$。</font>\n>\n> **<font color = green>误差函数 --> 依赖于目标，并根据其与目标的一致性对模型预测分配惩罚。</font>**\n>\n> **<font color = green>正则化项 --> 根据其他标准对模型进行惩罚。这个标准可以是除了目标以外的任何东西，例如权重。</font>**\n>\n> 由于数据分布 $P$ 是未知的，所以根据公式（2）期望风险不能直接降到最低。相反，给出了从分布中采样的训练集 $D$。**<font color = green>期望风险的最小化可以通过最小化经验风险 $\\mathcal{\\hat{L}}$ 得到。</font>**\n>\n> <img src=3.png width=70% />\n>\n> 其中，$(x_i, t_i)$ 是来自训练集 $D$ 的样本。\n>\n> 公式（3）给出了最小化经验风险，作者根据公式中的元素，将正则化方法分为以下几类：\n>\n> $\\blacksquare$ $\\mathcal{D}$：训练集 --> affect data 影响数据\n>\n> $\\blacksquare$ $\\mathcal{f}$：选择的模型族 --> network architectures 网络架构\n>\n> $\\blacksquare$ *$E$*：错误函数 --> error terms 错误项\n>\n> $\\blacksquare$ *$R$*：正则化项 --> regularization terms 正则化项\n>\n> $\\blacksquare$   优化过程本身 --> optimization procedures 优化过程\n\n\n\n## 1 通过数据进行正则化\n\n>训练模型的质量很大程度取决于训练数据。\n>\n><font color = green>通过对训练集 $\\mathcal{D}$ 应用一些变换 生成一个新的数据集，从而实现对数据的正则化。</font>\n>\n>进行数据正则化可根据以下俩原则：\n>\n>1）进行特征提取或预处理，将特征空间或数据分布修改为某种表示，从而简化学习任务；\n>\n>2）允许生成新样本来创建更大的、可能是无限的增强数据集。\n>\n>这两个原则在某种程度上是独立的，也可相结合。它们均依赖于（随机）参数的转换：\n>\n><img src=D2.png width=70% />\n>\n>作者给出第二个定义：\n>\n>**<font color = green>带有随机参数的变换是一个函数 $\\tau_{\\theta}$，其参数 $\\theta$ 遵循某种概率分布。</font>**\n>\n>所以，在此情况下，考虑 $\\tau_{\\theta}$ 可作用于 <font color = green>网络输入、隐层激活或目标。</font>  输入被高斯噪声破坏<font color = green>（给输入数据添加高斯噪声）</font>是随机参数变换的一个例子。\n>\n><img src=4.png width=70% />\n>\n>**<font color = green>变换参数的随机性带来新样本的产生，即 data augmentation 数据增广。数据增广通常专门指输入变换或隐藏激活。</font>**\n>\n>**<font color = blue>作者根据变换的性质及其参数的分布对基于数据的正则化方法进行分类。</font>**\n>\n>### 变换参数 $\\theta$ 的随机性\n>\n>$\\blacksquare$ **<font color = blue>确定性参数</font>**：参数 $\\theta$ 遵循 delta 分布，数据集大小保持不变。\n>\n>$\\blacksquare$ **<font color = blue>随机性参数</font>**：允许生成一个更大的，可能是无限的数据集。 $\\theta$ 的采样方法多种多样，有：\n>\n>1）**<font color = blue>随机</font>**：从指定的分布中画一个随机的 $\\theta$\n>\n>2）**<font color = blue>自适应</font>**： $\\theta$的值是一个优化过程的结果，通常目标是一个最大化变换样本上的网络误差（这种具有挑战性的样本被认为是当前训练阶段信息量最大的样本），或最小化网络预测和预定义的假目标 $t'$ 之间的差异。\n>\n>> $\\star$ **<font color = blue>约束优化</font>**：通常在硬约束下最大化误差找到 $\\theta$（支持 $\\theta$ 的分布控制最强的允许变换）；\n>>\n>> $\\star$ **<font color = blue>无约束优化</font>**：通过最大化修正误差函数找到 $\\theta$，使用 $\\theta$ 的分布作为权重（为了完整性在此提出，但并未测试）；\n>>\n>> $\\star$ **<font color = blue>随机</font>**：通过获取固定数量的 $\\theta$ 样本并使用产生最高误差的样本来找到 $\\theta$.\n>\n>### 对数据表示的影响\n>\n>$\\blacksquare$ **<font color = green>保留表示的转换 Representation-preserving transformations</font>**：保留特征空间并尝试保留数据分布。\n>\n>$\\blacksquare$ **<font color = green>保留修改的转换 Representation-modifying transformations</font>**：将数据映射到不同的表示（不同的分布甚至新的特征空间），这可能会解开原始表示的潜在因素并使学习问题更容易。\n>\n>### 转换空间\n>\n>$\\blacksquare$ **<font color = blue>输入</font>**：对输入 $x$ 进行变换；\n>\n>$\\blacksquare$ **<font color = blue>隐藏特征空间</font>**：对样本的一些深层表示进行变换（这也使用部分 $f$ 和 $w$ 将输入映射到隐藏特征空间；这种变换在网络 $f_w$ 内部起作用，因此可被认为是架构）\n>\n>$\\blacksquare$ **<font color = blue>目标</font>**：转换应用于 $t$（只能在训练阶段使用，因为标签在测试时没有显示给模型）\n>\n>### 普遍性\n>\n>$\\blacksquare$ **<font color = green>通用 Generic</font>** ：适用于所有数据域；\n>\n>$\\blacksquare$ **<font color = green>特定域 Domain-specific</font>**：针对当前问题的特定（手工制作），例如图像旋转。\n>\n>### $\\theta$ 分布的依赖关系\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta)$</font>**：所有样本的 $\\theta$ 分布相同\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|t)$</font>**：不同目标（类别）的 $\\theta$ 分布可能不同\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|t')$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|x$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|\\mathcal{D})$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|X)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|time)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|\\pi)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> 以上方法的综合</font>**：即 $p(\\theta|x, t)$, $p(\\theta|x, \\pi)$, $p(\\theta|x, t')$, $p(\\theta|x, \\mathcal{D})$, $p(\\theta|t, \\mathcal{D})$, $p(\\theta|x, t, \\mathcal{D})$\n>\n>### 阶段\n>\n>$\\blacksquare$ **<font color = green> 训练</font>**：训练样本的转换。\n>\n>$\\blacksquare$ **<font color = blue> 测试</font>**：测试样本的转换，例如对样本的多个增强变体进行分类，并将结果汇总在它们之上。\n>\n>**<font color = green> 表1回顾了使用通用转换的现有方法</font>**：\n>\n><img src=t1.png width=100% />\n>\n>**<font color = blue> 表2列出了特定域的方法</font>**，特别侧重于图像领域。最常用的方法是：图像的刚性变形和弹性变形。\n>\n><img src=t2.png width=100% />\n>\n>### 目标保留数据增广\n>\n>目标保留数据增广 --> 在输入和隐藏特征空间中使用随机变换，同时保持原始目标 $t$\n>\n><font color = red> 未完待续！！！</font>\n>\n>### 基于数据的正则化方法的总结\n>\n>作者对基于数据的正则化方法进行了形式化，展示了**<font color = green>看似与数据正则化无关的技术，例如保留目标的数据增广、dropout 或 Batch Normalization 等技术在方法上惊人的近似，都可看做是基于数据的正则化方法。</font>**\n\n \n\n## 2 通过网络架构进行正则化\n\n> 为了实现正则化效果，可以选择具有特定属性或匹配特定假设的网络架构 $f$.\n>\n> ### 关于映射的假设\n>\n> > <font color =green>为了很好地拟合数据 $P$，输入-输出 的映射 $f_w$ 必须具有某些属性。</font>尽管执行理想映射的精确属性可能很难，但可通过关于映射的简化假设来近似它们。<font color = green>这些属性和假设可以**以硬或软**的方式强加于模型拟合。</font>这限制了模型的搜索空间，并允许找到更好的解决方案。\n> >\n> > **<font color = green> 作者讨论的对 输入-输出 映射施加假设的方法是网络架构 $f$ 的选择。</font>**一方面，架构 $f$ 的选择 **<font color = green>硬连接</font>**了映射的某些属性；此外，在 $f$ 和优化算法之间的相互作用中，某些权重配置比其他配置更可能通过优化获得，从而进一步**<font color = green>以软方式限制可能的搜索空间</font>**。\n> >\n> > <font color = green>对映射施加某些假设的补充方法是**正则化项**，以及**（增广）数据集中存在的不变性**。</font>\n> >\n> > 假设可以 **<font color = green>硬连接</font>** 到某些层执行的操作的定义中，和 / 或层之间的连接中。\n> >\n> > **<font color = green>基于网络架构的方法如表三</font>**所示：\n> >\n> > <img src=t3_1.png width=100% />\n> >\n> > <img src=t3_2.png width=100% />\n> >\n> > 在隐藏特征空间中对数据进行变换的正则化方法可被视为体系结构的一部分。也就是说，<font color =green>在隐藏特征空间中对数据进行变换的正则化方法既属于数据正则化，也属于网络架构正则化。</font>\n>\n> ### $\\blacksquare$ 权值共享 Weight sharing\n>\n> >权值共享 --> 在网络的多个部分重复使用某个可训练参数。例如，卷积网络中的**<font color = blue>权值共享不仅减少了需要学习的权重的数量，它还编码了 shift-equivariance 的先验知识和特征提取的局部性。</font>**\n>\n> ### $\\blacksquare$ 激活函数 Activation function\n>\n> > 选择正确的激活函数非常重要。例如：\n> >\n> > 1）**ReLUs ** 在训练时间和准确性方面提高了许多深度架构的性能。**<font color = green>ReLUs 的成功既可归因于：ReLUs 可避免梯度消失问题；也可归因于：它们提供了更有表现力的映射家族 more expressive families of mappings</font>**。\n> >\n> > **<font color = blue> 一些激活函数是专门为正则化设计的。</font>**\n> >\n> > 2）**Dropout** ，**Maxout** 单元允许在测试时更精确地逼近模型集合预测的几何平均值。\n> >\n> > 3）**Stochastic pooling 随机池化** 是最大池化的噪音版本。作者声称，这允许对激活的分布进行建模，而不仅是取最大值。\n>\n> ### $\\blacksquare$ 噪声模型 Noisy models\n>\n> > **Stochastic pooling** 随机池化是确定性模型的随机泛化的一个例子。<font color = green>有些模型是通过向模型的各个部分注入随机噪声来实现的。 Dropout 是最常用的噪声模型</font>\n>\n> ### $\\blacksquare$ 多任务学习 Multi-task learning\n>\n> > **多任务学习  --> 是一种特殊类型的正则化。**它可与半监督学习相结合，在辅助任务上利用未标记数据。\n> >\n> > **元学习**中也使用了任务之间共享知识的类似概念，其中来自同一领域的多个任务被顺序学习，使用先前获得的知识作为新任务的偏差。\n> >\n> > **迁移学习**，将一个领域的只是迁移到另一个领域。\n>\n> ### $\\blacksquare$ 模型选择 Model selection\n>\n> > 可通过评估验证集上的预测来选择几个经过训练的模型（例如，具有不同的架构）中最好的模型。\n\n\n\n## 3 通过误差函数进行正则化\n\n> **<font color = green>理想情况下，误差函数 $E$（表示输出与目标的一致性） 反映了适当的质量概念，在某些情况下还反映了一些关于数据分布的假设。</font>**典型的例子是：**均方误差** 或 **交叉熵**。\n>\n> **<font color = blue> 误差函数 $E$ 也可以具有正则化效果。</font>**例如，Dice coefficient optimization 系数优化，它对类别不平衡具有鲁棒性。\n\n\n\n## 4 通过正则化项进行正则化\n\n> **<font color = green>正则化可以通过在损失函数中添加正则化器 $R$ 来实现。</font>**与误差函数 $E$ （表示输出与目标的一致性）不同，**<font color = green>正则化项独立于目标。</font>**相反，**<font color = blue>正则化项用于编码所需模型的其他属性，以提供归纳偏差（即关于映射的假设，而不是输出与目标的一致性）。</font>** 因此，**<font color = green>对于未标记的测试样本，正则化项 $R$ 的值能计算出来，而误差函数 $E$ 不能计算。</font>**\n>\n> **正则化项 $R$ 与目标 $t$ 的独立性有一个重要含义：它允许额外使用未标记的样本（半监督学习），根据其符合一些期望的属性来改进学习模型。**\n>\n> > **<font color = green> 一个经典的正则化方法是 weight decay 权值衰减**\n> >\n> > <img src=5.png width=70% />\n> >\n> > 其中，$\\lambda$ 是一个加权项，用于控制正则化对一致性的重要性。\n> >\n> > **从贝叶斯的角度来看** ，权重衰减对应于使用对称的多元正态分布作为权重的先验：\n> >\n> > $$ p(w) = \\mathcal{N}(w|0,\\lambda^{-1}I) $$\n> >\n> > <img src=e1.png width=70% />\n> >\n> > **图4 回顾了现有的通过正则化项进行正则化的方法。权重衰减（L2正则化）似乎仍然是最流行的正则化项。**\n> >\n> > **<font color = green>L2 正则化的目的就是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题，所以权重衰减也叫 L2 正则化。</font>**\n> >\n> > <img src=t4_1.png width=100% />\n> >\n> > <img src=t4_2.png width=100% />\n> >\n> > <img src=t4_3.png width=100% />\n\n\n\n## 5 通过优化进行正则化\n\n> **<font color = green>随机梯度下降（SGD）及其衍生</font> 是深度神经网络中最常用的优化算法，也是我们关注的中心。**作者也在下文列出了一些替代方法。\n>\n> **<font color = green>随机梯度下降法（SGD）</font> 是一种采用以下更新规则的迭代优化算法 ：**\n>\n> <img src=7.png width=70% />\n>\n> 如果算法在合理的时间内达到较低的训练误差（与训练集的大小呈线性关系，允许多次通过训练集 $\\mathcal{D}$，那么在某些温和的假设下，解决方案的泛化效果很好，从这个意义上来说：\n>\n> **<font color = green> SGD 作为一个隐性的正则化器：即使没有使用任何额外的正则化器，较短的训练时间也能防止过拟合。</font>** --> 这与论文《Understanding deep learning requires rethinking generalization  》中的观点一致：该论文作者在一系列实验中发现，**<font color = blue>正则化（例如 Dropout、数据增广和权重衰减）本身既不是良好泛化的必要条件，也不是充分条件。</font>**\n>\n> 作者将通过优化进行正则化的方法分为三组：\n>\n> $\\blacksquare$ 初始化 /热启动方法\n>\n> $\\blacksquare$ 更新方法\n>\n> $\\blacksquare$ 终止方法\n>\n> ### $\\blacksquare$ Initialization and warm-start methods 初始化/热启动方法\n>\n> > \n>\n\n\n\n\n\n## 建议、讨论、结论\n\n> > ### 1 该分类法的优势：\n> >\n> > 作者认为<font color = green>这样的分类法的优势</font>有两个方面：\n> >\n> > 1）它为正则化方法的用户提供了现有技术的概述，并让他们更好地了解如何为他们的问题选择理想的正则化技术组合。\n> >\n> > 2）它对于开发新方法很有用，因为它全面概述了可用于正则化模型的主要原则。\n>\n> > ###  2 作者建议：\n> >\n> > #### 1. **<font color = green>对现有正则化方法用户的建议</font>**\n> >\n> > 总的来说，<font color = green>尽可能多地使用数据中包含的信息以及先验知识，并主要从流行的方法开始</font>，以下程序可能是有帮助的:\n> >\n> > $\\blacksquare$ 对于第一步的常见建议：\n> >\n> > 1）<font color = green>深度学习就是要把变异的因素分解开来。</font>应该选择一个合适的数据表示；**已知的有意义的数据转换不应该外包给学习。** 在几种表征中，**冗余地提供相同的信息是可以的。**\n> >\n> > 2）<font color = green>输出非线性和误差函数应该反应学习目标。</font>\n> >\n> > 3）一个好的起点是通常工作良好的技术（例如，ReLU，成功的架构）。**超参数（和架构）可以联合调优，但是很缓慢**（根据经验进行插值 / 推断，而不是尝试太多的组合）。\n> >\n> > 4）通常，<font color = green> 从一个简化的数据集（例如，更少和/或更简单的样本）和一个简单的网络开始是有帮助的，</font> 在获得有希望的结果后，<font color = green>在调优超参数和尝试正则化方法时逐渐增加数据和网络的复杂性。</font>\n> >\n> > $\\blacksquare$ 通过数据进行正则化：\n> >\n> > 1）当不处理几乎无限 / 丰富的数据时：\n> >\n> > $\\star$ 如果可能的话，收集更多的真实数据（并使用考虑到其属性的方法）是可取的：\n> >\n> > * **有标记的样本**是最好的，但无标记的样本也可能有用（兼容半监督学习）。\n> > * **来自相同领域的样本**是最好的，但来自相似领域的样本也会有帮助（兼容领域适应和迁移学习）。\n> > * **可靠的高质量样本**是最好的，但低质量样本也有帮助（它们的信心 / 重要性可以相应地调整）。\n> > * **给额外的任务贴上标签**会很有帮助（与多任务学习兼容）。\n> > * **额外的输入特性（来自额外的信息源）和 / 或数据预处理（即特定于领域的数据转换）**可能会有所帮助（网络架构需要相应的调整）。\n> >\n> > $\\star$ **数据增广**（例如，保留目标的手工特定领域转换）可以很好地弥补有限的数据。如果一直增强数据的自然方法（充分模拟自然转换），则可以尝试（并组合）它们。\n> >\n> > $\\star$ 如果增广数据的自然方法未知或被证明是不充分的，如果有足够的数据可用，就有可能从数据中推断出转换（例如学习图像变形字段）。\n> >\n> > 2） 流行的泛型方法（例如 Dropout 的高级变体）通常也有帮助。\n> >\n> > $\\blacksquare$ 架构和正则化项：\n> >\n> > 1）关于映射的可能的有意义的属性的知识可以被用来如将不变性（对某些转换）硬连接到架构中，或者被表述为正则化项。\n> >\n> > 2）流行的方法也可能有帮助（见表3和表4），但应该选择匹配映射的假设（例如，仅当需要对常规网格数据进行局部和移位等变特征提取时，卷积层才完全合适）。\n> >\n> > $\\blacksquare$ 优化：\n> >\n> > 1）初始化：尽管预训练的现成模型大大加快了原型的制作速度，但良好的随机初始化也应该被考虑。\n> >\n> > 2）优化器：尝试一些不同的方法，包括先进的（例如 Nesterov momentum, Adam, ProxProm），可能会带来更好的结果。正确选择的参数，例如学习率，通常会产生很大的不同。\n> >\n> > \n> >\n> > #### 2. <font color = green>对新正则化方法的开发人员的建议</font>\n> >\n> > 了解最佳方法成功的原因是一个很好的基础。有希望的空白领域（分类法属性的某些组合）是可以解决的。强加在模型上的假设可能会对分类法的大多数元素产生强烈的影响。<font color = green> **数据增广比损失项更有表现力**（损失项只在训练样本的无限小的邻域强制属性；数据增广可以使用丰富的转换参数分布）。</font>数据和损失项以相当软的方式强加假设和不变性，并且可以调整它们的影响，而硬连接网络架构是强加假设的更苛刻的方式。施加它们的不同假设和选项具有不同的优点和缺点。\n> >\n> > \n> >\n> > #### 3. <font color = green> 基于数据方法的未来方向</font>\n> >\n> > 作者认为以下几个有前景的方向值得研究：\n> >\n> > 1） $\\theta$  的自适应采样可能会导致更低的误差和更短的训练时间（反过来，更短的训练时间可能会额外起到隐式正则化的作用）。\n> >\n> > 2）作者认为学习类依赖变换会导致更可信的样本。\n> >\n> > 3）在最近引发了关于真实世界对抗示例及其对摄像机位置变化等变换的鲁棒性 / 不变性的讨论后，对抗示例（以及对它们的网络鲁棒性）领域正获得越来越多的关注。对抗强烈的对抗性例子可能需要更好的正则化技术。\n> >\n> > \n> >\n> > #### 4. 总结\n> >\n> > 在这项工作中，<font color = green>作者为深度学习提供了一个广义的的正则化定义，确定了**神经网络训练的五个主要元素（数据，架构，错误项，正则化项，优化程序）**，通过每个元素描述了正则化，包括对每个元素的进一步、更精细的分类，并从这些子类别中提供了示例方法。</font> 我们没有试图详细解释引用的作品，而只是确定它们与我们的分类相关的属性。我们的工作证明了现有方法之间的一些联系。此外，我们的系统方法通过结合现有方法的最佳特性，能够发现新的、改进的正则化方法。\n\n\n\n********************************************************************************************************************************************************************************************************************************************************************\n\n***********************************************************************************************************************************************************************************************************************************************************************\n\n# Heuristic Dropout: An Efficient Regularization Method For Medical Image Segmentation Models_2022,Tsinghua University\n\n## \tAbstract\n\n> 对于真实场景中的医学图像分割，像素级的准确标注数据量通常较少，容易造成过拟合问题。这篇手稿深入研究了 Dropout 算法，该算法常用于神经网络以缓解过拟合问题。这篇手稿**从解决 co-adaptation problem 协同适应问题的角度**出发，解释了 <font color = green>Dropout 算法</font>的基本原理，并讨论了<font color =green>其衍生方法存在的局限性</font>。此外我们提出一种新颖的**Heuristic Dropout启发式 Dropout 算法来解决这些局限**。**<font color = green>该算法以信息熵和方差作为启发式规则。</font>** 它指导我们的算法更有效地丢弃遭受协同适应问题的特征，从而更好地缓解小规模医学图像分割数据集的过拟合问题。医学图像分割数据集和模型的实验表明，所提出的算法显著提高了这些模型的性能。\n\n\n\n## Intex Terms\n\n> 医学图像分割，过拟合问题， Dropout 算法，信息熵\n\n\n\n## 1. Introduction\n\n> **医学图像分割**是当前**计算机辅助医学诊断（Computer-aided Medical Diagnosis, CAD）系统**的重要组成部分，其准确性直接影响 CAD 系统的性能。近年来，CAD 系统越来越多地参与到实际的医疗诊断任务中。因此，提高医学图像分割模型的准确性和可靠性具有重要的意义和应用价值。\n>\n> 在医学图像分割领域， <font color = green>U-Net，nnU-Net，TransUNet 等深度学习模型</font>已经在各种任务中表现出了比传统方法更好的性能。与**自然图像分割**相比，**<font color = green>医学图像分割的数据标定高度依赖于专家知识，需要像素级的准确标定。因此，在专家指导下，像素级的准确标定数据量通常很小。</font>**小尺度的数据集容易出、造成过拟合问题，特别是当分割模型参数量较大时。\n>\n> 解决过拟合问题的方法有很多， Dropout 算法是其中一种简单而有效的方法。它在训练过程中以一定的概率随机丢弃模型中的神经元，缓解了协同适应问题，从而缓解了深度学习模型的过拟合问题。**<font color = green>Co-adaptation 协同适应是指每个神经元学习到的特征通常必须与上下文（即其他特定神经元）相结合的现象，以在训练过程中提供有用的信息。</font>** 然而，**从小规模医学图像分割数据集中学习到的这种经验依赖是脆弱的，在面对测试集的分布时可能不可信。** 因此，<font color = green>神经元之间过多的依赖关系往往会引发过拟合问题。</font> Dropout 算法中的 drop 操作减少了深度学习模型中神经元之间的依赖关系，防止了一些神经元过度依赖其他神经元，从而在一定程度上避免了过拟合问题。\n>\n> **<font color = green>根据 drop 过程是否完全随机，Dropout 算法的衍生方法可以分为两类。</font>** **<font color = blue>第一类是完全随机的方法</font>**，例如 **<font color = blue>Spatial Dropout </font>** 随机丢弃通道维度中的单元，**<font color = blue>DropBlock </font>** 将 2d blocks 视为单元并随机丢弃它们，**<font color = blue>Stochastic Depth</font>** 随机丢弃残差连接。**<font color = purple>第二类是基于规则的方法 </font>，** 例如  **<font color = purple>Weighted Channel Dropout</font>** 以通道的激活值作为指导规则，**<font color = purple>Focused Dropout</font>** 以 2d blocks 的激活值作为指导规则。然而，这两类现有方法都不是没有局限的。**<font color = blue>第一类，完全随机的方法，缺乏指导规则，因此可能效率低下，丢弃的特征不一定是遭受协同适应问题的特征</font>**，**<font color = purple>在第二类，基于规则的方法中，现有的指导规则不够准确，丢弃遭受协同适应问题的精确特征的效率仍有提升空间</font>** 。因此，**<font color = green>本手稿提出了一种结合信息熵和方差的新的指导规则</font>**。在此规则的指导下，进一步提高了所提出的算法丢弃遭受协同适应问题的特征的效率。在多个医学图像分割数据集和模型上的实验表明，该算法显著提高了模型精度。\n\n\n\n## 2. Methodology\n\n> 作者提出了一种新颖的启发式 Dropout 算法，**<font color = green>使用信息熵和方差作为指导规则来执行 Dropout 操作</font>**。该算法能够有效地丢弃遭受协同适应影响的特征，从而在很大程度上缓解了医学图像分割任务中的过拟合问题。\n\n### 2.1. Heuristic Metric\n\n> 为了有效地丢弃协同适应问题较严重的特征，作者采用信息熵作为启发式规则。**<font color = green>信息熵可以衡量一个分布的不确定性。</font>**\n>\n> <img src=h1.png width=35% />\n>\n> 其中，$X$ 是一个随机变量， $p(x)$ 是概率密度函数，$H(x)$ 是关于随机变量 $X$ 的信息熵。\n>\n> <img src=dropout_f7.png width=70% />\n>\n> 如 Dropout 一文中的图7所示，**<font color = green>遭受严重协同适应的特征具有不确定的视觉意义，因此具有较高的信息熵值，而遭受轻微协同适应的特征具有确定的视觉意义</font>**，例如，看起来像目标的点、边缘或几何轮廓，这些特征的信息熵值较低。**<font color = purple>以信息熵为指导原则，我们以更高的概率丢弃遭受严重协同适应问题的特征。</font>**此外，我们还需要**<font color = purple>方差作为另一个启发式规则</font>**。考虑 **<font color = blue>一个极端的情况，当分布接近于常量分布时，已知信息熵将接近最小值。</font>**然而，**<font color = purple>具有常量分布 constant distribution 的特征对训练几乎不提供什么有用的信息。因此，将方差作为另一个启发式规则，我们以更大的概率丢弃更接近常量分布的特征</font>**。\n\n### 2.2. Heuristic Dropout Algorithm\n\n> 结合信息熵和方差两种启发式规则，得到算法1。\n>\n> <img src=heuristic_a1.png width=70% />\n>\n> 我们计算输入特征图 input feature maps 的每个通道的信息熵 $e_i$ 和方差 $v_i$。我们使用 $e_i + \\frac{k}{v_i + \\epsilon}$ 作为指导规则。因为 feature maps 的值是连续分布的，所以我们首先要对值进行量化，然后根据直方图计算信息熵，如算法2所示。还发现使用 $3 \\times 3$的 Laplace 滤波器代替 all-zero 滤波器作为 drop mask 将为模型性能带来一点提升。\n>\n> <img src=heuristic_a2.png width=70% />\n>\n> 我们的算法可无缝插入到各种模型中。以  U-Net 为例，我们在 U-Net 的编码器和解码器的每个阶段的两个连续卷积层之间插入所提出的算法，即：在前一个卷积层的激活函数之后，正好在下一个卷积层之前。\n\n\n\n## 3. Results\n\n### 3.1. Datasets\n\n> 作者在 Pancreas-CT 数据集和 BAGLS 数据集上进行实验。**考虑到在实际应用环境中，由经验丰富的专家标记的训练样本一般很少，我们专门从这些数据集中随机抽取一个子集进行试验。**对于 Pancreas-CT 数据集，我们随机选择12个扫描，然后将其转换为2545个 $512 \\times 512$ 的 2D 切片，以方便训练模型。在这 12幅 3D CT 扫描中，我们随机选择 8幅 作为训练集，2幅 作为验证集，2幅 作为测试集。对于 BAGLS 数据集，我们随机选择 3000个 切片作为训练集，而验证集和测试集的大小保持与原始设置相同。\n\n### 3.2. Evaluation Metrics\n\n> 为了对实验结果进行定量分析，我们采用了医学图像分割领域中广泛使用的 **<font color = green>DICE 值</font>** 和 **<font color = green>IoU 值</font>**作为评价指标。\n>\n> <img src=heuristic_e1.png width=50% />\n>\n> 其中，$X$ 表示模型输出的掩膜 mask，$Y$ 表示输入图像对应的真值 ground truth.\n\n### 3.3. Experimental Settings\n\n> 我们使用 **Adam optimizer** 来训练所有的模型，学习率为 $1 \\times 10^{-3}$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$， $\\epsilon = 10^{-8}$。batch size 的大小设置为可以在 GeForce RTX 2080 Ti 上以混合精度执行的最大值。我们使用 CrossEntropy 并在 Pancreas-CT 数据集上训练100个 epoch。我们使用结合 CrossEntropy, DiceLoss 和 SSIMLoss 的混合损失函数，并在 BAGLS 数据集上训练 30个epoch。我们在训练数据集上使用标准数据增广。不对模型的输出结果进行后处理。我们独立重复所有对比试验5次并报告平均结果。\n\n### 3.4. Comparison with Dropout Derivative Methods\n\n> 为了验证该算法的有效性，我们在 Pancreas-CT 数据集和 BAGLS 数据集上进行了实验。我们将我们的算法和其它 Dropout 的衍生算法加入到几个模型中。图1 为实验结果的箱线图 box plots，表1为定量和整体对比。\n>\n> <img src=heuristic_f1.png width=60% />\n>\n> <img src=heuristic_t1.png width=80% />\n>\n> 试验结果表明，该算法在 Pancreas-CT 和 BAGLS 两个数据集上的性能都优于其他 Dropout 衍生方法。在 Pancreas-CT 数据集上，我们的算法对 U-Net 和 Attention U-Net 的 DICE 值分别提高了 3.67 和 3.37。在 BAGLS 数据集上，我们的算法对 U-Net 和 UNet3+ 的 IoU 值分别提高了 2.97 和1.12。该算法可以更加有效地提高医学图像分割模型的性能。\n\n### 3.5. Comparison Study on Hyperparameter $k$\n\n> 基于 U-Net 和 Pancreas-CT 数据集，作者研究了超参数 $k$ 的影响。随着 $k$ 的增加，性能呈现先增加后衰减的趋势，当 $k$ 为 3 时，性能最好。此外，从方框的方差（the variance of the box）可以看出，当 $k$ 为 2 时，模型性能比 $k$ 为 3 时更稳定和可预测。\n>\n> <img src=heuristic_f3.png width=60% />\n\n### 3.6. Verify Effectiveness of Alleviating Co-adaptation\n\n> 为了验证我们的算法比传统的 Dropout 更有效地缓解协同适应，我们在 Pancreas-CT 数据集上随机隐蔽了 U-Net 最终输出层之前的一定比例的中间特征图（intermediate feature maps）。**<font color = green>对于协同适应较少的模型，由于特征之间的依赖关系较少，掩蔽特征（masked features ）导致的性能下降应该更小。</font>** 如图 4 所示。我们的算法在隐蔽后的性能下降明显小于传统的 dropout。实验结果表明，**<font color = blue>使用我们的算法可以学习到更多独立特征和更少的依赖关系，因此我们的算法可以比传统的 Dropout 算法更大程度地缓解协同适应。</font>**\n>\n> <img src=heuristic_f4.png width=60% />\n\n### 3.7. Visualization of Segmentation Results\n\n> <img src=heuristic_f2.png width=100% />\n>\n> 图 2 演示了定性分析的可视化。从上到下显示三个切片的分割结果。可视化图表明，我们的算法能更准确地分割模型。\n\n\n\n## 4. Conclusion\n\n> 作者提出了一种新的启发式 Dropout 算法来解决小规模医学图像分割数据集的过拟合问题。该算法以信息熵和方差作为启发式规则，更有效地缓解了协同适应现象，从而更好地缓解了过拟合问题。在多个数据集和模型上的实验表明，该算法具有较好的性能。此外，我们将在未来的工作中研究我们的算法与自然图像的兼容性。\n\n\n\n1. > **<font color = green>启发式</font>**：类似于 **<font color = green> 灵感</font>**一类的东西，可以快速进行判断，不需要逻辑性的思考论证。启发式往往可以让人们 **<font color = green>跳出当前思维的局限</font>**，但因为缺乏科学依据与缜密的逻辑验证，所以有时也会出错。\n\n   \n\n2. **<font color = green>医学图像与自然图像的区别：</font>**\n\n   > 1）医学图像大多数时放射成像，功能性成像，磁共振成像，超声成像这几种方式，而自然图像大多数是自然光成像。自然成像中，光谱比较复杂，有散射的成分，波普宽度比较大，但放射成像例如 DR, CT等，各厂家需要去除人体内的散射，使光谱单一，所以，这导致了一个重要区别，也就是：\n>\n   > **<font color = green>在自然图像中，噪声分布绝大多数情况下可认为是均匀的，可近似为高斯噪声</font>**，因为直射和散射光造成光场分布可认为是均匀的；\n>\n   > 但**<font color = blue>在医学图像中，由于光源单一再加上探测手段，人体厚度的影响往往会导致噪声分布不均匀，往往认为是一种泊松噪声</font>**。\n>\n   > 所以，针对医学图像的算法直接应用于自然图像效果可能不行。\n>\n   > 2）医学图像多是单通道灰度图像，尽管大量医学图像是3D的，但医学图像中没有景深的概念。\n>\n   > 3）同体态的医学图像相似度非常高，**医学图像中的细微结构并不能像自然图像中那样认为是无关紧要的**，在相似度极高的背景组织中的细微变化有可能代表着某种病变。\n\n   \n\n3. **<font color = green>Co-adaptation 协同适应</font>**\n\n   > **过拟合**：在训练集上实现高性能，但没法很好地泛化到看不见的数据（测试集）上。\n   >\n   > **在神经网络中，协同适应意味着一些神经元高度依赖其他神经元**。如果那些独立的神经元接收到“坏”的输入，那么依赖的神经元也会受到影响，最终它会显著改变模型的性能，这就是过度拟合可能发生的情况。\n   >\n   > Hinton 提出 Dropout 来防止过拟合：网络中的每个神经元以0和1之间的概率随机丢弃。--> Hinton 认为 Dropout 能防止过拟合的原因在于：**<font color = green>通过实施 Dropout 模型被迫拥有可以学习良好特征（或所需数据表示）的神经元，而不依赖于其他神经元。因此，生成的模型对于看不见的数据可能更加鲁棒。</font>**\n\n   \n\n   \n\n   \n   \n   # Inproving Neural Networks By Preventing Co-adaptation of Feature Detectors_2012_Hinton\n   \n   > **<font color = green>协同适应 Co-adaptation</font>**：一个特征检测器只在其他几个特征检测器的上下文中有用。\n   >\n   > 为了阻止复杂的协同适应性，Dropout 通过在训练过程中随机丢弃一半的特征检测器，迫使 **<font color = green>每个神经元学习检测一种特征，这种特征通常有助于产生正确的答案，因为它必须在各种内部环境中运作。</font>** \n   \n   \n   \n   \n   \n   # Neuron-Specific Dropout: A Deterministic Regularization Technique to Prevent Neural Networks from Overfitting & Reduce Dependence on Large Training Samples\n   \n   ## Abstract\n   \n   > 为了发展输入与输出之间的复杂关系，深度神经网络对大量参数进行训练和调整。为了使这些网络高精度地工作，需要大量数据。**<font color = green>然而，有时训练所需的数据量并不存在或无法获得。Neuron-specific dropout (NSDropout) 被提出用来解决该问题。</font>**  NSDropout 会同时查看模型中层的训练过程和验证过程。通过比较数据集中每个神经元对每个类别产生的平均值，该网络能够丢弃目标单元。**<font color = purple>该层能够预测模型在测试过程中所观察的特征或噪声，而这些特征或噪声在观察验证样本时是不存在的。</font>** **<font color = blue>与 Dropout 不同的是，“thinned” networks “精简”网络不能 \"unthinned\" “未精简”用于测试。</font>** 与传统方法（包括 dropout 和其他正则化方法）相比，**<font color = green>Neuron-specific dropout 被证明可以用更少的数据达到类似的（如果不是更好的话）测试精度。</font>** 实验表明， Neuron-specific dropout 减少了网络过拟合的机会，并 **<font color = green>减少了图像识别中监督任务对大量训练样本的需要</font>**，同时产生了同类最佳（best-in-class）的结果。\n   \n   \n   \n   ## Keywords: \n   \n   > neural networks, regularization, model combination, deep learning, dropout\n   \n   \n   \n   ## 1. Introduction\n   \n   > 深度神经网络可以理解为输入与输出之间的复杂关系。通过利用数千甚至数百万个隐藏节点（神经元），这些模型可以生成一套足以预测癌症或驾驶汽车的规则。然而，要做到这一点，需要大量数据来训练并验证模型。**<font color = green>当数据量不足时，模型可能会关注训练数据中的缺陷或者采样噪声。</font>** 换句话说，该模型将发现训练数据中存在的细节，而这些细节可能在实际应用中并不存在（该模型将发现训练数据中可能并不存在于其实际应用中的细节）。这些最终会导致过拟合，并且因为没办法做出一个完美的数据集，因此已经发展了其他方法来尝试减少模型过拟合的趋势。最流行的方法之一是，当模型的验证精度和训练精度出现偏差时，停止训练。另一个方法是实施权重惩罚，如 L1 和 L2 以及软权重共享（soft weight sharing）。\n   >\n   > <img src=NSDropout_f1.png width=100% />\n   >\n   > **<font color = green>现在有几种方法来解决过拟合问题，一种是贝叶斯方法的使用</font>**。贝叶斯模型是根据贝叶斯定理构建统计模型。\n   >\n   > <img src=NSDropout_e1.png width=20% />\n   >\n   > 贝叶斯 ML 模型的目标是在给定先验分布 prior distribution $(p(\\theta))$ 和 likely hood $(p(x|\\theta))$ 的情况下估计后验分布 posterior distribution $(p(\\theta|x))$。这些模型与经典模型的不同之处在于包含了 $p(\\theta)$ 或先验分布。**<font color = green>一种流行的先验分布是高斯过程。</font>** 通过取所有参数设置的平均值，并将其值与给定训练数据的后验概率进行加权。有了先验高斯分布，我们可以假设后验分布是正态分布或落在正态钟形曲线上。**假设我们有无限的计算能力，防止过拟合最好的方法是计算一个完美的后验分布。** 然而，**<font color = green>逼近后验分布</font>** 已经被证明可以在小模型上提供很好的结果。\n   >\n   > 对于具有少量隐藏节点的模型，与单个模型相比，对使用不同架构和数据训练的不同模型的值进行平均可以提高性能。然而，对于较大的模型，此过程将过于耗费资源，无法证明回报是合理的。训练多个模型是困难的，因为找到最佳参数可能会耗费大量时间，而且训练多个大网络会占用大量资源。此外，在不同的数据子集上获取足够多的数据来训练多个网络是不可能的。最后，假设你能够使用不同数据子集来训练不同架构的多个网络，在需要快速处理的应用程序中，使用所有这些模型进行测试将花费太多的时间。\n   >\n   > 这就引出了防止过拟合的第二种选择。**<font color = green>Dropout 是一种简单而有效的方法来限制噪声对模型的影响</font>**。它通过“dropping 丢弃”隐藏或可见单元来防止模型过拟合，**<font color = green>本质上是同时训练多个模型</font>**。通过丢弃一个单元，该单元在该步骤中不再对模型及其决策产生影响。丢弃的神经元数量由概率 $p$ 决定，即独立于其它单元。\n   >\n   > <img src=NSDropout_f2.png width=100% />\n   >\n   > 图2：左：在训练阶段，假设索引 $i$ 处的值 $r^{(l)}$ 为1时，unit 出现。假设函数 $a_i^{(l)}$ 的输出在向量函数 $a^{(l)}$ 的输出值中不是最低的 $p$ 个百分比，则 $r_i^{(l)}$ 的值为1。右：在测试阶段，只有当 $r_i^{(l)}$ 的最终值为 1 时，unit 才会出现。\n   >\n   > 现在我们有了另一种防止过拟合的方法。 **<font color = green>Neuron-Specific dropout 采用了从一个层中丢弃隐藏或可见单元的思想，而不是随机的丢弃它们</font>** 。与其它流行的层不同， **<font color = green>Neurom-Specific Dropout 接受四种输入</font>** ：layer input 层输入，the true value of the sample 样本真值，validation layer input 验证层输入，the true value of the validation sample 验证样本真值。通过了解哪些神经元的值与该类样本的验证平均值最远，我们可以找到噪声或训练数据中的伪影在哪些地方影响了我们的模型决策。丢弃的神经元数量取决于比例 $p$。然而，这与 Dropout 不同，因为概率 $p$ 表示一层中有多少 百分比的 units 将被丢弃。例如，如果在具有 20 个 units 的层中将 $p$ 设置为 0.2，那么总的会有 4个 units 被丢弃。\n   >\n   > 通常，神经网络中使用的验证数据不应该在调整超参数之外影响模型的行为，但是 neuron-specific dropout 可以提高准确性，这样就可以分割传统的训练数据集，从而永远不会使用保留的验证数据。对训练数据进行分割，以便为新的验证集保留 20% 似乎时是最佳的。\n   >\n   > 类似于 Dropout，应用 neuron-specific dropout 会产生一个 \"thinned\" 的神经网络。这个 thinned 神经网络保存了从神经元丢弃中幸存下来的神经元的所有值。虽然可以解释为具有 $n$ 个 units 的神经网络代表 $2^n$ 个可能的 thinned 神经网络，但众所周知，随着训练的进行，从一个步骤到下一个步骤丢弃的不同的 units 的数量会减少。同样，可训练参数的总数仍然是 $O(n^2)$ ，或者更少。\n   >\n   > 与 dropout 不同的是，如果使用单个的，按比例缩小的神经网络，使用该层的好处不会显示出来。当最后一次使用 mask 时，发现测试结果最好。这是有意义的，因为与 dropout 不同， units 不是随机丢弃的。当模型开始找到受噪声和特征影响的 units 时，它会将它们归零，而把它们带回来则会带回它已经学会的在没有噪声和特征的情况下改进的权重。\n   >\n   > 本文结构如下。第 2 节描述了 neuron-specific dropout 的动机。第 3 节描述了之前的相关工作。第 4 节正式描述了 neuron-specific dropout  model 和它如何工作。第 5 节一个训练 neuron-specific dropout 网络的算法，并引入了不可见验证的思想。第 6 节给出了应用 NSDropout 的实验结果，并与其他形式的正则化和模型组合进行了比较。第 7 节讨论了 NSDropout 的显著特征，并分析了 neuron-specific 的影响，以及不同的参数如何改变网络的性能。\n   \n   \n   \n   ## 2. Motivation\n   \n   >  neuron-specific dropout 的动机来自于 dropout。与 neuron-specific dropout 类似， dropout 切断了与神经元的连接。这项研究最初是出于限制数据量的想法，但当发现 neuron-specific dropout 也可以帮助减少过拟合时，这项研究很快改变了主意。在日常生活中，人们学到的信息比需要的更多，无论是从对话，新闻还是课程。当大脑认为学习到的信息以后不会再被使用时，就会失去一部分。这有助于防止大脑变得混乱。\n   >\n   > 对于这种现象产生的一个可能的解释是大脑中一种称为干扰的现象。当一个记忆干扰其他记忆时，就会发生干扰。记忆可以定义为大脑中获取，存储，保留和稍后检索信息的过程。干扰可以是主动的或追溯的（事后的）。主动干扰是指大脑由于记忆较旧而无法记住信息。追溯性干扰是指大脑在收到新信息时保留先前学习信息的能力。**<font color = green> Neuron-specific dropout 使用类似于追溯性干扰的方法</font>** 。虽然模型本身无法知道哪些信息是有用的（类似于人脑），但验证数据可以让它们了解它们在测试时会看到什么。通过了解验证阶段存在哪些噪声，模型可以关闭或忘记哪些信息对于测试是不必要的。当每个隐藏单元被呈现出新的信息时，即前一层的输出时，它会接收并“学习”这些信息，然后，在激活之前，它会决定哪些信息“干扰”来自验证数据的信息。\n   \n   \n   \n   \n   \n   \n   \n   ## 8. Conclusion\n   \n   > Neuron-specific dropout (NSDropout) 是一种旨在提高神经网络准确性的 **<font color = green>确定性正则化技术，重点关注具有少量训练数据的网络</font>**。通过传统的学习技术，**<font color = green> 网络在一组数据的输入与输出之间建立了复杂的关系，然而这些复杂的关系往往不能泛化到看不见的未知数据</font>**。***<font color = purple>与 Dropout 不同的是， Dropout 可以随机破坏这些复杂的关系， Neuron-specific dropout 可以帮助网络理解这些复杂的关系中哪些导致了网络的过拟合，并关闭隐藏单元，强迫网络在没有这些导致网络过拟合的复杂关系的情况下学习。</font>*** 实验证明，使用 NSDropout 可以提高神经网络在图像分类领域的性能。NSDropout 能够在 MNIST 手写数字，Fashion-MNIST 和 CIFAR-10 中取得最好的（best-in-class）结果。\n   >\n   > 此外，为了提高图分类网络的性能，NSDropout 还减少了对大数据集的需求。当对 MNIST 手写数字进行训练时， NSDropout 网络仅使用 750 个训练样本就能达到完美的测试精度（a perfect test accuracy）。在 Fashion-MNIST 中， NSDropout 仅使用 60000 个训练样本中的 10000 个 就能达到近乎完美的准确率（a near-perfect accuracy）. **<font color = green>NSDropout 的一个关键特征是能够在训练期间将测试精度和训练精度联系起来。</font>** 这有助于限制网络过拟合的机会。\n   >\n   > **<font color = blue>NSDropout 的一个局限是训练模型所需时间的增加。</font>** 一个图像分类 NSDropout 模型的训练时间是相同架构的标准神经网络的 4 倍，并且没有进行优化。它需要比传统的 dropout 模型多 两倍 的时间。**<font color = blue>时间增加的一个主要原因是 NSDropout 层中按类排序和无序的多个输入。</font>** 虽然排序算法变得更快，并且可以对 NSDropout 进行更多的优化，但它们仍然占用了处理过程中的大部分时间。**<font color = green>目前 NSDropout 只是 丢弃（drops）它认为网络过于依赖的单元，但未来的工作可能会着眼于如何调整单元而不是丢弃它</font>**，从而在更广泛的应用程序中提高性能。\n   \n\n\n\n# Structural Dropout for Model Width Compression_2022\n\n## Abstract\n\n> 众所周知，现有的 ML 模型是高度过度参数化的（highly over-parameterized），并且使用了比给定任务所需更多的资源。以前的工作已经探索了离线压缩模型（compressing models offline），例如，从较大的模型中提取知识到较小的模型中。这对于压缩是有效的，但没有给出衡量模型可以压缩多少的经验方法，并且需要对每个压缩模型进行额外的训练。**<font color = green>我们提出一种只需要对原始模型和一组压缩模型进行一次训练的方法。</font>** 所提出的方法是一种 **<font color = green>structural dropout</font>**，它会在随机选择的索引之上剪枝掉所有处于隐藏状态的元素，从而迫使模型学习其特征的重要性排序。在学习了这种排序之后，在推理阶段可以剪枝掉不重要的特征，同时保持最大的准确性，显著减小参数大小。在这项工作中，我们聚焦于全连接层的 Structural Dropout，但这个概念可以应用于任何类型的具有无序特征的层，如卷积层或 attention layers。Structural Dropout 不需要额外的剪枝 / 重新训练，但需要对每个可能的隐藏大小（each possible hideen sizes）进行额外的验证。在推理阶段，非专业人员可以在广泛的高压缩和更精确的模型之间选择最适合他们需求的内存与精度的权衡。\n\n\n\n## 1. Introduction\n\n> 总结起来，这项工作的贡献如下：\n>\n> 1. Dropout  的一种变体，Structural Dropout，它训练一个嵌套网络的集合，以后可以在不进行额外的重新训练（retraining）的情况下将这些网络分离出来进行压缩。\n> 2. 在 3 个示例任务上验证 Structural Dropout，证明其在保持准确性的同时，各种方法的有效性。\n> 3. Structural Dropout 的实现：[An Implementation of Structural Dropout](https://github.com/JulianKnodt/structural_dropout  \"Strucutural Dropout \")\n>\n> <img src=StructuralDropout_f1.png width=100% />\n>\n> 在训练过程中，Structural Dropout 并不是随机选择要剪枝的索引，而是在统一随机选择索引（a uniformly randomly selected index）后剪枝所有节点，并根据丢弃的特征数量对期望进行归一化。在一定的可能性下，我们运行整个网络，用它间接地监督较小的网络（间接将其用作较小网络的监督）\n\n\n\n\n\n\n\n## 5. Discussion\n\n> Structural Dropout 作为现有架构的最小补充，可以执行超参数搜索和压缩。由于它不需要昂贵的重新训练和额外的领域知识，因此它比特定领域的修改更容易采用，并且与剪枝和量化正交。\n>\n> 在我们的实验中，很明显存在信息饱和的陡峭悬崖，并且可以以最小的精度变化来修剪 50%-80%之间的重要特征。如在 PointNet 上所见，当使用更高的 dropout rate 进行更积极的剪枝时，可以在不损失精度的情况下剪枝高达 80%。\n>\n> Structural Dropout 也可能有助于提高性能，因为纯粹通过对多个模型进行采样，其中一个模型可能在给定任务上表现更好。\n\n\n\n## 6. Limitation\n\n> 虽然我们的方法可以直接添加到现有的体系架构中，但它也有一些缺点。\n>\n> **<font color = green>一个显著的缺点是 SD 增加了搜索空间，使问题变得更加困难。</font>** 由于问题更加困难，尽管使用更少的参数可以加快速度，但训练过程可能需要更长的时间。这个训练时间并不比原来长很多，但是不清楚到底长多少。**<font color = green>除了增加的训练时间之外，比较所有通道宽度的验证损失是缓慢的，因为有大量的模型需要测试。</font>** 如果资源可用，这可以很容易地并行化，因为与训练不同，模型将是只读的，否则可以执行稀疏搜索。\n>\n> **<font color = green>SD 的另一个缺点是，它在训练过程中更难以验证和跟踪收敛。</font>** 由于 low channel width，模型在训练过程中可能随机出现精度较低的情况，因此很难确定模型的收敛性。所以，对于之前训练过的模型使用 SD 是有意义的，并且先验收敛参数已经设置。在这些模型上，它也可以用作对通道宽度执行超参数搜索的一种方式。\n>\n> 此外，**<font color = green>虽然我们假设所有的 SD 尺寸（SD sizes）在推断时应该是相同的，但在一个具有各种层的较大模型中，情况可能并非如此</font>**。对所有可能的通道大小选择执行详尽的搜索将是昂贵的，因此有效的搜索策略很重要。一个常见的假设是每个选择都可以独立做出，但我们将这一探索留给未来的工作。\n>\n> 最后，**<font color = green>我们的方法不能剪枝整个层，因为 SD 仅限于改变神经网络的宽度，而不能修改它的深度。</font>** 因此，性能瓶颈是深度的网络将无法获得同样多的好处。\n\n\n\n## 7. Future Work\n\n> 虽然我们展示了 Structural Dropout 在 FC 层上的应用，但同样的原理可以扩展到其他类型的层。例如，同样的思路也适用于 CNN 的 channel dimension，允许对特征进行修剪。另一种可能的扩展是针对 transformers，在 transformers 中选择多个注意力头 （ a number of attention heads）很重要。 Structural dropout 可应用于注意力头的大小和注意力头的数量。我们希望将这项工作扩展到探索在各种架构中使用 Structural Dropout 以实现实用的和高效的压缩。\n\n\n\n## 8. Conclusion\n\n>  Structural Dropout 是一种用于推测时间压缩（inference-time compression）的方法，可用作现有架构的 drop-in layer，以最小的精度损失实现大量的压缩。这是作者所知道的第一种方法，它允许在单个训练会话（a single training session）中训练许多任意压缩的模型，然后联合部署它们，代价是只部署最大的模型。从我们的实验中，我们希望 Structural Dropout 是一种用于压缩神经网络的强大工具。\n\n\n\n\n\n\n\n\n\n# Dropout Regularization for Automatic Segmented Dental Images_ACIIDS_2021\n\n## Abstract\n\n> 深度神经网络是指具有大量参数的网络，是深度学习系统的核心。从这些系统中产生了一个挑战，即它们如何针对训练数据 和 / 或 验证数据集执行。由于所涉及的参数数量众多，网络往往会消耗大量时间，这会导致称为过拟合的情况。**<font color = green>这种方法建议在模型的输入和第一个隐藏层之间引入一个 dropout 层</font>** 。这是非常特别的，与其他领域使用的传统 Dropout 不同，传统的 Dropout 在网络模型的每个隐藏层中引入 dropout 来解决过拟合。我们的方法涉及预处理步骤，该步骤处理数据增广以处理有限数量的牙科图像和侵蚀形态以消除图像中的噪音。此外，使用 canny 边缘检测方法进行分割以提取基于边缘的特征。除此之外，所使用的神经网路采用了 Keras 的顺序模型，这是为了将边缘分割步骤的迭代合并到一个模型中。对模型进行并行评估，首先没有 Dropout，另一个使用大小为 0.3 的 dropout 输入层。在模型中引入 dropout 作为权重正则化技术（weight regularization technique），提高了评估结果的准确性，无论是 precision 准确率还是 recall values 查全率，没有 dropout 的模型为 89.0%，而有 dropout 的模型为 91.3%。\n\n\n\n\n\n## Keywords: \n\n> Deep learning,  Over-fitting,  Regularization technique,  Dropout\n\n\n\n\n\n## 1. Introduction\n\n> 过拟合是各种深度学习系统中普遍存在的问题。当模型在训练集上训练得太好，而在测试集上训练得不太好时，通常会发生过拟合的情况。或者，欠拟合是指我们的模型在训练集和测试集上都表现不佳。\n>\n> 因此，这两种情况可以通过几种称为权重正则化技术（weight regularization technique）的方法来处理。这些方法包括 early stopping, L1,L2 regularization 和 Dropout。在我们的方法中，我们使用 Dropout，包括丢弃神经网络模型中隐藏和可见的单元。这是通过在训练阶段忽略在随机选择的特定神经元集（certain set of neurons）的 units 单元来实现的。从技术上讲，在每个训练阶段，单个 units 要么以 $1-p$ 的概率从网络中丢弃，要么以 p 的概率保留，这样剩下的是一个简化的网路（reduced network）.\n>\n>  Dropout 背后的关键思想是在训练过程中，从神经网络中随机丢弃 units 及其它们的连接，以防止 units 过度自适应（co-adapting）。在训练阶段丢弃不同网络模型的 units 后，这使得测试更容易接近网络平均预测的效果。从 dropout 的过程中，减少了过拟合，并进一步对其他正则化方法进行了重大改进。\n>\n> 在其他研究（例如：[Dropout Regularization in Deep Learning Models with Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/   \"Dropout Regularization \") ）中，展示了 dropout 如何应用于深度学习系统。可以通过多种方式在网络模型中引入 Dropout。它可以作为输入和第一个隐藏层之间的一个层来引入。其次，它可以应用于两个隐藏层之间以及最后一个隐藏层和输出层之间。\n>\n> 我们提出的方法使用了第一个方法，在输入层和第一个隐藏层之间引入 dropout。Dropout 在大型网络中非常有用，它具有各种约束条件，如 learning rate, decay 和 momentum，以调高评估性能。\n\n\n\n\n\n## 5. Conclusion\n\n> Dropout 是一种通过减少过拟合来改进神经网络的技术。相比于模型的隐藏层之间引入一个独立的 dropout layer，在输入可见层中引入该算法得到了很好的结果。深度神经网络模型的训练需要很长时间，从使用我们的方法进行的实验中，我们见证了模型复杂度的降低和训练时间的增加。\n>\n> 我们的 Dropout 方法可以与其他正则化方法一起使用，以获得更好的性能结果。其他可用于获得更好的性能的权重正则化技术包括 early stopping 以解决在模型图上见证的验证损失变化。\n\n\n\n\n\n> 1. 神经元特定的 dropout 是针对训练样本不足或无法获得的问题，提出的方法能够在保证精度的同时，减少训练样本的需求。\n> 2. 用于模型宽度压缩的结构 dropout ，是针对离线压缩模型没有给出衡量模型可以压缩多少的方法且需要对每个压缩模型进行额外的重新训练的问题，作者提出的方法不需要额外的剪枝或者重新训练。\n> 3. 牙医图像自动分割的 dropout 正则化，传统的dropout是在网络模型的每个隐藏层中引入 dropout 来解决过拟合的，而作者是在模型的输入和第一个隐藏层之间引入 dropout层，也就是研究了针对牙医图像自动分割技术， dropout 通过什么样的方式能更好地应用于深度学习系统。\n\n\n\n\n\n\n\n\n\n# Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers_ICML_2022\n\n## Abstract\n\n> Sparsely activated transformers, 如 Mixture of Experts(MoE)，由于其惊人的缩放能力而引起了极大的兴趣，这可以不显著增加计算成本的情况下显著增加模型大小。为了实现这一点， MoE 模型用 Transformer 中的 Mixture-of-Experts  sub-layer 替换前馈子层（feed-forward sub-layer），并使用一个 gating network 门控网络将每个令牌路由到其指定的专家。由于有效训练此类模型的常见做法需要将专家和令牌分布在不同的机器上，因此这种路由策略通常会产生巨大的跨机器通信成本，因为令牌及其分配的专家可能位于不同的机器中。在这篇文章中，作者提出 Gating Dropout，它允许令牌忽略 gating network 并停留在它们的本地机器上，从而减少了跨机器通信。与传统的 dropout 类似，我们也表明， Gating Dropout 在训练中有正则化效果，从而提高了正则化性能。我们验证了 Gating Dropout 在多语言机器翻译任务中的有效性。我们的结果表明， Gating Dropout 改进了最先进的 MoE 模型，具有更快的 wallclock 时间收敛率和更好的 BLEU 分数，适用于各种模型大小和数据集。\n\n\n\n## 6. Conclusion\n\n> 我们提出 Gating Dropout 作为一种通讯高效的正则化技术来训练 sparsely activated transformers。我们观察到 sparsely activated transformers，例如 MoE 模型，通常具有非常高的跨机器通信成本，因为它们需要通过 all-to-all 通信操作将令牌发送给指定专家。Gating Dropout 通过随机跳过 all-to-all 操作来降低通信成本。这种随机跳跃在训练期间也具有正则化效果，从而提高了泛化性能。多语言翻译任务的实验证明了该方法在吞吐量（throughput）、泛化性能和收敛速度方面的有效性。\n>\n> 关于未来的工作，我们正在研究如何通过结合 Gating Dropout 和专家剪枝来提高推理速度。Gating Dropout 目前对推理速度没有影响，因为它只是在推理阶段关闭。此外，我们还对整个训练阶段中不同 dropout rate 的影响感兴趣，因为从探索-利用的角度，探索在训练的早期阶段可能更为重要。\n\n\n\n\n\n# Dropout Regularization in Hierarchical Mixture of Experts_Neurocomputing_2021\n\n专家分层混合中的 Dropout 正则化\n\n## Abstract\n\n> Dropout 是一种非常有效的防止过拟合的方法，近年来已成为多层神经网络的首选正则器。专家的分层混合是一个分层门控模型（hierarchically gated model），它定义了一个软决策树，其中叶子对应于专家，决策节点对应于在其子项之间软选择的门控模型，因此，该模型定义了输入空间的软分层分区。在这项工作中，我们提出了一种用于专家分层混合的 dropout 变体，它忠实于模型定义的树层次结构，而不是像多层感知器那样具有平面的、单元独立的 dropout 应用程序。我们表明，在合成回归数据以及 MNIST 和 CIFAR-10 数据集上，我们提出的 dropout 机制可以防止在树上的过拟合，并在多个层次上提高泛化能力并提供更平滑的拟合。\n\n\n\n## 5. Conclusions\n\n> 我们提出了一种新的 Dropout 机制，可应用于专家分层混合方法及其扩展。与具有条件独立单元的平面架构上的 dropout 相比，我们的方法忠实于模型树层次结构中存在的门控依赖关系（the gating dependencies）。\n>\n> 我们展示了我们的方法在一个合成玩具数据集以及用于数字识别和图像分类任务的两个真实数据集上的有效性。在所有的数据集上，我们看到专家的分层混合在有太多级别和叶子时确实会过拟合，但是我们提出的方法可以作为一种有效的正则化器，其中 dropout rate 作为权衡偏差 bias 和方差 variance 的超参数。\n>\n> 我们还定性地评估 dropout 对模型学习到的表示的影响，这些模型通过提供可视化来可视化 dropout 的影响。由于我们仅不对称地丢弃左子树这一事实，我们的 dropout 方法有效地从具有不同复杂性的树结构模型的集合中采样。这种方法通过充当具有不同复杂性的模型的插值来引入正则化。\n\n\n\n\n\n\n\n# Clustering-Based Adaptive Dropout for CNN-Based Classification_PR_2020\n\n## Abstract\n\n> Dropout 被广泛用于提高深度网络的泛化能力，而目前的 dropout 变体很少动态调整网络隐藏单元或权重的 dropout 概率（dropout probabilities）以适应它们对网络优化的贡献。这篇文章提出了一种基于聚类的 dropout（clustering-based dropout），该算法基于特征、权重或它们的衍生物的网络特征，其中这些特征的 dropout 概率根据相应的聚类组自适应更新以区分它们的贡献。在 Fashion-MNIST 和 CIFAR-10 数据库以及 FER2013 和 CK+ 表情数据库上的实验结果表明，所提出的基于聚类的 dropout 比原始的  dropout 和各种 dropout 变体具有更好的准确性，并且与最先进的算法相比具有最具竞争力的性能。\n\n\n\n## Keywords:\n\n> Feature and weight clustering,  Feature derivative dropout,  Self-adaptive dropout probability,  Facial expression recognition\n\n\n\n## 1. Introduction\n\n> 为了提高深度网络的正则化能力，提出了 regularizer 正则化器、batch normalization 批归一化 和 sparse deep feature learning 稀疏深度特征学习【Sparse deep feature learning for facial expression recognition _PR_2019 】，来减少过拟合的可能性。Dropout 随机丢弃网络隐藏单元或权重，也被应用于很多目标识别问题。受隐藏单元 dropout 的启发， connection (weight) dropout 被提出来随机丢弃权重元素。Khan【Regularization of deep neural networks with spectral dropout _NN_2019 】 等人提出了对 feature map 的光谱变换进行 dropout，其中引入了与 feature map 的重塑维度（the reshaped dimension of the feature map）相对应的三种不同的变体。\n>\n> 然而，传统 dropout 中的隐藏单元或权重是逐个元素地抑制的，这可能会忽略元素块 element block 中隐含的结构信息。Tompson 【Efficient object localization using convolutional networks_CVPR_2014】等人提出 spatial dropout 来丢弃一整个 feature map，即同时丢弃或保留（dropped or retained）一个 feature map 的所有隐藏单元。Poernomo 和 Kang 【Biased dropout and crossmap dropout: learning towards effective dropout regularization in convolutional neural network_NN_2018】根据隐藏单元响应【Learning both weights and connections for efficient neural network _NIPS_2015】的大小将特征分为大小相等的两组，并为每一组分配一个 dropout 概率。同时，提出一个额外的 cross-map dropout，其中不同 feature maps 上相同坐标的元素被同时丢弃或保留。然而，两组不足以区分不同特征之间的贡献，应该设计更多的组。Rohit 等人【Guided dropout _AAAI_2019】根据节点的强度（the strength of each node），提出通过删除节点来引导 dropout。Zhang 等人【ML-LocNet: improving object localization with multi-view learning network _ECCV_2018】提出 region dropout，利用显著区域（salient regions）的组合进行训练。但是，区域的相对位置和大小是固定的，不够灵活。Zhang 等人【Image ordinal classification and understanding: grid dropout with masking label _ICME_2018】提出 grid dropout 来减少搜索空间，以方便对全局特征的探索。然而，相同 grid 中的元素可能彼此之间存在显著差异，因此分配给整个网格 grid 相同的 dropout 概率可能不适用于相同 grid 中显著不同的元素。\n>\n> 对于 dropout 的特征（hidden unit, feature or weight）分组，最先进的 dropout 变体没有以足够的灵活性和多样性来划分这些特征。实际上，对于网络反向传播，即使特征图和权重矩阵中的相邻元素对网络损失的贡献也大有不同。例如，图 1 展示了使用 ResNet18 的表情图像的特征图的活动区域，其中根据 heat maps response 热图影响将不同的 feature maps 分为三个不同的重要性等级，即：insignificant, fair and significant。直观的说，特征元素响应的大小应该与 dropout 概率负相关。然而，传统的 dropout 和最先进的变体无法收集这些 insignificant 无关紧要的 feature maps 或分布在整个 map 上的元素用于 dropout。在这项工作中，在 dropout 中引入了 network element clustering 网路元素聚类，将相似的元素分组，以使它们共享相同的 dropout 概率。因此，利用所提出的聚类方法，可以通过分配一个具有较大 dropout 概率的相应组来同时抑制不重要的元素。\n>\n> 对于 dropout 概率设置，在整个网络训练过程中保持固定的 dropout 概率可能会忽略不同部分对网络优化的动态影响。 Wager 等人【Dropout training as adaptive regularization _NIPS_2013】将 dropout 训练视为一种具有二阶导数近似的自适应正则化形式。 Ba and Frey 等人【Adaptive dropout for training deep neural networks _NIPS_2013】根据矩阵元素性能 matrix elements performance，提出了一种通过更新 a probability mask matrix 概率掩码矩阵的自适应 dropout 方法。在这项工作中， dropout 概率是根据平均特征响应的聚类组（the clustering group of average characteristic response）动态更新的。\n>\n> <img src=Clustering-basedAdaptiveDropout_f1.png width=80% />\n>\n> 图1 残差网络（ResNet18）的最后一个卷积层中示例表达式的 512 个 feature maps 中的 6 个。根据感兴趣区域对 RaFD 数据库的影响，feature maps 可以被分为不同的重要性等级（importance levels），即： insignificant, fair and significant。\n>\n> 为了考虑 dropout 的特征，通常使用深度网络中的全连接层特征（FC layer features，即 layer input）和权重矩阵（weight matrix）作为判别特征来确定识别性能（as the discriminative features to determine the recognition performance）。因此， FC features, the weights 及其 their derivatives 被用作聚类的特征。\n>\n> 这项工作的主要贡献总结如下：\n>\n> * 提出了一种基于 FC features, weights or their derivatives 聚类的新型 dropout 算法；\n> * 根据每组 feature, weight or derivative clustering 的响应幅度，提出了 dropout 概率的自适应更新方法；\n> * 在 Fashion-MNIST 和 CIFAR10 数据库以及 FER2013 和 CK+ 表情数据库上取得了有竞争力的性能。\n>\n> 本文分为以下几个部分。第 2 节介绍了提出的 clustering-based dropout。第 3 节给出了实验结果和相应的插图。最后，在第 4 节提出结论和讨论。\n\n\n\n\n\n## 4. Conclusion\n\n> 考虑到全连接特征、权重、特征和权重的衍生物中的元素对网络优化的贡献不同，提出了一种具有自适应 dropout 概率的基于聚类的 dropout 算法。本文提出的 dropout 进一步嵌入到 ResNet18 的 FC 层，用于四个公共数据库，即 Fashion-MNIST, CIFAR-10, FER2013 和  CK+，实验结果验证了所提出的 dropout 相比于其他 dropout 变体和相关的最新算法的竞争力。\n>\n> 虽然本文提出的基于聚类的 dropout 方法获得了具有竞争力的结果，但仍有进一步改进的空间。首先，引入超参数对网络学习的影响，如簇的数量（the number of clusters），需要进一步研究。其次，深入研究不同模型选择下基于聚类的 dropout 的理论分析。最后，提出的 dropout 应该应用于更多的模型和任务。\n\n\n\n\n\n\n\n# Correlation-based structural dropout for convolutional neural networks_PR_2021\n\n## Abstract\n\n> 卷积神经网络很容易遭受过拟合问题的影响，因为它们在小型训练数据集的情况下经常被过度参数化（over-parameterized）。**<font color = green>传统的 dropout </font>** 随机丢弃 feature units 对于全连接网络效果很好，但 **<font color = green>由于中间特征的高空间相关性（high spatial correlation of the intermediate features）</font>** 而不能很好地正则化 CNNs，这**<font color = green>使得丢弃的信息流过网络，从而导致 under-dropping </font>**问题。为了更好地正则化 CNNs，已经提出了一些 structural dropout methods，例如 **<font color = blue>SpatialDropout 和 DropBlock</font>**，它们通过在连续区域中随机丢弃 feature units 来实现。然而，这些方法 **<font color = blue>可能会因为丢弃关键的判别特征（ critical discriminative features ）而遭受 over-dropping 问题</font>** ，从而限制了 CNNs 的性能。为了解决这些问题，我们提出了一种新颖的 structural dropout method，Correlation based Dropout（CorrDrop），通过 **<font color = purple>基于 feature correlation 丢弃 feature units</font>** 来正则化 CNNs。与之前的 dropout 方法不同，我们的 CorrDrop 可以 **<font color = purple>聚焦于判别信息（discriminative information），并以 spatial-wise 或 channel-wise 的方式丢弃 features</font>** 。在不同的数据集，网络架构和各种任务（如，图像分类和目标定位）上的广泛实验证明了我们的方法优于其他方法。\n\n\n\n## 1. Introduction\n\n> 卷积神经网络已经广泛应用于机器学习社区和计算机视觉任务中，包括图像识别和目标检测。近年来， ResNet, InceptionNet 和 DenseNet 等很多先进的 CNNs 被设计来提高传统 CNNs 的性能。提出了更深和更宽的深度学习模型，以在各种计算机视觉任务中实现最先进的性能。然而，这些模型有数百万个参数，因此很容易遭受过拟合的问题，尤其在训练数据有限的情况下。因此，开发正则化方法来缓解 CNNs 的过拟合是必不可少的。\n>\n> 早期提出的正则化方法有很多，如 weight decay, early stopping, data augmentation, batch normalization 和 dropout。这些方法已被采用作为常规的工具来正则化深度神经网络。其中，传统的 dropout 在全连接（FCs）网络中运行良好。然而，这种 **<font color = green>dropout 并不能通过在 feature map 中随机丢弃单个 feature unit 来有效地正则化 CNNs，因为空间相关的 features 仍然允许丢弃的信息在网络中流动，从而导致 under-dropping 问题。</font>**\n>\n> 为了使 dropout 对 CNNs 更有效，最近提出了一些 structural dropout methods，包括 **<font color = blue> SpatialDropout，Cutout 和 DropBlock</font>** ，以提高 CNNs 的正则化能力。这些方式 **<font color = blue>试图通过在 input/feature space 中随机丢弃整个 channels 或 square of regions </font>** 来正则化 CNNs。然而，由于 the feature units 以随机方式在连续区域中被丢弃，而 **<font color = blue>不考虑图像中的语义信息</font>** ，因此它们存在 over-dropping 问题。 **<font color = blue>这种丢弃 feature unit 的随机方式可能会丢弃 the input/feature maps 中的整个判别区域（the whole of discriminative regions）</font>** ，并限制模型的学习能力。如图 1 所示， **<font color = green>传统的 dropout 丢弃 feature maps 中的 single unit</font>** ，而 **<font color = blue>structural DropBlock 直接丢弃 feature maps 中的 a square of feature units</font>** ，并且可能会将信息语义区域归零。\n>\n> <img src=CorrDrop_f1.png width=80% />\n>\n> 图 1. Dropout, DropBlock 和我们的 Spatial-wise CorrDrop masks（前三行）的示例。红色的部分表示要屏蔽的区域（the regions to be masked）。**<font color = orange>最后一行表示 CorrDrop 对应的相关热图（the corresponding correlation heatmap）</font>** 。**<font color = purple>聚焦于主要目标的 feature units 之间的相关性更强。</font>** 与 Dropout 和 DropBlock 相比，  **<font color = purple>CorrDrop 考虑了判别性信息（discriminative information），自适应地丢弃 feature units </font>** 以缓解 under-dropping  和 over-dropping 问题。\n\n> **<font color = orange>受观察到的目标的判别区域（discriminative region of an object）将具有更高的特征相关性（feature correlations）的启发</font>**（参见图 1 最后一行），我们提出了 Correlation-based Dropout（CorrDrop），这是一种新颖且有效的 CNNs 结构 dropout 方法，该方法考虑到 spatial / channel dimensions 上的 feature correlation，从而丢弃 feature units。\n>\n> 不同于之前的随机丢弃 feature units 的 structural dropout methods（如 DropBlock），我们的  **<font color = purple>CorrDrop 基于判别信息（discriminative information）自适应地丢弃 feature units</font>** 。具体来说，我们 **<font color = purple>首先计算 feature correlation map 以指示最具辨别力的区域（the most discriminative regions），然后自适应地屏蔽那些辨别力较差的区域（those less discriminative regions），即特征相关值较小的区域（regions with small feature correlation values）</font>** 。由于 feature correlation 根据相关性计算的方法可以进一步分为 spatial-wise feature correlation 和 channel-wise feature correlation，我们提出了 CorrDrop 的两种变体： Spatial-wise CorrDrop 和 Channel-wise CorrDrop，它们分别在 spatial dimension 和 channel dimension 自适应地丢弃 features。如图 1 所示，与传统的 dropout 和 DropBlock 遭遇 under-/over-dropping  问题相比，我们的 **<font color = purple>CorrDrop 通过丢弃相关性较低的区域（part of less correlated regions）来生成自适应掩膜（adaptive  masks）</font>**。图像分类的大量实验表明，在公共数据集上不同的 CNNs 架构下，，我们的 CorrDrop 始终优于 dropout he DropBlock。此外，我们也证明了我们的 CorrDrop 在其他计算机视觉任务（如如目标定位）中也能很好地正则化 CNN 模型。\n>\n> 这项工作的初步版本已经作为会议版本【Corrdrop: Correlation based dropout for convolutional neural networks _ICASSP_2020】呈现出来。在这个扩展版本中，我们包含了额外的内容，包括 the channel-wise CorrDrop，更多的消融实验，最先进的 CNNs 实验和额外的视觉任务。主要贡献可以总结如下：\n>\n> * 我们提出了 Correlation based structural dropout（CorrDrop），它丢弃了 feature maps 中不太相关的特征（the less correlated features），这缓解了以前的 dropout 变体以随机方式丢弃 features 的 under-/over-dropping 问题。\n> * 针对 feature map 中 spatial-wise 和 channel-wise features，提出了相应的 Spatial-wise CorrDrop(SCD) 和 Channel-wise CorrDrop(CCD)。实验结果表明，**<font color = purple>它们的互补性在于 SCD 在简单数据集（如 CIFAR-10 和 SVHN）上表现良好，而 CCD 在复杂数据集（CIFAR-100 和 TinyImageNet）上表现出色。</font>**\n> * 在各种数据集，架构和视觉任务上的大量实验表明，我们的方法可以得到持续的改进。\n>\n> 这篇文章剩余部分组织如下。第 2 节简要回顾了深度学习中正则化方法和 注意力机制的相关研究成果。在第 3 节中，我们详细介绍了 CorrDrop 。第 4 节给出了实验结果。最后，我们在第 5 节得出结论。\n\n\n\n\n\n## 3. Methodology\n\n> 由于 under-/over-dropping 问题，大多数现有的基于 dropout 的方法在正则化 CNNs 方面受到限制。通过利用特征的相关性，我们提出一种有效的 structural dropout： correlation-based dropout（CorrDrop），它根据判别信息（discriminative information）自适应地丢弃 feature units，并且可以有效地正则化 CNNs。考虑到 CNNs 的 spatial-wise feature correlation 和 channel-wise feature correlation，我们进一步推导出 CorrDrop 的两种变体，即： Spatial-wise CorrDrop 和 Channel-wise CorrDrop。这两种变体的流程如图 2 和 图 3 所示。在下面的部分中，我们首先描述基于特征正交性（feature orthogonality）的特征相关性（feature correlation）的计算。然后，我们根据 correlation map  来采样 mask。最后，我们说明了 Spatial-wise CorrDrop 和 Channel-wise CorrDrop 的策略。\n\n### 3.1. Feature correlation calculation\n\n> 与以往随机丢弃 feature units  的方法不同，我们试图根据 feature correlation 来自适应地丢弃 feature units，这反映了判别信息（discriminative information）。**<font color = purple>最近的研究【Learning deep features for discriminative localization _CVPR_2016】【Grad–cam: Visual explanations from deep networks via gradient-based localization _ICCV_2017】表明，目标的判别区域（discriminative regions）将有更高的特征相关性（feature correlations）。</font>** 这些观察让我们做出基本假设，即通过丢弃那些 low-correlated features 可以更有效地正则化 CNNs。  **<font color = purple>为了表示 feature correlation，我们使用特征正交性（feature orthogonality）的度量，如之前的工作【Improved training of convolutional filters _CVPR_2019】。</font>** 给定 feature matrix $A = [a_1, ..., a_N]^T \\in \\mathcal{R}^{N \\times K}$，其中，$N$ 是 feature units  的数量，$K$ 是 feature dimension。correlation 的计算可以描述如下：\n>\n> <img src=CorrDrop_e1.png width=50% />\n>\n> 其中 $|.|$ 表示绝对值运算，$I$ 是一个大小为 $N \\times N$ 的单位矩阵。我们首先对  $A$  的每一行进行归一化（normalize），根据特征正交性（feature orthogonality）计算 correlation scores。$P$ 是一个大小为 $N \\times N$ 的矩阵，$P_i$ 表示 $P$ 的第 $i$-$th$ 行。 a single unit 的 $P$ 行的非对角元素表示所有其他 feature units  的投影（Off-diagonal elements of a row of $P$ for a single unit denote projection of all the other feature units）。每行的平均值表示每个 unit 的 correlation score。$F_i$ 的值越高该 unit 与其他 unit 高度相关。\n\n### 3.2. Correlation based dropout mask sampling\n\n> 为了根据 feature correlation 自适应地丢弃 units，我们根据 $F$ 中的值为每个 unit 分配 丢弃概率（dropout probability）。一般情况下，$F_i$ 的值越大，我们的丢弃概率越小。$A$ 中第 $i$-$th$ 个 feature unit 的丢弃概率可以表示为：\n>\n> <img src=CorrDrop_e4.png width=50% />\n>\n> 其中 $i$ 和 $j$ 表示 $F$ 中 feature unit 的索引。为了确保丢弃概率 $\\gamma_i \\in (0,1)$，我们将每个 unit 的 correlation score 进行归一化。\n>\n> 基于丢弃概率 $\\gamma_i$ ，从伯努利分布中采样 dropout mask $M \\in \\mathcal{R}^{N}$\n>\n> <img src=CorrDrop_e5.png width=50% />\n>\n> 经验上，类似于其他 dropout 变体，一个超参数 $p$ 被引入以确保我们的 CorrDrop 不会丢弃太多 feature units。利用基于 correlation 的 dropout mask $M$，我们调整 keep probability 并生成另一个 mask $B \\in \\mathcal{R}^{N}$。当两个 masks 对应的值都为 0 时， the units 被丢弃，并得到 the final dropout mask $S \\in \\mathcal{R}^{N}$。CorrDrop 的 final dropout mask 可制定为：\n>\n> <img src=CorrDrop_e6.png width=50% />\n>\n> 其中 $numel(M)$ 计算 $M$ 的 units 数量，$sum(M)$ 计算值为 1 的 units 数量。\n\n> <img src=CorrDrop_f2.png width=100% />\n>\n> 图 2. Spatial-wise CorrDrop 的过程。1）通过 spatial-wise  local average pooling 对前一层的 feature maps 进行下采样，kernel size 和步长为 $k$，用于局部特征收集和降维（local features gathering and dimension reduction）。2）基于特征正交性（feature orthogonality） 计算 correlation map，并从具有自适应丢弃概率的伯努利分布中采样 dropout mask。3）通过最近邻上采样生成 CorrDrop mask。4）通过对 CorrDrop mask 和 original feature map 进行逐元素相乘得到 regularized feature 正则化特征。\n>\n> <img src=CorrDrop_f3.png width=100% />\n>\n> 图 3. Channel-wise CorrDrop  的过程。1）基于 correlation calculation 计算 correlation vector。2）根据 correlation vector 对 CorrDrop mask 进行采样，即相关越少的通道（the less correlated channels）越容易被丢弃。3）将 CorrDrop mask 与 original feature map 进行逐通道相乘。\n\n### 3.3 Spatial-wise CorrDrop\n\n> 在空间维度，我们假设高度相关的单元（highly correlated units）构成 feature maps 中的判别部分（discriminative parts），这些判别部分应以较高的概率保留。给定中间第 $l$-$th$ 层的 feature maps 为 $V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}$，其中，$N = H \\times W$ 是 feature map 中的 units 的数量， $C$ 是 channels 的数量，$H$ 和 $W$ 分别表示 feature map  的高和宽。每一行 $v_i^{(l)} \\in \\mathcal{R}^C$ 表示一个 unit 的 feature vector。**<font color = green>由于 CNNs 中的特征是局部相关的，所以在 feature map 中丢弃单个 unit 效果不太好【Dropblock: A regularization method for convolutional networks _NIPS_2018】</font>** 。继之前的 DropBlock 在 feature map 中丢弃连续区域（continuous regions）的工作之后，我们进一步考虑每个局部区域的相关性和丢弃单元块（drop blocks of units）。为了获得一个 structural mask，我们首先通过 local average pooling 收集 feature map 中的局部信息，同时降低 feature map 的维度以加快相关性计算（correlation calculation）。当将 block 的大小设置为 $k$ 时，我们在 feature map 上进行 local average pooling， kernel size 为 $k$，步长为 $k$。具体来说，我们在每个 feature map 中从左到右、从上到下扫描每个大小为 $k \\times k$ 的 block，并计算每个 block 的激活值的平均值，可以描述为：\n>\n> <img src=CorrDrop_e9.png width=50% />\n>\n> 得到的 feature map 是 $V^{(l)'} \\in \\mathcal{R}^{N' \\times C}$，其中，$N' = H' \\times W'$，$H' = ceil(\\frac{H}{k})$，$W' = ceil(\\frac{W}{k})$。丢弃概率 $p$ 相应调整为：\n>\n> <img src=CorrDrop_e10.png width=50% />\n>\n> 通过下采样 feature map $V^{(l)'}$，我们采样 corrdrop mask 为：\n>\n> <img src=CorrDrop_e11.png width=50% />\n>\n> 其中， $\\Phi(.)$ 是如公式(1)-(3) 所示的特征相关性计算函数，$\\Psi(.)$ 表示如公式(4)-(8)所示的 dropout mask sampling operation。为了生成 structural mask，我们采用最近邻上采样的方法将 corrdrop mask $S_s^{(l)'} \\in \\mathcal{R}^{H' \\times W'}$ 上采样到 $S_s^{(l)} \\in \\mathcal{R}^{H \\times W}$。$S_s^{(l)}$ 中的每一个 zero entry 将被扩展为 $k \\times k$ blocks。因此，square regions of units 将被丢弃。最后，将 the spatial-wise corrdrop mask 与 the original feature maps $V^{(l)}$ 的每一个通道相乘，并 masks out 掉部分 feature regions，其表示为：\n>\n> <img src=CorrDrop_e12.png width=50% />\n>\n> 其中，$\\odot$ 表示逐点相乘运算。过程如图 2 所示。采用这种方式，我们根据局部信息来计算 feature correlation，并丢弃具有 small average correlation  的 square of regions。\n\n### 3.4. Channel-wise CorrDrop\n\n> 除了 spatial-wise features 之外，值得注意的是，**<font color = purple>每个 CNN filter 可以检测到输入数据的不同模式，即 channel-wise features 对应不同的语义模式。</font>** **<font color = green>【Weighted channel dropout for regularization of deep convolutional neural network_AAAI_2019】中的工作表明，more semantic feature channels 具有 more class-specific，其中包括一些冗余和较少激活的通道。</font>** 因此，我们尝试基于 channel-wise feature correlation 来丢弃那些不相关的特征通道并提高泛化能力，从而产生我们的 Channel-wise CorrDrop。类似于 Spatial-wise CorrDrop，给定中间第 $l$-$th$ 层的 feature maps 为 $V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}$，其中 $N = H \\times W$ 是 feature map 中的 unit 数，$C$ 是通道数，$H$ 和 $W$ 分别是 feature map 的高和宽。我们首先将第 $l$-$th$ 层 feature map $V^{(l)}$ reshape 为 $U^{(l)} = [u_1^{(l)}, ..., u_C^{(l)}]^T \\in \\mathcal{R}^{C \\times N}$。同理， the channel-wise dropout mask 计算为：\n>\n> <img src=CorrDrop_e13.png width=50% />\n>\n> 其中， $p$ 是 dropout probability，$F_C^{(l)} \\in \\mathcal{R}^C$ 是 correlation map，$S_C^{(l)} \\in \\mathcal{R}^C$ 是 corrdrop mask。按如下方式执行 the channel-wise corrdrop：\n>\n> <img src=CorrDrop_e14.png width=50% />\n>\n> 其中 $\\odot$ 指逐通道相乘，如果 $S_C^{(l)}$ 中的第 $j$-$th$ 个元素为0则 $U^{(l)}$ 的第 $j$-$th$ 个channel 将被置 0。\n\n\n\n\n\n\n\n## 5. Conclusions\n\n> 在本文中，我们提出一种新颖且有效的 structural dropout 来有效地正则化 CNNs。与现有的正则化方法会遇到 CNNs 的 under/over-dropping 问题不同，我们的方法通过基于 spatial and channel dimensions 的feature correlation 丢弃 feature 来解决这些问题。大量实验表明我们的方法在不同的机器视觉额任务，网络架构和数据集上优于其他同类方法。此外， the feature activation map 的可视化让我们了解到我们的方法可以强制模型学习更紧凑的表示（learn more compact representations）。除了图像分类任务以外，我们还验证了我们的方法在弱监督目标定位方面的有效性，并揭示了我们的方法在各种计算机视觉任务中的潜在应用。我们还表明，我们的方法可以很容易地插入普通的 CNNs 架构以正则化 CNNs。我们相信我们提出的 CorrDrop 可以作为计算机视觉社区中地通用正则化技术。\n>\n> 在未来的工作中，我们将进一步研究我们的方法在其他计算机视觉任务中的有效性，例如目标检测，语义分割等等。另一方面，图 8 中的 feature maps  的可视化启发我们继续利用特征的相关性来进一步提高网络的表征能力。\n\n\n\n\n\n# Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification_2021\n\n## Abstract\n\n> 在细粒度视觉分类（FGVC）任务中，从同一超类别（如鸟）中对一个目标的子类别进行分类，**<font color = green>高度依赖于多个判别特征</font>**。**<font color = blue>现有方法主要通过引入  attention mechanisms 来定位判别部分或特征编码方法以弱监督的方式提取高度参数化的特征来解决这个问题</font>**。在这项工作中，我们提出了一种名为 Channel DropBlock（CDB）的轻量级但有效的正则化方法，并结合两个可选的相关度量（alternative correlation metrics）来解决这个问题。**<font color = purple>关键思想是在训练阶段随机屏蔽（mask out）一组相关通道，从协同适应中破坏特征，从而增强特征表示（enhance feature representations）</font>**。在三个基准 FGVC 数据集上的大量实验表明，CDB 有效地提高了性能。\n\n> <img src=CDB_f1.png width=100% />\n>\n> 图1 CDB block 的说明。通道相关矩阵（the channel correlation matrix）是根据不同的度量生成的。然后，通过对 input feature map 应用 drop mask ，将一个通道及其对应的视觉组（its corresponding visual group）随机丢弃，丢弃的元素为 0，否则为1。\n\n\n\n## 1 Introduction\n\n> 本论文贡献总结如下：\n>\n> 1）我们通过提出一种新颖的轻量级正则化结构来**<font color = purple>解决 FGVC 任务中判别特征学习（discriminative feature learning）的挑战</font>**，该结构丢弃一组相关通道来激发网络增强特征表示，从而提取更多的判别模式（discriminative patterns）。\n>\n> 2）我们提出两个指标来度量不同特征通道之间的成对相关性，这可以帮助我们深入了解特征通道。\n>\n> 3）我们在三个流行的细粒度基准数据集上进行了大量实验，结果表明，当应用于基线网络或集成到现有方法时，所提出的 CDB 显著提高了 FGVC 的性能。\n\n\n\n> <img src=CDB_a1.png width=100% />\n\n\n\n\n\n## 3 CDB: Channel DropBlock\n\n> 在本节中，我们介绍了所提出的 Channel DropBlock（CDB）的细节。它是一种基于 dropout 的正则化技术，可以很容易地应用于分类模型的 convolutional feature maps，以改善 feature representations。我们首先描述动机 motivation，并与相关方法进行比较（第 3.1 节）。然后我们描述 Channel DropBlock 算法，该算法基于 channel correlation matrix（第 3.2 和 3.3 节）丢弃 correlated channel groups。\n\n### 3.1 Motivation\n\n> 正如之前的工作【】所示，**<font color = green>卷积特征的每个通道对应一个视觉模式</font>。**然而，由于模式之间的共同适应性，只有部分模式有助于最终预测，这将降低推理准确性，尤其是当子类别接近且难以区分时（例如，在 FGVC 任务中）。虽然 dropout 能有效地破坏特征中的协同适应性，但它对卷积特征通道的效果较差，因为这些通道是成对相关的，并且如果我们单独丢弃通道，关于输入的模式仍然可以发送到下一层。这种直觉建议我们屏蔽一组相关的通道（mask out a correlated group of channels）而不是当个通道（a single channel），以鼓励模型学习更多判别部分（discriminative parts）。**<font color = purple>CDB 的主要动机是破坏协同适应性，诱导模型充分利用更具判别性的特征（more discriminative features）。这是通过随机屏蔽整个相关通道组来实现的，这仅仅有助于最终预测的一个视觉证据。</font>**\n>\n> 我们最初开发 CDB 作为一种 attention-based  的方法，专门从 the input feature 中移除 the most important channel groups。这条线索类似于 ADL 的想法【】，因为我们开发了一个重要的分支和一个 dropout 分支，它们是随机选择的，并以对抗方式突出重要通道（highlight important channels）并移除最大激活的组（remove maximally activated group）。我们将这个实验作为消融研究进行，与随机选择的实验相比，改进有限，因为随机的实验可以给出更多的遮挡组合，并且更有可能破坏通道之间的协同适应（destruct co-adaptations between channels）。**<font color  = orange>我们的所有实验都专注于随机选择的 Channel DropBlock。</font>**\n>\n> 与 MA-CNN【】在 the final feature map 上聚类通道（clusters channels）并为每个聚类设置单独的分类器相比，本文提出的 CDB 被设计作为一个正则化块（a regularization block），更灵活地应用于任何分类模型的 convolutional feature maps。\n>\n> 相比于 SpatialDropout【】，**<font color = purple>CDB 强调通道之间是相互关联的，视觉证据仍然可以通过单独的 dropout 发送到下一层。</font>**\n>\n> 与 DropBlock 【】在空间上丢弃相关单元（drops correlated units）相比，**<font color = purple>提出的 CDB 计算逐通道的相关性（calculates correlations channel-wise），并且可以通过我们提供的两个独特的相关性度量（two unique correlation metrics）来捕获更精确的视觉证据。</font>**\n\n\n\n### 3.2 Channel DropBlock Algorithm\n\n> Algorithm 1 和 Figure 1 展示了  the Channel DropBlock 的主要过程。具体来说，CDB 的输入是 a convolutional feature map $F \\in \\mathcal{R}^{C \\times H \\times W}$，其中，$C$ 是通道的数量，$H$ 和 $W$ 分别表示 $F$ 的高和宽。**<font color = purple>我们通过计算 each feature channel 之间的两两相似度来获得 the correlation matrix $M \\in \\mathcal{R}^{C \\times C}$</font>**(描述在 3.3 节)。为了获得  the drop mask，CDB 首先从 $M$ 中随机选择一行，通过将 top $\\gamma$ 个最相关的元素设置为0，其它元素设置为1，来生成 the drop mask $M_d \\in \\mathcal{R}^{C}$。然后，通过广播乘法（broadcasting multiplication）将 the drop mask 应用于 the input feature map。通过这种方式，连续组中的特征（features in a contiguous group）被一起丢掉，这隐藏了一个特定的判别模式，并鼓励模型学习其他有助于最终预测的判别信息（discriminative information）。与 dropout 类似，所提出的 CDB 仅在具有归一化的训练阶段起作用，在推理阶段不涉及额外的参数和计算成本。\n>\n> CDB 有两个主要的超参数：$insert_pos$ 和 $\\gamma$。参数 $insert_pos$ 表示 CDB 应用的位置，$\\gamma$ 控制 dropped group 中的通道数量。\n>\n> **$insert_pos$ 的影响：**随着 CNN 结构越来越深，高层神经元对整个图像反应强烈，语义丰富，但不可避免地会丢失来自小的判别区域的细节信息（detailed information from small discriminative regions）。由于 $insert_pos$ 的设置不同， the input feature map 的信息也不同。在我们的实验中，我们完成了一项消融实验（图 Table 2 所述），将提出的 CDB block 应用于 CNN 的不同的层。\n>\n> **设置 $\\gamma$ 的值：**另一个超参数涉及到我们何时将 correlated channels 聚合成 group。这里我们将 $\\gamma$ 定义为进行 CDB 时被丢弃的组中（a dropped group）通道的百分比。在实践中，不同的 correlation metrics 会导致不同的簇数 (cluster numbers) 和 each cluster 中的通道数，因此，$\\gamma$ 的设置与我们选择的 correlation metrics 不同。\n>\n> <img src=CDB_f2.png width=100% />\n>\n> 图2：channel correlation metrics 的说明：(a) max activation，将通道分组为有区别的局部区域（discriminative local region）; (b) bilinear pooling metric 双线性池化度量，根据 visual pattern 视觉模式对 channel 进行分组。 \n\n\n\n### 3.3 Channel Correlation\n\n> 理想情况下， a correlation metric 应该是对称的，并且可以将 feature channels 聚集到不同的 visual pattern groups。在本文中，我们研究了两个候选 metric 来评估 channel 之间的 correlation。\n>\n> **Max activation metric.** 为了将 feature channels 分成 group，一个直观的想法是将 feature channels 分成不同的焦点局部区域（different focused local regions）。**<font color  = purple>受 MA-CNN【】思想的启发，我们将最大激活位置接近的通道视为一个 pattern group（we treat channels with close maximal activation position as one pattern group）。</font>** 我们使用 $3 \\times 3$ average pooling 来平滑 feature maps 并使用 argmax(.) 操作来获得 each feature channel 中峰值响应的坐标，这将 the input feature map $F$ 转换为位置矩阵 $P \\in \\mathcal{R}^{C \\times 2}$，由下式给出：\n>\n> <img src=CDB_e1.png width=60% />\n>\n> 其中，$t_x^i$，$t_y^i$ 是第 $i^{th}$ 个 channel 的峰值响应的坐标。然后计算每个激活位置之间逐对的欧氏距离并获得 the correlation matrix $M$：\n>\n> <img src=CDB_e2.png width=60% />\n>\n> 在该度量中，feature channels 被分组成 discriminative local regions 具有区别性的局部区域。此外，它是一个无参数的度量，不涉及可学习的参数。 Figure 2(a) 展示了 the max activation metric 的过程。\n>\n> **Bilinear pooling metric.** 我们还研究了一个基于 bilinear pooling operator 的 correlation metric【】，它计算归一化余弦距离来度量通道相似性（channel similarities）。该方法将 the input feature map $F$ 重构（reshape） 为一个形状为 $C \\times HW$ 的矩阵，记为 $X \\in \\mathcal{R}^{C \\times HW}$。然后通过 a normalization function 和 a bilinear pooling operator 对 reshaped matrix 重构后的矩阵进行输入，得到 channels 之间的 spatial relationship：\n>\n> <img src=CDB_e3.png width=60% />\n>\n> 其中，$\\mathcal{N(.)}$ 表示矩阵第 2 维度上的 L2 normalization function。$XX^T$ 是 the homogeneous bilinear feature 齐次双线性特征。相比于 the max activation metric，bilinear pooling metric 中的 each channel group 表示 one specific visual pattern。同样，在训练阶段和推理阶段都不涉及可训练的参数。Figure 2(b) 展示了 the bilinear pooling metric 的过程。\n\n\n\n\n\n\n\n## 5 Conclusion\n\n> 本文引入了一种新颖的正则化技术，Channel DropBlock（CDB），该技术通过相关性度量对通道进行聚类，并在训练阶段随机丢弃一个相关通道组（a correlated channel group），从而破坏协同适配的特征通道（destructs feature channels from co-adaptations）。我们证明，与现有的 FGVC 方法相比，CDB 在增强特征表示和提取多种判别模式方面更加轻量级和有效。我们在三个经过广泛测试的细粒度数据集上进行了实验，验证了所提出方法的优越性。未来工作的两个特别有趣的方向包括探索具有自适应大小的通道分组方法，以及使用综合指标度量通道相关性。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/CNN-Regularization.md","raw":"---\ntitle: CNN-Regularization\ntop: false\ncover: false\ntoc: true\ndate: 2022-06-10 15:55:36\npassword:\nsummary:\ndescription:\ncategories: CNN Regularization\ntags: CNN Regularization\n---\n\n\n\n# 正则化\n\n> 过拟合 --> 在训练集上表现很好，但在测试集上效果不佳。（随着模型复杂度增加，训练误差减小，测试误差不减）\n>\n> 发生过拟合时，模型的偏差小而方差大。\n\n> 正则化 --> 解决模型过拟合问题！\n>\n> 正则化通过对学习算法进行微调，使得该模型具有更好的泛化能力，改善模型在未知数据上的表现。\n\n\n\n> **[过拟合](https://zh.m.wikipedia.org/zh-hans/%E9%81%8E%E9%81%A9 \"什么是过拟合？\")**\n>\n> **<font color = green>过拟合的本质是训练算法从统计噪声中获取了信息并表达在了模型结构的参数当中。</font>**\n>\n> 过拟合 --> 指过于紧密或精确地匹配训练集数据，以致于无法良好地拟合测试集数据 --> **<font color = green>过拟合一般可视为违反奥卡姆剃刀原理（简约法则，若无必要，勿增实体）</font>**\n>\n> **<font color = green>过拟合存在的原因 --> 选择模型的标准和评价模型的标准不一致导致的。</font>** 选择模型时往往选取在训练数据上表现最好的模型；而评价模型时则是观察模型在不可见数据上的表现。当模型尝试“记住”训练数据而非从训练数据中学习规律时，就可能发生过拟合。\n>\n> 在统计学系和机器学习中，为了避免或减轻过拟合，可使用以下技巧：\n>\n> <font color = green>$\\blacksquare$ 模型选择 Model Selection</font>  --> 给定数据的情况下，从一组模型中选出最优模型（或具有代表性的模型）的过程。\n>\n> <font color = green>$\\blacksquare$ 交叉验证 Cross-Validation</font>-->  一种预测模型拟合性能的方法。包括 Leave-one-out Cross-Validation 和 K-fold Cross Validation。\n>\n> <font color = green>$\\blacksquare$  提前停止 Early Stopping</font>  --> 当训练集上的 loss 不再减小（减小的程度小于某个阈值）时停止继续训练，即用于提前停止训练的回调函数callbacks。\n>\n> **<font color = green>$\\blacksquare$  正则化 Regularization</font>** -->  机器学习和逆问题领域中，**<font color = green>正则化</font>** 是指为解决适定性问题或过拟合而加入额外信息的过程。\n>\n> <font color = green>$\\blacksquare$  剪枝 Pruning</font> -->  机器学习和搜索算法中，通过移除决策树中分辨能力较弱的部分而减小决策树大小的方法，其降低了模型的复杂度，因此能够降低过拟合风险，从而降低泛化误差。\n>\n> <font color = green>$\\blacksquare$ 贝叶斯信息量准则 Bayesian Information Criterion / Schwarz Information Criterion</font>  -->  在有限集合中进行模型选择的准则。\n>\n> <font color = green>$\\blacksquare$  赤池信息量准则 Akaike Information Criterion</font> -->  基于信息熵，用于评估统计模型的复杂度和衡量统计模型拟合资料的优良性的一种标准。\n>\n> <font color = green>$\\blacksquare$  dropout</font>  -->  Hinton 提出的一种正则化方法，即在神经网络训练过程中，通过随机丢弃部分神经元，来减小神经元之间的协同适应性，从而降低网络过拟合风险。\n\n\n\n> **[深度学习中的正则化策略](https://zhuanlan.zhihu.com/p/37120298 \"正则化？\")**\n>\n> 正则化 --> 深度学习中，正则化是惩罚每个节点的权重矩阵。\n>\n> 用于深度学习的正则化技巧：\n>\n> <font color = green> L1 & L2 正则化 </font>  --> 均是在损失函数 cost function 中增加一个正则项，即：\n>\n> $$ Cost function = Loss(say, binary_{cross entropy}) + Regularization_{term} $$\n\n\n\n# Regularization For Deep Learning: A Taxonomy_2017\n\n> 正则化的定义很多，作者提出一个系统的，统一的分类方法将现有的正则化方法进行分类，并为开发人员提供了实用的正则化方法的建议。\n>\n> 作者将目前的正则化方法分类为 affect data 影响数据、network architectures 网络架构、error terms 错误项、regularization terms 正则化项、optimization procedures 优化过程 这几种方法。\n>\n> 在<font color = green>传统</font>意义上的优化和<font color = green>较老</font>的神经网络文献中，<font color = green>正则化只用于损失函数中的惩罚项</font>。\n>\n> 2016年 Goodfellow 等人 将正则化广泛定义为：<font color = green>为减少模型的测试误差，而非训练误差，对学习算法所作的任何修改。</font>即，正则化被定义为：\n>\n> **<font color = green>任何使模型能够更好地泛化的辅助技术，即在测试集上产生更好效果的技术都被称为正则化。</font>**--> 该定义更符合机器学习文献，而非逆问题文献。可包括<font color = green>损失函数的各种属性，损失优化算法或其他技术。</font>\n>\n>  \n\n> 为了为接下来提出的分类法的顶层提供一个证明，作者梳理了机器学习的理论框架。\n>\n> ## 理论框架\n>\n> 机器学习的中心任务是 <font color = green> 模型拟合：找到一个函数 $f$，它能很好地近似从输入 $x$ 到期望输出 $f(x)$ 的期望映射。</font>\n>\n> 很多应用中，<font color = green>神经网络已被证明是一个选择 $f$ 的很好的函数族。</font>\n>\n> 一个神经网络是一个具有可训练权值 $w \\in W$的函数 $f_w : x --> y$。\n>\n> <font color = green>训练网络意味着找到一个使损失函数 $L$ 最小的权重配置 $w^*$ ：\n>\n> <img src=1.png width=70% />\n>\n> 通常损失函数采用期望风险的形式：\n>\n> <img src=2.png width=70% />\n>\n> 其中包含两部分：<font color = green>误差函数$E$和正则化项$R$。</font>\n>\n> **<font color = green>误差函数 --> 依赖于目标，并根据其与目标的一致性对模型预测分配惩罚。</font>**\n>\n> **<font color = green>正则化项 --> 根据其他标准对模型进行惩罚。这个标准可以是除了目标以外的任何东西，例如权重。</font>**\n>\n> 由于数据分布 $P$ 是未知的，所以根据公式（2）期望风险不能直接降到最低。相反，给出了从分布中采样的训练集 $D$。**<font color = green>期望风险的最小化可以通过最小化经验风险 $\\mathcal{\\hat{L}}$ 得到。</font>**\n>\n> <img src=3.png width=70% />\n>\n> 其中，$(x_i, t_i)$ 是来自训练集 $D$ 的样本。\n>\n> 公式（3）给出了最小化经验风险，作者根据公式中的元素，将正则化方法分为以下几类：\n>\n> $\\blacksquare$ $\\mathcal{D}$：训练集 --> affect data 影响数据\n>\n> $\\blacksquare$ $\\mathcal{f}$：选择的模型族 --> network architectures 网络架构\n>\n> $\\blacksquare$ *$E$*：错误函数 --> error terms 错误项\n>\n> $\\blacksquare$ *$R$*：正则化项 --> regularization terms 正则化项\n>\n> $\\blacksquare$   优化过程本身 --> optimization procedures 优化过程\n\n\n\n## 1 通过数据进行正则化\n\n>训练模型的质量很大程度取决于训练数据。\n>\n><font color = green>通过对训练集 $\\mathcal{D}$ 应用一些变换 生成一个新的数据集，从而实现对数据的正则化。</font>\n>\n>进行数据正则化可根据以下俩原则：\n>\n>1）进行特征提取或预处理，将特征空间或数据分布修改为某种表示，从而简化学习任务；\n>\n>2）允许生成新样本来创建更大的、可能是无限的增强数据集。\n>\n>这两个原则在某种程度上是独立的，也可相结合。它们均依赖于（随机）参数的转换：\n>\n><img src=D2.png width=70% />\n>\n>作者给出第二个定义：\n>\n>**<font color = green>带有随机参数的变换是一个函数 $\\tau_{\\theta}$，其参数 $\\theta$ 遵循某种概率分布。</font>**\n>\n>所以，在此情况下，考虑 $\\tau_{\\theta}$ 可作用于 <font color = green>网络输入、隐层激活或目标。</font>  输入被高斯噪声破坏<font color = green>（给输入数据添加高斯噪声）</font>是随机参数变换的一个例子。\n>\n><img src=4.png width=70% />\n>\n>**<font color = green>变换参数的随机性带来新样本的产生，即 data augmentation 数据增广。数据增广通常专门指输入变换或隐藏激活。</font>**\n>\n>**<font color = blue>作者根据变换的性质及其参数的分布对基于数据的正则化方法进行分类。</font>**\n>\n>### 变换参数 $\\theta$ 的随机性\n>\n>$\\blacksquare$ **<font color = blue>确定性参数</font>**：参数 $\\theta$ 遵循 delta 分布，数据集大小保持不变。\n>\n>$\\blacksquare$ **<font color = blue>随机性参数</font>**：允许生成一个更大的，可能是无限的数据集。 $\\theta$ 的采样方法多种多样，有：\n>\n>1）**<font color = blue>随机</font>**：从指定的分布中画一个随机的 $\\theta$\n>\n>2）**<font color = blue>自适应</font>**： $\\theta$的值是一个优化过程的结果，通常目标是一个最大化变换样本上的网络误差（这种具有挑战性的样本被认为是当前训练阶段信息量最大的样本），或最小化网络预测和预定义的假目标 $t'$ 之间的差异。\n>\n>> $\\star$ **<font color = blue>约束优化</font>**：通常在硬约束下最大化误差找到 $\\theta$（支持 $\\theta$ 的分布控制最强的允许变换）；\n>>\n>> $\\star$ **<font color = blue>无约束优化</font>**：通过最大化修正误差函数找到 $\\theta$，使用 $\\theta$ 的分布作为权重（为了完整性在此提出，但并未测试）；\n>>\n>> $\\star$ **<font color = blue>随机</font>**：通过获取固定数量的 $\\theta$ 样本并使用产生最高误差的样本来找到 $\\theta$.\n>\n>### 对数据表示的影响\n>\n>$\\blacksquare$ **<font color = green>保留表示的转换 Representation-preserving transformations</font>**：保留特征空间并尝试保留数据分布。\n>\n>$\\blacksquare$ **<font color = green>保留修改的转换 Representation-modifying transformations</font>**：将数据映射到不同的表示（不同的分布甚至新的特征空间），这可能会解开原始表示的潜在因素并使学习问题更容易。\n>\n>### 转换空间\n>\n>$\\blacksquare$ **<font color = blue>输入</font>**：对输入 $x$ 进行变换；\n>\n>$\\blacksquare$ **<font color = blue>隐藏特征空间</font>**：对样本的一些深层表示进行变换（这也使用部分 $f$ 和 $w$ 将输入映射到隐藏特征空间；这种变换在网络 $f_w$ 内部起作用，因此可被认为是架构）\n>\n>$\\blacksquare$ **<font color = blue>目标</font>**：转换应用于 $t$（只能在训练阶段使用，因为标签在测试时没有显示给模型）\n>\n>### 普遍性\n>\n>$\\blacksquare$ **<font color = green>通用 Generic</font>** ：适用于所有数据域；\n>\n>$\\blacksquare$ **<font color = green>特定域 Domain-specific</font>**：针对当前问题的特定（手工制作），例如图像旋转。\n>\n>### $\\theta$ 分布的依赖关系\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta)$</font>**：所有样本的 $\\theta$ 分布相同\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|t)$</font>**：不同目标（类别）的 $\\theta$ 分布可能不同\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|t')$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|x$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|\\mathcal{D})$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|X)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|time)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> $p(\\theta|\\pi)$</font>**：\n>\n>$\\blacksquare$ **<font color = blue> 以上方法的综合</font>**：即 $p(\\theta|x, t)$, $p(\\theta|x, \\pi)$, $p(\\theta|x, t')$, $p(\\theta|x, \\mathcal{D})$, $p(\\theta|t, \\mathcal{D})$, $p(\\theta|x, t, \\mathcal{D})$\n>\n>### 阶段\n>\n>$\\blacksquare$ **<font color = green> 训练</font>**：训练样本的转换。\n>\n>$\\blacksquare$ **<font color = blue> 测试</font>**：测试样本的转换，例如对样本的多个增强变体进行分类，并将结果汇总在它们之上。\n>\n>**<font color = green> 表1回顾了使用通用转换的现有方法</font>**：\n>\n><img src=t1.png width=100% />\n>\n>**<font color = blue> 表2列出了特定域的方法</font>**，特别侧重于图像领域。最常用的方法是：图像的刚性变形和弹性变形。\n>\n><img src=t2.png width=100% />\n>\n>### 目标保留数据增广\n>\n>目标保留数据增广 --> 在输入和隐藏特征空间中使用随机变换，同时保持原始目标 $t$\n>\n><font color = red> 未完待续！！！</font>\n>\n>### 基于数据的正则化方法的总结\n>\n>作者对基于数据的正则化方法进行了形式化，展示了**<font color = green>看似与数据正则化无关的技术，例如保留目标的数据增广、dropout 或 Batch Normalization 等技术在方法上惊人的近似，都可看做是基于数据的正则化方法。</font>**\n\n \n\n## 2 通过网络架构进行正则化\n\n> 为了实现正则化效果，可以选择具有特定属性或匹配特定假设的网络架构 $f$.\n>\n> ### 关于映射的假设\n>\n> > <font color =green>为了很好地拟合数据 $P$，输入-输出 的映射 $f_w$ 必须具有某些属性。</font>尽管执行理想映射的精确属性可能很难，但可通过关于映射的简化假设来近似它们。<font color = green>这些属性和假设可以**以硬或软**的方式强加于模型拟合。</font>这限制了模型的搜索空间，并允许找到更好的解决方案。\n> >\n> > **<font color = green> 作者讨论的对 输入-输出 映射施加假设的方法是网络架构 $f$ 的选择。</font>**一方面，架构 $f$ 的选择 **<font color = green>硬连接</font>**了映射的某些属性；此外，在 $f$ 和优化算法之间的相互作用中，某些权重配置比其他配置更可能通过优化获得，从而进一步**<font color = green>以软方式限制可能的搜索空间</font>**。\n> >\n> > <font color = green>对映射施加某些假设的补充方法是**正则化项**，以及**（增广）数据集中存在的不变性**。</font>\n> >\n> > 假设可以 **<font color = green>硬连接</font>** 到某些层执行的操作的定义中，和 / 或层之间的连接中。\n> >\n> > **<font color = green>基于网络架构的方法如表三</font>**所示：\n> >\n> > <img src=t3_1.png width=100% />\n> >\n> > <img src=t3_2.png width=100% />\n> >\n> > 在隐藏特征空间中对数据进行变换的正则化方法可被视为体系结构的一部分。也就是说，<font color =green>在隐藏特征空间中对数据进行变换的正则化方法既属于数据正则化，也属于网络架构正则化。</font>\n>\n> ### $\\blacksquare$ 权值共享 Weight sharing\n>\n> >权值共享 --> 在网络的多个部分重复使用某个可训练参数。例如，卷积网络中的**<font color = blue>权值共享不仅减少了需要学习的权重的数量，它还编码了 shift-equivariance 的先验知识和特征提取的局部性。</font>**\n>\n> ### $\\blacksquare$ 激活函数 Activation function\n>\n> > 选择正确的激活函数非常重要。例如：\n> >\n> > 1）**ReLUs ** 在训练时间和准确性方面提高了许多深度架构的性能。**<font color = green>ReLUs 的成功既可归因于：ReLUs 可避免梯度消失问题；也可归因于：它们提供了更有表现力的映射家族 more expressive families of mappings</font>**。\n> >\n> > **<font color = blue> 一些激活函数是专门为正则化设计的。</font>**\n> >\n> > 2）**Dropout** ，**Maxout** 单元允许在测试时更精确地逼近模型集合预测的几何平均值。\n> >\n> > 3）**Stochastic pooling 随机池化** 是最大池化的噪音版本。作者声称，这允许对激活的分布进行建模，而不仅是取最大值。\n>\n> ### $\\blacksquare$ 噪声模型 Noisy models\n>\n> > **Stochastic pooling** 随机池化是确定性模型的随机泛化的一个例子。<font color = green>有些模型是通过向模型的各个部分注入随机噪声来实现的。 Dropout 是最常用的噪声模型</font>\n>\n> ### $\\blacksquare$ 多任务学习 Multi-task learning\n>\n> > **多任务学习  --> 是一种特殊类型的正则化。**它可与半监督学习相结合，在辅助任务上利用未标记数据。\n> >\n> > **元学习**中也使用了任务之间共享知识的类似概念，其中来自同一领域的多个任务被顺序学习，使用先前获得的知识作为新任务的偏差。\n> >\n> > **迁移学习**，将一个领域的只是迁移到另一个领域。\n>\n> ### $\\blacksquare$ 模型选择 Model selection\n>\n> > 可通过评估验证集上的预测来选择几个经过训练的模型（例如，具有不同的架构）中最好的模型。\n\n\n\n## 3 通过误差函数进行正则化\n\n> **<font color = green>理想情况下，误差函数 $E$（表示输出与目标的一致性） 反映了适当的质量概念，在某些情况下还反映了一些关于数据分布的假设。</font>**典型的例子是：**均方误差** 或 **交叉熵**。\n>\n> **<font color = blue> 误差函数 $E$ 也可以具有正则化效果。</font>**例如，Dice coefficient optimization 系数优化，它对类别不平衡具有鲁棒性。\n\n\n\n## 4 通过正则化项进行正则化\n\n> **<font color = green>正则化可以通过在损失函数中添加正则化器 $R$ 来实现。</font>**与误差函数 $E$ （表示输出与目标的一致性）不同，**<font color = green>正则化项独立于目标。</font>**相反，**<font color = blue>正则化项用于编码所需模型的其他属性，以提供归纳偏差（即关于映射的假设，而不是输出与目标的一致性）。</font>** 因此，**<font color = green>对于未标记的测试样本，正则化项 $R$ 的值能计算出来，而误差函数 $E$ 不能计算。</font>**\n>\n> **正则化项 $R$ 与目标 $t$ 的独立性有一个重要含义：它允许额外使用未标记的样本（半监督学习），根据其符合一些期望的属性来改进学习模型。**\n>\n> > **<font color = green> 一个经典的正则化方法是 weight decay 权值衰减**\n> >\n> > <img src=5.png width=70% />\n> >\n> > 其中，$\\lambda$ 是一个加权项，用于控制正则化对一致性的重要性。\n> >\n> > **从贝叶斯的角度来看** ，权重衰减对应于使用对称的多元正态分布作为权重的先验：\n> >\n> > $$ p(w) = \\mathcal{N}(w|0,\\lambda^{-1}I) $$\n> >\n> > <img src=e1.png width=70% />\n> >\n> > **图4 回顾了现有的通过正则化项进行正则化的方法。权重衰减（L2正则化）似乎仍然是最流行的正则化项。**\n> >\n> > **<font color = green>L2 正则化的目的就是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题，所以权重衰减也叫 L2 正则化。</font>**\n> >\n> > <img src=t4_1.png width=100% />\n> >\n> > <img src=t4_2.png width=100% />\n> >\n> > <img src=t4_3.png width=100% />\n\n\n\n## 5 通过优化进行正则化\n\n> **<font color = green>随机梯度下降（SGD）及其衍生</font> 是深度神经网络中最常用的优化算法，也是我们关注的中心。**作者也在下文列出了一些替代方法。\n>\n> **<font color = green>随机梯度下降法（SGD）</font> 是一种采用以下更新规则的迭代优化算法 ：**\n>\n> <img src=7.png width=70% />\n>\n> 如果算法在合理的时间内达到较低的训练误差（与训练集的大小呈线性关系，允许多次通过训练集 $\\mathcal{D}$，那么在某些温和的假设下，解决方案的泛化效果很好，从这个意义上来说：\n>\n> **<font color = green> SGD 作为一个隐性的正则化器：即使没有使用任何额外的正则化器，较短的训练时间也能防止过拟合。</font>** --> 这与论文《Understanding deep learning requires rethinking generalization  》中的观点一致：该论文作者在一系列实验中发现，**<font color = blue>正则化（例如 Dropout、数据增广和权重衰减）本身既不是良好泛化的必要条件，也不是充分条件。</font>**\n>\n> 作者将通过优化进行正则化的方法分为三组：\n>\n> $\\blacksquare$ 初始化 /热启动方法\n>\n> $\\blacksquare$ 更新方法\n>\n> $\\blacksquare$ 终止方法\n>\n> ### $\\blacksquare$ Initialization and warm-start methods 初始化/热启动方法\n>\n> > \n>\n\n\n\n\n\n## 建议、讨论、结论\n\n> > ### 1 该分类法的优势：\n> >\n> > 作者认为<font color = green>这样的分类法的优势</font>有两个方面：\n> >\n> > 1）它为正则化方法的用户提供了现有技术的概述，并让他们更好地了解如何为他们的问题选择理想的正则化技术组合。\n> >\n> > 2）它对于开发新方法很有用，因为它全面概述了可用于正则化模型的主要原则。\n>\n> > ###  2 作者建议：\n> >\n> > #### 1. **<font color = green>对现有正则化方法用户的建议</font>**\n> >\n> > 总的来说，<font color = green>尽可能多地使用数据中包含的信息以及先验知识，并主要从流行的方法开始</font>，以下程序可能是有帮助的:\n> >\n> > $\\blacksquare$ 对于第一步的常见建议：\n> >\n> > 1）<font color = green>深度学习就是要把变异的因素分解开来。</font>应该选择一个合适的数据表示；**已知的有意义的数据转换不应该外包给学习。** 在几种表征中，**冗余地提供相同的信息是可以的。**\n> >\n> > 2）<font color = green>输出非线性和误差函数应该反应学习目标。</font>\n> >\n> > 3）一个好的起点是通常工作良好的技术（例如，ReLU，成功的架构）。**超参数（和架构）可以联合调优，但是很缓慢**（根据经验进行插值 / 推断，而不是尝试太多的组合）。\n> >\n> > 4）通常，<font color = green> 从一个简化的数据集（例如，更少和/或更简单的样本）和一个简单的网络开始是有帮助的，</font> 在获得有希望的结果后，<font color = green>在调优超参数和尝试正则化方法时逐渐增加数据和网络的复杂性。</font>\n> >\n> > $\\blacksquare$ 通过数据进行正则化：\n> >\n> > 1）当不处理几乎无限 / 丰富的数据时：\n> >\n> > $\\star$ 如果可能的话，收集更多的真实数据（并使用考虑到其属性的方法）是可取的：\n> >\n> > * **有标记的样本**是最好的，但无标记的样本也可能有用（兼容半监督学习）。\n> > * **来自相同领域的样本**是最好的，但来自相似领域的样本也会有帮助（兼容领域适应和迁移学习）。\n> > * **可靠的高质量样本**是最好的，但低质量样本也有帮助（它们的信心 / 重要性可以相应地调整）。\n> > * **给额外的任务贴上标签**会很有帮助（与多任务学习兼容）。\n> > * **额外的输入特性（来自额外的信息源）和 / 或数据预处理（即特定于领域的数据转换）**可能会有所帮助（网络架构需要相应的调整）。\n> >\n> > $\\star$ **数据增广**（例如，保留目标的手工特定领域转换）可以很好地弥补有限的数据。如果一直增强数据的自然方法（充分模拟自然转换），则可以尝试（并组合）它们。\n> >\n> > $\\star$ 如果增广数据的自然方法未知或被证明是不充分的，如果有足够的数据可用，就有可能从数据中推断出转换（例如学习图像变形字段）。\n> >\n> > 2） 流行的泛型方法（例如 Dropout 的高级变体）通常也有帮助。\n> >\n> > $\\blacksquare$ 架构和正则化项：\n> >\n> > 1）关于映射的可能的有意义的属性的知识可以被用来如将不变性（对某些转换）硬连接到架构中，或者被表述为正则化项。\n> >\n> > 2）流行的方法也可能有帮助（见表3和表4），但应该选择匹配映射的假设（例如，仅当需要对常规网格数据进行局部和移位等变特征提取时，卷积层才完全合适）。\n> >\n> > $\\blacksquare$ 优化：\n> >\n> > 1）初始化：尽管预训练的现成模型大大加快了原型的制作速度，但良好的随机初始化也应该被考虑。\n> >\n> > 2）优化器：尝试一些不同的方法，包括先进的（例如 Nesterov momentum, Adam, ProxProm），可能会带来更好的结果。正确选择的参数，例如学习率，通常会产生很大的不同。\n> >\n> > \n> >\n> > #### 2. <font color = green>对新正则化方法的开发人员的建议</font>\n> >\n> > 了解最佳方法成功的原因是一个很好的基础。有希望的空白领域（分类法属性的某些组合）是可以解决的。强加在模型上的假设可能会对分类法的大多数元素产生强烈的影响。<font color = green> **数据增广比损失项更有表现力**（损失项只在训练样本的无限小的邻域强制属性；数据增广可以使用丰富的转换参数分布）。</font>数据和损失项以相当软的方式强加假设和不变性，并且可以调整它们的影响，而硬连接网络架构是强加假设的更苛刻的方式。施加它们的不同假设和选项具有不同的优点和缺点。\n> >\n> > \n> >\n> > #### 3. <font color = green> 基于数据方法的未来方向</font>\n> >\n> > 作者认为以下几个有前景的方向值得研究：\n> >\n> > 1） $\\theta$  的自适应采样可能会导致更低的误差和更短的训练时间（反过来，更短的训练时间可能会额外起到隐式正则化的作用）。\n> >\n> > 2）作者认为学习类依赖变换会导致更可信的样本。\n> >\n> > 3）在最近引发了关于真实世界对抗示例及其对摄像机位置变化等变换的鲁棒性 / 不变性的讨论后，对抗示例（以及对它们的网络鲁棒性）领域正获得越来越多的关注。对抗强烈的对抗性例子可能需要更好的正则化技术。\n> >\n> > \n> >\n> > #### 4. 总结\n> >\n> > 在这项工作中，<font color = green>作者为深度学习提供了一个广义的的正则化定义，确定了**神经网络训练的五个主要元素（数据，架构，错误项，正则化项，优化程序）**，通过每个元素描述了正则化，包括对每个元素的进一步、更精细的分类，并从这些子类别中提供了示例方法。</font> 我们没有试图详细解释引用的作品，而只是确定它们与我们的分类相关的属性。我们的工作证明了现有方法之间的一些联系。此外，我们的系统方法通过结合现有方法的最佳特性，能够发现新的、改进的正则化方法。\n\n\n\n********************************************************************************************************************************************************************************************************************************************************************\n\n***********************************************************************************************************************************************************************************************************************************************************************\n\n# Heuristic Dropout: An Efficient Regularization Method For Medical Image Segmentation Models_2022,Tsinghua University\n\n## \tAbstract\n\n> 对于真实场景中的医学图像分割，像素级的准确标注数据量通常较少，容易造成过拟合问题。这篇手稿深入研究了 Dropout 算法，该算法常用于神经网络以缓解过拟合问题。这篇手稿**从解决 co-adaptation problem 协同适应问题的角度**出发，解释了 <font color = green>Dropout 算法</font>的基本原理，并讨论了<font color =green>其衍生方法存在的局限性</font>。此外我们提出一种新颖的**Heuristic Dropout启发式 Dropout 算法来解决这些局限**。**<font color = green>该算法以信息熵和方差作为启发式规则。</font>** 它指导我们的算法更有效地丢弃遭受协同适应问题的特征，从而更好地缓解小规模医学图像分割数据集的过拟合问题。医学图像分割数据集和模型的实验表明，所提出的算法显著提高了这些模型的性能。\n\n\n\n## Intex Terms\n\n> 医学图像分割，过拟合问题， Dropout 算法，信息熵\n\n\n\n## 1. Introduction\n\n> **医学图像分割**是当前**计算机辅助医学诊断（Computer-aided Medical Diagnosis, CAD）系统**的重要组成部分，其准确性直接影响 CAD 系统的性能。近年来，CAD 系统越来越多地参与到实际的医疗诊断任务中。因此，提高医学图像分割模型的准确性和可靠性具有重要的意义和应用价值。\n>\n> 在医学图像分割领域， <font color = green>U-Net，nnU-Net，TransUNet 等深度学习模型</font>已经在各种任务中表现出了比传统方法更好的性能。与**自然图像分割**相比，**<font color = green>医学图像分割的数据标定高度依赖于专家知识，需要像素级的准确标定。因此，在专家指导下，像素级的准确标定数据量通常很小。</font>**小尺度的数据集容易出、造成过拟合问题，特别是当分割模型参数量较大时。\n>\n> 解决过拟合问题的方法有很多， Dropout 算法是其中一种简单而有效的方法。它在训练过程中以一定的概率随机丢弃模型中的神经元，缓解了协同适应问题，从而缓解了深度学习模型的过拟合问题。**<font color = green>Co-adaptation 协同适应是指每个神经元学习到的特征通常必须与上下文（即其他特定神经元）相结合的现象，以在训练过程中提供有用的信息。</font>** 然而，**从小规模医学图像分割数据集中学习到的这种经验依赖是脆弱的，在面对测试集的分布时可能不可信。** 因此，<font color = green>神经元之间过多的依赖关系往往会引发过拟合问题。</font> Dropout 算法中的 drop 操作减少了深度学习模型中神经元之间的依赖关系，防止了一些神经元过度依赖其他神经元，从而在一定程度上避免了过拟合问题。\n>\n> **<font color = green>根据 drop 过程是否完全随机，Dropout 算法的衍生方法可以分为两类。</font>** **<font color = blue>第一类是完全随机的方法</font>**，例如 **<font color = blue>Spatial Dropout </font>** 随机丢弃通道维度中的单元，**<font color = blue>DropBlock </font>** 将 2d blocks 视为单元并随机丢弃它们，**<font color = blue>Stochastic Depth</font>** 随机丢弃残差连接。**<font color = purple>第二类是基于规则的方法 </font>，** 例如  **<font color = purple>Weighted Channel Dropout</font>** 以通道的激活值作为指导规则，**<font color = purple>Focused Dropout</font>** 以 2d blocks 的激活值作为指导规则。然而，这两类现有方法都不是没有局限的。**<font color = blue>第一类，完全随机的方法，缺乏指导规则，因此可能效率低下，丢弃的特征不一定是遭受协同适应问题的特征</font>**，**<font color = purple>在第二类，基于规则的方法中，现有的指导规则不够准确，丢弃遭受协同适应问题的精确特征的效率仍有提升空间</font>** 。因此，**<font color = green>本手稿提出了一种结合信息熵和方差的新的指导规则</font>**。在此规则的指导下，进一步提高了所提出的算法丢弃遭受协同适应问题的特征的效率。在多个医学图像分割数据集和模型上的实验表明，该算法显著提高了模型精度。\n\n\n\n## 2. Methodology\n\n> 作者提出了一种新颖的启发式 Dropout 算法，**<font color = green>使用信息熵和方差作为指导规则来执行 Dropout 操作</font>**。该算法能够有效地丢弃遭受协同适应影响的特征，从而在很大程度上缓解了医学图像分割任务中的过拟合问题。\n\n### 2.1. Heuristic Metric\n\n> 为了有效地丢弃协同适应问题较严重的特征，作者采用信息熵作为启发式规则。**<font color = green>信息熵可以衡量一个分布的不确定性。</font>**\n>\n> <img src=h1.png width=35% />\n>\n> 其中，$X$ 是一个随机变量， $p(x)$ 是概率密度函数，$H(x)$ 是关于随机变量 $X$ 的信息熵。\n>\n> <img src=dropout_f7.png width=70% />\n>\n> 如 Dropout 一文中的图7所示，**<font color = green>遭受严重协同适应的特征具有不确定的视觉意义，因此具有较高的信息熵值，而遭受轻微协同适应的特征具有确定的视觉意义</font>**，例如，看起来像目标的点、边缘或几何轮廓，这些特征的信息熵值较低。**<font color = purple>以信息熵为指导原则，我们以更高的概率丢弃遭受严重协同适应问题的特征。</font>**此外，我们还需要**<font color = purple>方差作为另一个启发式规则</font>**。考虑 **<font color = blue>一个极端的情况，当分布接近于常量分布时，已知信息熵将接近最小值。</font>**然而，**<font color = purple>具有常量分布 constant distribution 的特征对训练几乎不提供什么有用的信息。因此，将方差作为另一个启发式规则，我们以更大的概率丢弃更接近常量分布的特征</font>**。\n\n### 2.2. Heuristic Dropout Algorithm\n\n> 结合信息熵和方差两种启发式规则，得到算法1。\n>\n> <img src=heuristic_a1.png width=70% />\n>\n> 我们计算输入特征图 input feature maps 的每个通道的信息熵 $e_i$ 和方差 $v_i$。我们使用 $e_i + \\frac{k}{v_i + \\epsilon}$ 作为指导规则。因为 feature maps 的值是连续分布的，所以我们首先要对值进行量化，然后根据直方图计算信息熵，如算法2所示。还发现使用 $3 \\times 3$的 Laplace 滤波器代替 all-zero 滤波器作为 drop mask 将为模型性能带来一点提升。\n>\n> <img src=heuristic_a2.png width=70% />\n>\n> 我们的算法可无缝插入到各种模型中。以  U-Net 为例，我们在 U-Net 的编码器和解码器的每个阶段的两个连续卷积层之间插入所提出的算法，即：在前一个卷积层的激活函数之后，正好在下一个卷积层之前。\n\n\n\n## 3. Results\n\n### 3.1. Datasets\n\n> 作者在 Pancreas-CT 数据集和 BAGLS 数据集上进行实验。**考虑到在实际应用环境中，由经验丰富的专家标记的训练样本一般很少，我们专门从这些数据集中随机抽取一个子集进行试验。**对于 Pancreas-CT 数据集，我们随机选择12个扫描，然后将其转换为2545个 $512 \\times 512$ 的 2D 切片，以方便训练模型。在这 12幅 3D CT 扫描中，我们随机选择 8幅 作为训练集，2幅 作为验证集，2幅 作为测试集。对于 BAGLS 数据集，我们随机选择 3000个 切片作为训练集，而验证集和测试集的大小保持与原始设置相同。\n\n### 3.2. Evaluation Metrics\n\n> 为了对实验结果进行定量分析，我们采用了医学图像分割领域中广泛使用的 **<font color = green>DICE 值</font>** 和 **<font color = green>IoU 值</font>**作为评价指标。\n>\n> <img src=heuristic_e1.png width=50% />\n>\n> 其中，$X$ 表示模型输出的掩膜 mask，$Y$ 表示输入图像对应的真值 ground truth.\n\n### 3.3. Experimental Settings\n\n> 我们使用 **Adam optimizer** 来训练所有的模型，学习率为 $1 \\times 10^{-3}$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$， $\\epsilon = 10^{-8}$。batch size 的大小设置为可以在 GeForce RTX 2080 Ti 上以混合精度执行的最大值。我们使用 CrossEntropy 并在 Pancreas-CT 数据集上训练100个 epoch。我们使用结合 CrossEntropy, DiceLoss 和 SSIMLoss 的混合损失函数，并在 BAGLS 数据集上训练 30个epoch。我们在训练数据集上使用标准数据增广。不对模型的输出结果进行后处理。我们独立重复所有对比试验5次并报告平均结果。\n\n### 3.4. Comparison with Dropout Derivative Methods\n\n> 为了验证该算法的有效性，我们在 Pancreas-CT 数据集和 BAGLS 数据集上进行了实验。我们将我们的算法和其它 Dropout 的衍生算法加入到几个模型中。图1 为实验结果的箱线图 box plots，表1为定量和整体对比。\n>\n> <img src=heuristic_f1.png width=60% />\n>\n> <img src=heuristic_t1.png width=80% />\n>\n> 试验结果表明，该算法在 Pancreas-CT 和 BAGLS 两个数据集上的性能都优于其他 Dropout 衍生方法。在 Pancreas-CT 数据集上，我们的算法对 U-Net 和 Attention U-Net 的 DICE 值分别提高了 3.67 和 3.37。在 BAGLS 数据集上，我们的算法对 U-Net 和 UNet3+ 的 IoU 值分别提高了 2.97 和1.12。该算法可以更加有效地提高医学图像分割模型的性能。\n\n### 3.5. Comparison Study on Hyperparameter $k$\n\n> 基于 U-Net 和 Pancreas-CT 数据集，作者研究了超参数 $k$ 的影响。随着 $k$ 的增加，性能呈现先增加后衰减的趋势，当 $k$ 为 3 时，性能最好。此外，从方框的方差（the variance of the box）可以看出，当 $k$ 为 2 时，模型性能比 $k$ 为 3 时更稳定和可预测。\n>\n> <img src=heuristic_f3.png width=60% />\n\n### 3.6. Verify Effectiveness of Alleviating Co-adaptation\n\n> 为了验证我们的算法比传统的 Dropout 更有效地缓解协同适应，我们在 Pancreas-CT 数据集上随机隐蔽了 U-Net 最终输出层之前的一定比例的中间特征图（intermediate feature maps）。**<font color = green>对于协同适应较少的模型，由于特征之间的依赖关系较少，掩蔽特征（masked features ）导致的性能下降应该更小。</font>** 如图 4 所示。我们的算法在隐蔽后的性能下降明显小于传统的 dropout。实验结果表明，**<font color = blue>使用我们的算法可以学习到更多独立特征和更少的依赖关系，因此我们的算法可以比传统的 Dropout 算法更大程度地缓解协同适应。</font>**\n>\n> <img src=heuristic_f4.png width=60% />\n\n### 3.7. Visualization of Segmentation Results\n\n> <img src=heuristic_f2.png width=100% />\n>\n> 图 2 演示了定性分析的可视化。从上到下显示三个切片的分割结果。可视化图表明，我们的算法能更准确地分割模型。\n\n\n\n## 4. Conclusion\n\n> 作者提出了一种新的启发式 Dropout 算法来解决小规模医学图像分割数据集的过拟合问题。该算法以信息熵和方差作为启发式规则，更有效地缓解了协同适应现象，从而更好地缓解了过拟合问题。在多个数据集和模型上的实验表明，该算法具有较好的性能。此外，我们将在未来的工作中研究我们的算法与自然图像的兼容性。\n\n\n\n1. > **<font color = green>启发式</font>**：类似于 **<font color = green> 灵感</font>**一类的东西，可以快速进行判断，不需要逻辑性的思考论证。启发式往往可以让人们 **<font color = green>跳出当前思维的局限</font>**，但因为缺乏科学依据与缜密的逻辑验证，所以有时也会出错。\n\n   \n\n2. **<font color = green>医学图像与自然图像的区别：</font>**\n\n   > 1）医学图像大多数时放射成像，功能性成像，磁共振成像，超声成像这几种方式，而自然图像大多数是自然光成像。自然成像中，光谱比较复杂，有散射的成分，波普宽度比较大，但放射成像例如 DR, CT等，各厂家需要去除人体内的散射，使光谱单一，所以，这导致了一个重要区别，也就是：\n>\n   > **<font color = green>在自然图像中，噪声分布绝大多数情况下可认为是均匀的，可近似为高斯噪声</font>**，因为直射和散射光造成光场分布可认为是均匀的；\n>\n   > 但**<font color = blue>在医学图像中，由于光源单一再加上探测手段，人体厚度的影响往往会导致噪声分布不均匀，往往认为是一种泊松噪声</font>**。\n>\n   > 所以，针对医学图像的算法直接应用于自然图像效果可能不行。\n>\n   > 2）医学图像多是单通道灰度图像，尽管大量医学图像是3D的，但医学图像中没有景深的概念。\n>\n   > 3）同体态的医学图像相似度非常高，**医学图像中的细微结构并不能像自然图像中那样认为是无关紧要的**，在相似度极高的背景组织中的细微变化有可能代表着某种病变。\n\n   \n\n3. **<font color = green>Co-adaptation 协同适应</font>**\n\n   > **过拟合**：在训练集上实现高性能，但没法很好地泛化到看不见的数据（测试集）上。\n   >\n   > **在神经网络中，协同适应意味着一些神经元高度依赖其他神经元**。如果那些独立的神经元接收到“坏”的输入，那么依赖的神经元也会受到影响，最终它会显著改变模型的性能，这就是过度拟合可能发生的情况。\n   >\n   > Hinton 提出 Dropout 来防止过拟合：网络中的每个神经元以0和1之间的概率随机丢弃。--> Hinton 认为 Dropout 能防止过拟合的原因在于：**<font color = green>通过实施 Dropout 模型被迫拥有可以学习良好特征（或所需数据表示）的神经元，而不依赖于其他神经元。因此，生成的模型对于看不见的数据可能更加鲁棒。</font>**\n\n   \n\n   \n\n   \n   \n   # Inproving Neural Networks By Preventing Co-adaptation of Feature Detectors_2012_Hinton\n   \n   > **<font color = green>协同适应 Co-adaptation</font>**：一个特征检测器只在其他几个特征检测器的上下文中有用。\n   >\n   > 为了阻止复杂的协同适应性，Dropout 通过在训练过程中随机丢弃一半的特征检测器，迫使 **<font color = green>每个神经元学习检测一种特征，这种特征通常有助于产生正确的答案，因为它必须在各种内部环境中运作。</font>** \n   \n   \n   \n   \n   \n   # Neuron-Specific Dropout: A Deterministic Regularization Technique to Prevent Neural Networks from Overfitting & Reduce Dependence on Large Training Samples\n   \n   ## Abstract\n   \n   > 为了发展输入与输出之间的复杂关系，深度神经网络对大量参数进行训练和调整。为了使这些网络高精度地工作，需要大量数据。**<font color = green>然而，有时训练所需的数据量并不存在或无法获得。Neuron-specific dropout (NSDropout) 被提出用来解决该问题。</font>**  NSDropout 会同时查看模型中层的训练过程和验证过程。通过比较数据集中每个神经元对每个类别产生的平均值，该网络能够丢弃目标单元。**<font color = purple>该层能够预测模型在测试过程中所观察的特征或噪声，而这些特征或噪声在观察验证样本时是不存在的。</font>** **<font color = blue>与 Dropout 不同的是，“thinned” networks “精简”网络不能 \"unthinned\" “未精简”用于测试。</font>** 与传统方法（包括 dropout 和其他正则化方法）相比，**<font color = green>Neuron-specific dropout 被证明可以用更少的数据达到类似的（如果不是更好的话）测试精度。</font>** 实验表明， Neuron-specific dropout 减少了网络过拟合的机会，并 **<font color = green>减少了图像识别中监督任务对大量训练样本的需要</font>**，同时产生了同类最佳（best-in-class）的结果。\n   \n   \n   \n   ## Keywords: \n   \n   > neural networks, regularization, model combination, deep learning, dropout\n   \n   \n   \n   ## 1. Introduction\n   \n   > 深度神经网络可以理解为输入与输出之间的复杂关系。通过利用数千甚至数百万个隐藏节点（神经元），这些模型可以生成一套足以预测癌症或驾驶汽车的规则。然而，要做到这一点，需要大量数据来训练并验证模型。**<font color = green>当数据量不足时，模型可能会关注训练数据中的缺陷或者采样噪声。</font>** 换句话说，该模型将发现训练数据中存在的细节，而这些细节可能在实际应用中并不存在（该模型将发现训练数据中可能并不存在于其实际应用中的细节）。这些最终会导致过拟合，并且因为没办法做出一个完美的数据集，因此已经发展了其他方法来尝试减少模型过拟合的趋势。最流行的方法之一是，当模型的验证精度和训练精度出现偏差时，停止训练。另一个方法是实施权重惩罚，如 L1 和 L2 以及软权重共享（soft weight sharing）。\n   >\n   > <img src=NSDropout_f1.png width=100% />\n   >\n   > **<font color = green>现在有几种方法来解决过拟合问题，一种是贝叶斯方法的使用</font>**。贝叶斯模型是根据贝叶斯定理构建统计模型。\n   >\n   > <img src=NSDropout_e1.png width=20% />\n   >\n   > 贝叶斯 ML 模型的目标是在给定先验分布 prior distribution $(p(\\theta))$ 和 likely hood $(p(x|\\theta))$ 的情况下估计后验分布 posterior distribution $(p(\\theta|x))$。这些模型与经典模型的不同之处在于包含了 $p(\\theta)$ 或先验分布。**<font color = green>一种流行的先验分布是高斯过程。</font>** 通过取所有参数设置的平均值，并将其值与给定训练数据的后验概率进行加权。有了先验高斯分布，我们可以假设后验分布是正态分布或落在正态钟形曲线上。**假设我们有无限的计算能力，防止过拟合最好的方法是计算一个完美的后验分布。** 然而，**<font color = green>逼近后验分布</font>** 已经被证明可以在小模型上提供很好的结果。\n   >\n   > 对于具有少量隐藏节点的模型，与单个模型相比，对使用不同架构和数据训练的不同模型的值进行平均可以提高性能。然而，对于较大的模型，此过程将过于耗费资源，无法证明回报是合理的。训练多个模型是困难的，因为找到最佳参数可能会耗费大量时间，而且训练多个大网络会占用大量资源。此外，在不同的数据子集上获取足够多的数据来训练多个网络是不可能的。最后，假设你能够使用不同数据子集来训练不同架构的多个网络，在需要快速处理的应用程序中，使用所有这些模型进行测试将花费太多的时间。\n   >\n   > 这就引出了防止过拟合的第二种选择。**<font color = green>Dropout 是一种简单而有效的方法来限制噪声对模型的影响</font>**。它通过“dropping 丢弃”隐藏或可见单元来防止模型过拟合，**<font color = green>本质上是同时训练多个模型</font>**。通过丢弃一个单元，该单元在该步骤中不再对模型及其决策产生影响。丢弃的神经元数量由概率 $p$ 决定，即独立于其它单元。\n   >\n   > <img src=NSDropout_f2.png width=100% />\n   >\n   > 图2：左：在训练阶段，假设索引 $i$ 处的值 $r^{(l)}$ 为1时，unit 出现。假设函数 $a_i^{(l)}$ 的输出在向量函数 $a^{(l)}$ 的输出值中不是最低的 $p$ 个百分比，则 $r_i^{(l)}$ 的值为1。右：在测试阶段，只有当 $r_i^{(l)}$ 的最终值为 1 时，unit 才会出现。\n   >\n   > 现在我们有了另一种防止过拟合的方法。 **<font color = green>Neuron-Specific dropout 采用了从一个层中丢弃隐藏或可见单元的思想，而不是随机的丢弃它们</font>** 。与其它流行的层不同， **<font color = green>Neurom-Specific Dropout 接受四种输入</font>** ：layer input 层输入，the true value of the sample 样本真值，validation layer input 验证层输入，the true value of the validation sample 验证样本真值。通过了解哪些神经元的值与该类样本的验证平均值最远，我们可以找到噪声或训练数据中的伪影在哪些地方影响了我们的模型决策。丢弃的神经元数量取决于比例 $p$。然而，这与 Dropout 不同，因为概率 $p$ 表示一层中有多少 百分比的 units 将被丢弃。例如，如果在具有 20 个 units 的层中将 $p$ 设置为 0.2，那么总的会有 4个 units 被丢弃。\n   >\n   > 通常，神经网络中使用的验证数据不应该在调整超参数之外影响模型的行为，但是 neuron-specific dropout 可以提高准确性，这样就可以分割传统的训练数据集，从而永远不会使用保留的验证数据。对训练数据进行分割，以便为新的验证集保留 20% 似乎时是最佳的。\n   >\n   > 类似于 Dropout，应用 neuron-specific dropout 会产生一个 \"thinned\" 的神经网络。这个 thinned 神经网络保存了从神经元丢弃中幸存下来的神经元的所有值。虽然可以解释为具有 $n$ 个 units 的神经网络代表 $2^n$ 个可能的 thinned 神经网络，但众所周知，随着训练的进行，从一个步骤到下一个步骤丢弃的不同的 units 的数量会减少。同样，可训练参数的总数仍然是 $O(n^2)$ ，或者更少。\n   >\n   > 与 dropout 不同的是，如果使用单个的，按比例缩小的神经网络，使用该层的好处不会显示出来。当最后一次使用 mask 时，发现测试结果最好。这是有意义的，因为与 dropout 不同， units 不是随机丢弃的。当模型开始找到受噪声和特征影响的 units 时，它会将它们归零，而把它们带回来则会带回它已经学会的在没有噪声和特征的情况下改进的权重。\n   >\n   > 本文结构如下。第 2 节描述了 neuron-specific dropout 的动机。第 3 节描述了之前的相关工作。第 4 节正式描述了 neuron-specific dropout  model 和它如何工作。第 5 节一个训练 neuron-specific dropout 网络的算法，并引入了不可见验证的思想。第 6 节给出了应用 NSDropout 的实验结果，并与其他形式的正则化和模型组合进行了比较。第 7 节讨论了 NSDropout 的显著特征，并分析了 neuron-specific 的影响，以及不同的参数如何改变网络的性能。\n   \n   \n   \n   ## 2. Motivation\n   \n   >  neuron-specific dropout 的动机来自于 dropout。与 neuron-specific dropout 类似， dropout 切断了与神经元的连接。这项研究最初是出于限制数据量的想法，但当发现 neuron-specific dropout 也可以帮助减少过拟合时，这项研究很快改变了主意。在日常生活中，人们学到的信息比需要的更多，无论是从对话，新闻还是课程。当大脑认为学习到的信息以后不会再被使用时，就会失去一部分。这有助于防止大脑变得混乱。\n   >\n   > 对于这种现象产生的一个可能的解释是大脑中一种称为干扰的现象。当一个记忆干扰其他记忆时，就会发生干扰。记忆可以定义为大脑中获取，存储，保留和稍后检索信息的过程。干扰可以是主动的或追溯的（事后的）。主动干扰是指大脑由于记忆较旧而无法记住信息。追溯性干扰是指大脑在收到新信息时保留先前学习信息的能力。**<font color = green> Neuron-specific dropout 使用类似于追溯性干扰的方法</font>** 。虽然模型本身无法知道哪些信息是有用的（类似于人脑），但验证数据可以让它们了解它们在测试时会看到什么。通过了解验证阶段存在哪些噪声，模型可以关闭或忘记哪些信息对于测试是不必要的。当每个隐藏单元被呈现出新的信息时，即前一层的输出时，它会接收并“学习”这些信息，然后，在激活之前，它会决定哪些信息“干扰”来自验证数据的信息。\n   \n   \n   \n   \n   \n   \n   \n   ## 8. Conclusion\n   \n   > Neuron-specific dropout (NSDropout) 是一种旨在提高神经网络准确性的 **<font color = green>确定性正则化技术，重点关注具有少量训练数据的网络</font>**。通过传统的学习技术，**<font color = green> 网络在一组数据的输入与输出之间建立了复杂的关系，然而这些复杂的关系往往不能泛化到看不见的未知数据</font>**。***<font color = purple>与 Dropout 不同的是， Dropout 可以随机破坏这些复杂的关系， Neuron-specific dropout 可以帮助网络理解这些复杂的关系中哪些导致了网络的过拟合，并关闭隐藏单元，强迫网络在没有这些导致网络过拟合的复杂关系的情况下学习。</font>*** 实验证明，使用 NSDropout 可以提高神经网络在图像分类领域的性能。NSDropout 能够在 MNIST 手写数字，Fashion-MNIST 和 CIFAR-10 中取得最好的（best-in-class）结果。\n   >\n   > 此外，为了提高图分类网络的性能，NSDropout 还减少了对大数据集的需求。当对 MNIST 手写数字进行训练时， NSDropout 网络仅使用 750 个训练样本就能达到完美的测试精度（a perfect test accuracy）。在 Fashion-MNIST 中， NSDropout 仅使用 60000 个训练样本中的 10000 个 就能达到近乎完美的准确率（a near-perfect accuracy）. **<font color = green>NSDropout 的一个关键特征是能够在训练期间将测试精度和训练精度联系起来。</font>** 这有助于限制网络过拟合的机会。\n   >\n   > **<font color = blue>NSDropout 的一个局限是训练模型所需时间的增加。</font>** 一个图像分类 NSDropout 模型的训练时间是相同架构的标准神经网络的 4 倍，并且没有进行优化。它需要比传统的 dropout 模型多 两倍 的时间。**<font color = blue>时间增加的一个主要原因是 NSDropout 层中按类排序和无序的多个输入。</font>** 虽然排序算法变得更快，并且可以对 NSDropout 进行更多的优化，但它们仍然占用了处理过程中的大部分时间。**<font color = green>目前 NSDropout 只是 丢弃（drops）它认为网络过于依赖的单元，但未来的工作可能会着眼于如何调整单元而不是丢弃它</font>**，从而在更广泛的应用程序中提高性能。\n   \n\n\n\n# Structural Dropout for Model Width Compression_2022\n\n## Abstract\n\n> 众所周知，现有的 ML 模型是高度过度参数化的（highly over-parameterized），并且使用了比给定任务所需更多的资源。以前的工作已经探索了离线压缩模型（compressing models offline），例如，从较大的模型中提取知识到较小的模型中。这对于压缩是有效的，但没有给出衡量模型可以压缩多少的经验方法，并且需要对每个压缩模型进行额外的训练。**<font color = green>我们提出一种只需要对原始模型和一组压缩模型进行一次训练的方法。</font>** 所提出的方法是一种 **<font color = green>structural dropout</font>**，它会在随机选择的索引之上剪枝掉所有处于隐藏状态的元素，从而迫使模型学习其特征的重要性排序。在学习了这种排序之后，在推理阶段可以剪枝掉不重要的特征，同时保持最大的准确性，显著减小参数大小。在这项工作中，我们聚焦于全连接层的 Structural Dropout，但这个概念可以应用于任何类型的具有无序特征的层，如卷积层或 attention layers。Structural Dropout 不需要额外的剪枝 / 重新训练，但需要对每个可能的隐藏大小（each possible hideen sizes）进行额外的验证。在推理阶段，非专业人员可以在广泛的高压缩和更精确的模型之间选择最适合他们需求的内存与精度的权衡。\n\n\n\n## 1. Introduction\n\n> 总结起来，这项工作的贡献如下：\n>\n> 1. Dropout  的一种变体，Structural Dropout，它训练一个嵌套网络的集合，以后可以在不进行额外的重新训练（retraining）的情况下将这些网络分离出来进行压缩。\n> 2. 在 3 个示例任务上验证 Structural Dropout，证明其在保持准确性的同时，各种方法的有效性。\n> 3. Structural Dropout 的实现：[An Implementation of Structural Dropout](https://github.com/JulianKnodt/structural_dropout  \"Strucutural Dropout \")\n>\n> <img src=StructuralDropout_f1.png width=100% />\n>\n> 在训练过程中，Structural Dropout 并不是随机选择要剪枝的索引，而是在统一随机选择索引（a uniformly randomly selected index）后剪枝所有节点，并根据丢弃的特征数量对期望进行归一化。在一定的可能性下，我们运行整个网络，用它间接地监督较小的网络（间接将其用作较小网络的监督）\n\n\n\n\n\n\n\n## 5. Discussion\n\n> Structural Dropout 作为现有架构的最小补充，可以执行超参数搜索和压缩。由于它不需要昂贵的重新训练和额外的领域知识，因此它比特定领域的修改更容易采用，并且与剪枝和量化正交。\n>\n> 在我们的实验中，很明显存在信息饱和的陡峭悬崖，并且可以以最小的精度变化来修剪 50%-80%之间的重要特征。如在 PointNet 上所见，当使用更高的 dropout rate 进行更积极的剪枝时，可以在不损失精度的情况下剪枝高达 80%。\n>\n> Structural Dropout 也可能有助于提高性能，因为纯粹通过对多个模型进行采样，其中一个模型可能在给定任务上表现更好。\n\n\n\n## 6. Limitation\n\n> 虽然我们的方法可以直接添加到现有的体系架构中，但它也有一些缺点。\n>\n> **<font color = green>一个显著的缺点是 SD 增加了搜索空间，使问题变得更加困难。</font>** 由于问题更加困难，尽管使用更少的参数可以加快速度，但训练过程可能需要更长的时间。这个训练时间并不比原来长很多，但是不清楚到底长多少。**<font color = green>除了增加的训练时间之外，比较所有通道宽度的验证损失是缓慢的，因为有大量的模型需要测试。</font>** 如果资源可用，这可以很容易地并行化，因为与训练不同，模型将是只读的，否则可以执行稀疏搜索。\n>\n> **<font color = green>SD 的另一个缺点是，它在训练过程中更难以验证和跟踪收敛。</font>** 由于 low channel width，模型在训练过程中可能随机出现精度较低的情况，因此很难确定模型的收敛性。所以，对于之前训练过的模型使用 SD 是有意义的，并且先验收敛参数已经设置。在这些模型上，它也可以用作对通道宽度执行超参数搜索的一种方式。\n>\n> 此外，**<font color = green>虽然我们假设所有的 SD 尺寸（SD sizes）在推断时应该是相同的，但在一个具有各种层的较大模型中，情况可能并非如此</font>**。对所有可能的通道大小选择执行详尽的搜索将是昂贵的，因此有效的搜索策略很重要。一个常见的假设是每个选择都可以独立做出，但我们将这一探索留给未来的工作。\n>\n> 最后，**<font color = green>我们的方法不能剪枝整个层，因为 SD 仅限于改变神经网络的宽度，而不能修改它的深度。</font>** 因此，性能瓶颈是深度的网络将无法获得同样多的好处。\n\n\n\n## 7. Future Work\n\n> 虽然我们展示了 Structural Dropout 在 FC 层上的应用，但同样的原理可以扩展到其他类型的层。例如，同样的思路也适用于 CNN 的 channel dimension，允许对特征进行修剪。另一种可能的扩展是针对 transformers，在 transformers 中选择多个注意力头 （ a number of attention heads）很重要。 Structural dropout 可应用于注意力头的大小和注意力头的数量。我们希望将这项工作扩展到探索在各种架构中使用 Structural Dropout 以实现实用的和高效的压缩。\n\n\n\n## 8. Conclusion\n\n>  Structural Dropout 是一种用于推测时间压缩（inference-time compression）的方法，可用作现有架构的 drop-in layer，以最小的精度损失实现大量的压缩。这是作者所知道的第一种方法，它允许在单个训练会话（a single training session）中训练许多任意压缩的模型，然后联合部署它们，代价是只部署最大的模型。从我们的实验中，我们希望 Structural Dropout 是一种用于压缩神经网络的强大工具。\n\n\n\n\n\n\n\n\n\n# Dropout Regularization for Automatic Segmented Dental Images_ACIIDS_2021\n\n## Abstract\n\n> 深度神经网络是指具有大量参数的网络，是深度学习系统的核心。从这些系统中产生了一个挑战，即它们如何针对训练数据 和 / 或 验证数据集执行。由于所涉及的参数数量众多，网络往往会消耗大量时间，这会导致称为过拟合的情况。**<font color = green>这种方法建议在模型的输入和第一个隐藏层之间引入一个 dropout 层</font>** 。这是非常特别的，与其他领域使用的传统 Dropout 不同，传统的 Dropout 在网络模型的每个隐藏层中引入 dropout 来解决过拟合。我们的方法涉及预处理步骤，该步骤处理数据增广以处理有限数量的牙科图像和侵蚀形态以消除图像中的噪音。此外，使用 canny 边缘检测方法进行分割以提取基于边缘的特征。除此之外，所使用的神经网路采用了 Keras 的顺序模型，这是为了将边缘分割步骤的迭代合并到一个模型中。对模型进行并行评估，首先没有 Dropout，另一个使用大小为 0.3 的 dropout 输入层。在模型中引入 dropout 作为权重正则化技术（weight regularization technique），提高了评估结果的准确性，无论是 precision 准确率还是 recall values 查全率，没有 dropout 的模型为 89.0%，而有 dropout 的模型为 91.3%。\n\n\n\n\n\n## Keywords: \n\n> Deep learning,  Over-fitting,  Regularization technique,  Dropout\n\n\n\n\n\n## 1. Introduction\n\n> 过拟合是各种深度学习系统中普遍存在的问题。当模型在训练集上训练得太好，而在测试集上训练得不太好时，通常会发生过拟合的情况。或者，欠拟合是指我们的模型在训练集和测试集上都表现不佳。\n>\n> 因此，这两种情况可以通过几种称为权重正则化技术（weight regularization technique）的方法来处理。这些方法包括 early stopping, L1,L2 regularization 和 Dropout。在我们的方法中，我们使用 Dropout，包括丢弃神经网络模型中隐藏和可见的单元。这是通过在训练阶段忽略在随机选择的特定神经元集（certain set of neurons）的 units 单元来实现的。从技术上讲，在每个训练阶段，单个 units 要么以 $1-p$ 的概率从网络中丢弃，要么以 p 的概率保留，这样剩下的是一个简化的网路（reduced network）.\n>\n>  Dropout 背后的关键思想是在训练过程中，从神经网络中随机丢弃 units 及其它们的连接，以防止 units 过度自适应（co-adapting）。在训练阶段丢弃不同网络模型的 units 后，这使得测试更容易接近网络平均预测的效果。从 dropout 的过程中，减少了过拟合，并进一步对其他正则化方法进行了重大改进。\n>\n> 在其他研究（例如：[Dropout Regularization in Deep Learning Models with Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/   \"Dropout Regularization \") ）中，展示了 dropout 如何应用于深度学习系统。可以通过多种方式在网络模型中引入 Dropout。它可以作为输入和第一个隐藏层之间的一个层来引入。其次，它可以应用于两个隐藏层之间以及最后一个隐藏层和输出层之间。\n>\n> 我们提出的方法使用了第一个方法，在输入层和第一个隐藏层之间引入 dropout。Dropout 在大型网络中非常有用，它具有各种约束条件，如 learning rate, decay 和 momentum，以调高评估性能。\n\n\n\n\n\n## 5. Conclusion\n\n> Dropout 是一种通过减少过拟合来改进神经网络的技术。相比于模型的隐藏层之间引入一个独立的 dropout layer，在输入可见层中引入该算法得到了很好的结果。深度神经网络模型的训练需要很长时间，从使用我们的方法进行的实验中，我们见证了模型复杂度的降低和训练时间的增加。\n>\n> 我们的 Dropout 方法可以与其他正则化方法一起使用，以获得更好的性能结果。其他可用于获得更好的性能的权重正则化技术包括 early stopping 以解决在模型图上见证的验证损失变化。\n\n\n\n\n\n> 1. 神经元特定的 dropout 是针对训练样本不足或无法获得的问题，提出的方法能够在保证精度的同时，减少训练样本的需求。\n> 2. 用于模型宽度压缩的结构 dropout ，是针对离线压缩模型没有给出衡量模型可以压缩多少的方法且需要对每个压缩模型进行额外的重新训练的问题，作者提出的方法不需要额外的剪枝或者重新训练。\n> 3. 牙医图像自动分割的 dropout 正则化，传统的dropout是在网络模型的每个隐藏层中引入 dropout 来解决过拟合的，而作者是在模型的输入和第一个隐藏层之间引入 dropout层，也就是研究了针对牙医图像自动分割技术， dropout 通过什么样的方式能更好地应用于深度学习系统。\n\n\n\n\n\n\n\n\n\n# Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers_ICML_2022\n\n## Abstract\n\n> Sparsely activated transformers, 如 Mixture of Experts(MoE)，由于其惊人的缩放能力而引起了极大的兴趣，这可以不显著增加计算成本的情况下显著增加模型大小。为了实现这一点， MoE 模型用 Transformer 中的 Mixture-of-Experts  sub-layer 替换前馈子层（feed-forward sub-layer），并使用一个 gating network 门控网络将每个令牌路由到其指定的专家。由于有效训练此类模型的常见做法需要将专家和令牌分布在不同的机器上，因此这种路由策略通常会产生巨大的跨机器通信成本，因为令牌及其分配的专家可能位于不同的机器中。在这篇文章中，作者提出 Gating Dropout，它允许令牌忽略 gating network 并停留在它们的本地机器上，从而减少了跨机器通信。与传统的 dropout 类似，我们也表明， Gating Dropout 在训练中有正则化效果，从而提高了正则化性能。我们验证了 Gating Dropout 在多语言机器翻译任务中的有效性。我们的结果表明， Gating Dropout 改进了最先进的 MoE 模型，具有更快的 wallclock 时间收敛率和更好的 BLEU 分数，适用于各种模型大小和数据集。\n\n\n\n## 6. Conclusion\n\n> 我们提出 Gating Dropout 作为一种通讯高效的正则化技术来训练 sparsely activated transformers。我们观察到 sparsely activated transformers，例如 MoE 模型，通常具有非常高的跨机器通信成本，因为它们需要通过 all-to-all 通信操作将令牌发送给指定专家。Gating Dropout 通过随机跳过 all-to-all 操作来降低通信成本。这种随机跳跃在训练期间也具有正则化效果，从而提高了泛化性能。多语言翻译任务的实验证明了该方法在吞吐量（throughput）、泛化性能和收敛速度方面的有效性。\n>\n> 关于未来的工作，我们正在研究如何通过结合 Gating Dropout 和专家剪枝来提高推理速度。Gating Dropout 目前对推理速度没有影响，因为它只是在推理阶段关闭。此外，我们还对整个训练阶段中不同 dropout rate 的影响感兴趣，因为从探索-利用的角度，探索在训练的早期阶段可能更为重要。\n\n\n\n\n\n# Dropout Regularization in Hierarchical Mixture of Experts_Neurocomputing_2021\n\n专家分层混合中的 Dropout 正则化\n\n## Abstract\n\n> Dropout 是一种非常有效的防止过拟合的方法，近年来已成为多层神经网络的首选正则器。专家的分层混合是一个分层门控模型（hierarchically gated model），它定义了一个软决策树，其中叶子对应于专家，决策节点对应于在其子项之间软选择的门控模型，因此，该模型定义了输入空间的软分层分区。在这项工作中，我们提出了一种用于专家分层混合的 dropout 变体，它忠实于模型定义的树层次结构，而不是像多层感知器那样具有平面的、单元独立的 dropout 应用程序。我们表明，在合成回归数据以及 MNIST 和 CIFAR-10 数据集上，我们提出的 dropout 机制可以防止在树上的过拟合，并在多个层次上提高泛化能力并提供更平滑的拟合。\n\n\n\n## 5. Conclusions\n\n> 我们提出了一种新的 Dropout 机制，可应用于专家分层混合方法及其扩展。与具有条件独立单元的平面架构上的 dropout 相比，我们的方法忠实于模型树层次结构中存在的门控依赖关系（the gating dependencies）。\n>\n> 我们展示了我们的方法在一个合成玩具数据集以及用于数字识别和图像分类任务的两个真实数据集上的有效性。在所有的数据集上，我们看到专家的分层混合在有太多级别和叶子时确实会过拟合，但是我们提出的方法可以作为一种有效的正则化器，其中 dropout rate 作为权衡偏差 bias 和方差 variance 的超参数。\n>\n> 我们还定性地评估 dropout 对模型学习到的表示的影响，这些模型通过提供可视化来可视化 dropout 的影响。由于我们仅不对称地丢弃左子树这一事实，我们的 dropout 方法有效地从具有不同复杂性的树结构模型的集合中采样。这种方法通过充当具有不同复杂性的模型的插值来引入正则化。\n\n\n\n\n\n\n\n# Clustering-Based Adaptive Dropout for CNN-Based Classification_PR_2020\n\n## Abstract\n\n> Dropout 被广泛用于提高深度网络的泛化能力，而目前的 dropout 变体很少动态调整网络隐藏单元或权重的 dropout 概率（dropout probabilities）以适应它们对网络优化的贡献。这篇文章提出了一种基于聚类的 dropout（clustering-based dropout），该算法基于特征、权重或它们的衍生物的网络特征，其中这些特征的 dropout 概率根据相应的聚类组自适应更新以区分它们的贡献。在 Fashion-MNIST 和 CIFAR-10 数据库以及 FER2013 和 CK+ 表情数据库上的实验结果表明，所提出的基于聚类的 dropout 比原始的  dropout 和各种 dropout 变体具有更好的准确性，并且与最先进的算法相比具有最具竞争力的性能。\n\n\n\n## Keywords:\n\n> Feature and weight clustering,  Feature derivative dropout,  Self-adaptive dropout probability,  Facial expression recognition\n\n\n\n## 1. Introduction\n\n> 为了提高深度网络的正则化能力，提出了 regularizer 正则化器、batch normalization 批归一化 和 sparse deep feature learning 稀疏深度特征学习【Sparse deep feature learning for facial expression recognition _PR_2019 】，来减少过拟合的可能性。Dropout 随机丢弃网络隐藏单元或权重，也被应用于很多目标识别问题。受隐藏单元 dropout 的启发， connection (weight) dropout 被提出来随机丢弃权重元素。Khan【Regularization of deep neural networks with spectral dropout _NN_2019 】 等人提出了对 feature map 的光谱变换进行 dropout，其中引入了与 feature map 的重塑维度（the reshaped dimension of the feature map）相对应的三种不同的变体。\n>\n> 然而，传统 dropout 中的隐藏单元或权重是逐个元素地抑制的，这可能会忽略元素块 element block 中隐含的结构信息。Tompson 【Efficient object localization using convolutional networks_CVPR_2014】等人提出 spatial dropout 来丢弃一整个 feature map，即同时丢弃或保留（dropped or retained）一个 feature map 的所有隐藏单元。Poernomo 和 Kang 【Biased dropout and crossmap dropout: learning towards effective dropout regularization in convolutional neural network_NN_2018】根据隐藏单元响应【Learning both weights and connections for efficient neural network _NIPS_2015】的大小将特征分为大小相等的两组，并为每一组分配一个 dropout 概率。同时，提出一个额外的 cross-map dropout，其中不同 feature maps 上相同坐标的元素被同时丢弃或保留。然而，两组不足以区分不同特征之间的贡献，应该设计更多的组。Rohit 等人【Guided dropout _AAAI_2019】根据节点的强度（the strength of each node），提出通过删除节点来引导 dropout。Zhang 等人【ML-LocNet: improving object localization with multi-view learning network _ECCV_2018】提出 region dropout，利用显著区域（salient regions）的组合进行训练。但是，区域的相对位置和大小是固定的，不够灵活。Zhang 等人【Image ordinal classification and understanding: grid dropout with masking label _ICME_2018】提出 grid dropout 来减少搜索空间，以方便对全局特征的探索。然而，相同 grid 中的元素可能彼此之间存在显著差异，因此分配给整个网格 grid 相同的 dropout 概率可能不适用于相同 grid 中显著不同的元素。\n>\n> 对于 dropout 的特征（hidden unit, feature or weight）分组，最先进的 dropout 变体没有以足够的灵活性和多样性来划分这些特征。实际上，对于网络反向传播，即使特征图和权重矩阵中的相邻元素对网络损失的贡献也大有不同。例如，图 1 展示了使用 ResNet18 的表情图像的特征图的活动区域，其中根据 heat maps response 热图影响将不同的 feature maps 分为三个不同的重要性等级，即：insignificant, fair and significant。直观的说，特征元素响应的大小应该与 dropout 概率负相关。然而，传统的 dropout 和最先进的变体无法收集这些 insignificant 无关紧要的 feature maps 或分布在整个 map 上的元素用于 dropout。在这项工作中，在 dropout 中引入了 network element clustering 网路元素聚类，将相似的元素分组，以使它们共享相同的 dropout 概率。因此，利用所提出的聚类方法，可以通过分配一个具有较大 dropout 概率的相应组来同时抑制不重要的元素。\n>\n> 对于 dropout 概率设置，在整个网络训练过程中保持固定的 dropout 概率可能会忽略不同部分对网络优化的动态影响。 Wager 等人【Dropout training as adaptive regularization _NIPS_2013】将 dropout 训练视为一种具有二阶导数近似的自适应正则化形式。 Ba and Frey 等人【Adaptive dropout for training deep neural networks _NIPS_2013】根据矩阵元素性能 matrix elements performance，提出了一种通过更新 a probability mask matrix 概率掩码矩阵的自适应 dropout 方法。在这项工作中， dropout 概率是根据平均特征响应的聚类组（the clustering group of average characteristic response）动态更新的。\n>\n> <img src=Clustering-basedAdaptiveDropout_f1.png width=80% />\n>\n> 图1 残差网络（ResNet18）的最后一个卷积层中示例表达式的 512 个 feature maps 中的 6 个。根据感兴趣区域对 RaFD 数据库的影响，feature maps 可以被分为不同的重要性等级（importance levels），即： insignificant, fair and significant。\n>\n> 为了考虑 dropout 的特征，通常使用深度网络中的全连接层特征（FC layer features，即 layer input）和权重矩阵（weight matrix）作为判别特征来确定识别性能（as the discriminative features to determine the recognition performance）。因此， FC features, the weights 及其 their derivatives 被用作聚类的特征。\n>\n> 这项工作的主要贡献总结如下：\n>\n> * 提出了一种基于 FC features, weights or their derivatives 聚类的新型 dropout 算法；\n> * 根据每组 feature, weight or derivative clustering 的响应幅度，提出了 dropout 概率的自适应更新方法；\n> * 在 Fashion-MNIST 和 CIFAR10 数据库以及 FER2013 和 CK+ 表情数据库上取得了有竞争力的性能。\n>\n> 本文分为以下几个部分。第 2 节介绍了提出的 clustering-based dropout。第 3 节给出了实验结果和相应的插图。最后，在第 4 节提出结论和讨论。\n\n\n\n\n\n## 4. Conclusion\n\n> 考虑到全连接特征、权重、特征和权重的衍生物中的元素对网络优化的贡献不同，提出了一种具有自适应 dropout 概率的基于聚类的 dropout 算法。本文提出的 dropout 进一步嵌入到 ResNet18 的 FC 层，用于四个公共数据库，即 Fashion-MNIST, CIFAR-10, FER2013 和  CK+，实验结果验证了所提出的 dropout 相比于其他 dropout 变体和相关的最新算法的竞争力。\n>\n> 虽然本文提出的基于聚类的 dropout 方法获得了具有竞争力的结果，但仍有进一步改进的空间。首先，引入超参数对网络学习的影响，如簇的数量（the number of clusters），需要进一步研究。其次，深入研究不同模型选择下基于聚类的 dropout 的理论分析。最后，提出的 dropout 应该应用于更多的模型和任务。\n\n\n\n\n\n\n\n# Correlation-based structural dropout for convolutional neural networks_PR_2021\n\n## Abstract\n\n> 卷积神经网络很容易遭受过拟合问题的影响，因为它们在小型训练数据集的情况下经常被过度参数化（over-parameterized）。**<font color = green>传统的 dropout </font>** 随机丢弃 feature units 对于全连接网络效果很好，但 **<font color = green>由于中间特征的高空间相关性（high spatial correlation of the intermediate features）</font>** 而不能很好地正则化 CNNs，这**<font color = green>使得丢弃的信息流过网络，从而导致 under-dropping </font>**问题。为了更好地正则化 CNNs，已经提出了一些 structural dropout methods，例如 **<font color = blue>SpatialDropout 和 DropBlock</font>**，它们通过在连续区域中随机丢弃 feature units 来实现。然而，这些方法 **<font color = blue>可能会因为丢弃关键的判别特征（ critical discriminative features ）而遭受 over-dropping 问题</font>** ，从而限制了 CNNs 的性能。为了解决这些问题，我们提出了一种新颖的 structural dropout method，Correlation based Dropout（CorrDrop），通过 **<font color = purple>基于 feature correlation 丢弃 feature units</font>** 来正则化 CNNs。与之前的 dropout 方法不同，我们的 CorrDrop 可以 **<font color = purple>聚焦于判别信息（discriminative information），并以 spatial-wise 或 channel-wise 的方式丢弃 features</font>** 。在不同的数据集，网络架构和各种任务（如，图像分类和目标定位）上的广泛实验证明了我们的方法优于其他方法。\n\n\n\n## 1. Introduction\n\n> 卷积神经网络已经广泛应用于机器学习社区和计算机视觉任务中，包括图像识别和目标检测。近年来， ResNet, InceptionNet 和 DenseNet 等很多先进的 CNNs 被设计来提高传统 CNNs 的性能。提出了更深和更宽的深度学习模型，以在各种计算机视觉任务中实现最先进的性能。然而，这些模型有数百万个参数，因此很容易遭受过拟合的问题，尤其在训练数据有限的情况下。因此，开发正则化方法来缓解 CNNs 的过拟合是必不可少的。\n>\n> 早期提出的正则化方法有很多，如 weight decay, early stopping, data augmentation, batch normalization 和 dropout。这些方法已被采用作为常规的工具来正则化深度神经网络。其中，传统的 dropout 在全连接（FCs）网络中运行良好。然而，这种 **<font color = green>dropout 并不能通过在 feature map 中随机丢弃单个 feature unit 来有效地正则化 CNNs，因为空间相关的 features 仍然允许丢弃的信息在网络中流动，从而导致 under-dropping 问题。</font>**\n>\n> 为了使 dropout 对 CNNs 更有效，最近提出了一些 structural dropout methods，包括 **<font color = blue> SpatialDropout，Cutout 和 DropBlock</font>** ，以提高 CNNs 的正则化能力。这些方式 **<font color = blue>试图通过在 input/feature space 中随机丢弃整个 channels 或 square of regions </font>** 来正则化 CNNs。然而，由于 the feature units 以随机方式在连续区域中被丢弃，而 **<font color = blue>不考虑图像中的语义信息</font>** ，因此它们存在 over-dropping 问题。 **<font color = blue>这种丢弃 feature unit 的随机方式可能会丢弃 the input/feature maps 中的整个判别区域（the whole of discriminative regions）</font>** ，并限制模型的学习能力。如图 1 所示， **<font color = green>传统的 dropout 丢弃 feature maps 中的 single unit</font>** ，而 **<font color = blue>structural DropBlock 直接丢弃 feature maps 中的 a square of feature units</font>** ，并且可能会将信息语义区域归零。\n>\n> <img src=CorrDrop_f1.png width=80% />\n>\n> 图 1. Dropout, DropBlock 和我们的 Spatial-wise CorrDrop masks（前三行）的示例。红色的部分表示要屏蔽的区域（the regions to be masked）。**<font color = orange>最后一行表示 CorrDrop 对应的相关热图（the corresponding correlation heatmap）</font>** 。**<font color = purple>聚焦于主要目标的 feature units 之间的相关性更强。</font>** 与 Dropout 和 DropBlock 相比，  **<font color = purple>CorrDrop 考虑了判别性信息（discriminative information），自适应地丢弃 feature units </font>** 以缓解 under-dropping  和 over-dropping 问题。\n\n> **<font color = orange>受观察到的目标的判别区域（discriminative region of an object）将具有更高的特征相关性（feature correlations）的启发</font>**（参见图 1 最后一行），我们提出了 Correlation-based Dropout（CorrDrop），这是一种新颖且有效的 CNNs 结构 dropout 方法，该方法考虑到 spatial / channel dimensions 上的 feature correlation，从而丢弃 feature units。\n>\n> 不同于之前的随机丢弃 feature units 的 structural dropout methods（如 DropBlock），我们的  **<font color = purple>CorrDrop 基于判别信息（discriminative information）自适应地丢弃 feature units</font>** 。具体来说，我们 **<font color = purple>首先计算 feature correlation map 以指示最具辨别力的区域（the most discriminative regions），然后自适应地屏蔽那些辨别力较差的区域（those less discriminative regions），即特征相关值较小的区域（regions with small feature correlation values）</font>** 。由于 feature correlation 根据相关性计算的方法可以进一步分为 spatial-wise feature correlation 和 channel-wise feature correlation，我们提出了 CorrDrop 的两种变体： Spatial-wise CorrDrop 和 Channel-wise CorrDrop，它们分别在 spatial dimension 和 channel dimension 自适应地丢弃 features。如图 1 所示，与传统的 dropout 和 DropBlock 遭遇 under-/over-dropping  问题相比，我们的 **<font color = purple>CorrDrop 通过丢弃相关性较低的区域（part of less correlated regions）来生成自适应掩膜（adaptive  masks）</font>**。图像分类的大量实验表明，在公共数据集上不同的 CNNs 架构下，，我们的 CorrDrop 始终优于 dropout he DropBlock。此外，我们也证明了我们的 CorrDrop 在其他计算机视觉任务（如如目标定位）中也能很好地正则化 CNN 模型。\n>\n> 这项工作的初步版本已经作为会议版本【Corrdrop: Correlation based dropout for convolutional neural networks _ICASSP_2020】呈现出来。在这个扩展版本中，我们包含了额外的内容，包括 the channel-wise CorrDrop，更多的消融实验，最先进的 CNNs 实验和额外的视觉任务。主要贡献可以总结如下：\n>\n> * 我们提出了 Correlation based structural dropout（CorrDrop），它丢弃了 feature maps 中不太相关的特征（the less correlated features），这缓解了以前的 dropout 变体以随机方式丢弃 features 的 under-/over-dropping 问题。\n> * 针对 feature map 中 spatial-wise 和 channel-wise features，提出了相应的 Spatial-wise CorrDrop(SCD) 和 Channel-wise CorrDrop(CCD)。实验结果表明，**<font color = purple>它们的互补性在于 SCD 在简单数据集（如 CIFAR-10 和 SVHN）上表现良好，而 CCD 在复杂数据集（CIFAR-100 和 TinyImageNet）上表现出色。</font>**\n> * 在各种数据集，架构和视觉任务上的大量实验表明，我们的方法可以得到持续的改进。\n>\n> 这篇文章剩余部分组织如下。第 2 节简要回顾了深度学习中正则化方法和 注意力机制的相关研究成果。在第 3 节中，我们详细介绍了 CorrDrop 。第 4 节给出了实验结果。最后，我们在第 5 节得出结论。\n\n\n\n\n\n## 3. Methodology\n\n> 由于 under-/over-dropping 问题，大多数现有的基于 dropout 的方法在正则化 CNNs 方面受到限制。通过利用特征的相关性，我们提出一种有效的 structural dropout： correlation-based dropout（CorrDrop），它根据判别信息（discriminative information）自适应地丢弃 feature units，并且可以有效地正则化 CNNs。考虑到 CNNs 的 spatial-wise feature correlation 和 channel-wise feature correlation，我们进一步推导出 CorrDrop 的两种变体，即： Spatial-wise CorrDrop 和 Channel-wise CorrDrop。这两种变体的流程如图 2 和 图 3 所示。在下面的部分中，我们首先描述基于特征正交性（feature orthogonality）的特征相关性（feature correlation）的计算。然后，我们根据 correlation map  来采样 mask。最后，我们说明了 Spatial-wise CorrDrop 和 Channel-wise CorrDrop 的策略。\n\n### 3.1. Feature correlation calculation\n\n> 与以往随机丢弃 feature units  的方法不同，我们试图根据 feature correlation 来自适应地丢弃 feature units，这反映了判别信息（discriminative information）。**<font color = purple>最近的研究【Learning deep features for discriminative localization _CVPR_2016】【Grad–cam: Visual explanations from deep networks via gradient-based localization _ICCV_2017】表明，目标的判别区域（discriminative regions）将有更高的特征相关性（feature correlations）。</font>** 这些观察让我们做出基本假设，即通过丢弃那些 low-correlated features 可以更有效地正则化 CNNs。  **<font color = purple>为了表示 feature correlation，我们使用特征正交性（feature orthogonality）的度量，如之前的工作【Improved training of convolutional filters _CVPR_2019】。</font>** 给定 feature matrix $A = [a_1, ..., a_N]^T \\in \\mathcal{R}^{N \\times K}$，其中，$N$ 是 feature units  的数量，$K$ 是 feature dimension。correlation 的计算可以描述如下：\n>\n> <img src=CorrDrop_e1.png width=50% />\n>\n> 其中 $|.|$ 表示绝对值运算，$I$ 是一个大小为 $N \\times N$ 的单位矩阵。我们首先对  $A$  的每一行进行归一化（normalize），根据特征正交性（feature orthogonality）计算 correlation scores。$P$ 是一个大小为 $N \\times N$ 的矩阵，$P_i$ 表示 $P$ 的第 $i$-$th$ 行。 a single unit 的 $P$ 行的非对角元素表示所有其他 feature units  的投影（Off-diagonal elements of a row of $P$ for a single unit denote projection of all the other feature units）。每行的平均值表示每个 unit 的 correlation score。$F_i$ 的值越高该 unit 与其他 unit 高度相关。\n\n### 3.2. Correlation based dropout mask sampling\n\n> 为了根据 feature correlation 自适应地丢弃 units，我们根据 $F$ 中的值为每个 unit 分配 丢弃概率（dropout probability）。一般情况下，$F_i$ 的值越大，我们的丢弃概率越小。$A$ 中第 $i$-$th$ 个 feature unit 的丢弃概率可以表示为：\n>\n> <img src=CorrDrop_e4.png width=50% />\n>\n> 其中 $i$ 和 $j$ 表示 $F$ 中 feature unit 的索引。为了确保丢弃概率 $\\gamma_i \\in (0,1)$，我们将每个 unit 的 correlation score 进行归一化。\n>\n> 基于丢弃概率 $\\gamma_i$ ，从伯努利分布中采样 dropout mask $M \\in \\mathcal{R}^{N}$\n>\n> <img src=CorrDrop_e5.png width=50% />\n>\n> 经验上，类似于其他 dropout 变体，一个超参数 $p$ 被引入以确保我们的 CorrDrop 不会丢弃太多 feature units。利用基于 correlation 的 dropout mask $M$，我们调整 keep probability 并生成另一个 mask $B \\in \\mathcal{R}^{N}$。当两个 masks 对应的值都为 0 时， the units 被丢弃，并得到 the final dropout mask $S \\in \\mathcal{R}^{N}$。CorrDrop 的 final dropout mask 可制定为：\n>\n> <img src=CorrDrop_e6.png width=50% />\n>\n> 其中 $numel(M)$ 计算 $M$ 的 units 数量，$sum(M)$ 计算值为 1 的 units 数量。\n\n> <img src=CorrDrop_f2.png width=100% />\n>\n> 图 2. Spatial-wise CorrDrop 的过程。1）通过 spatial-wise  local average pooling 对前一层的 feature maps 进行下采样，kernel size 和步长为 $k$，用于局部特征收集和降维（local features gathering and dimension reduction）。2）基于特征正交性（feature orthogonality） 计算 correlation map，并从具有自适应丢弃概率的伯努利分布中采样 dropout mask。3）通过最近邻上采样生成 CorrDrop mask。4）通过对 CorrDrop mask 和 original feature map 进行逐元素相乘得到 regularized feature 正则化特征。\n>\n> <img src=CorrDrop_f3.png width=100% />\n>\n> 图 3. Channel-wise CorrDrop  的过程。1）基于 correlation calculation 计算 correlation vector。2）根据 correlation vector 对 CorrDrop mask 进行采样，即相关越少的通道（the less correlated channels）越容易被丢弃。3）将 CorrDrop mask 与 original feature map 进行逐通道相乘。\n\n### 3.3 Spatial-wise CorrDrop\n\n> 在空间维度，我们假设高度相关的单元（highly correlated units）构成 feature maps 中的判别部分（discriminative parts），这些判别部分应以较高的概率保留。给定中间第 $l$-$th$ 层的 feature maps 为 $V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}$，其中，$N = H \\times W$ 是 feature map 中的 units 的数量， $C$ 是 channels 的数量，$H$ 和 $W$ 分别表示 feature map  的高和宽。每一行 $v_i^{(l)} \\in \\mathcal{R}^C$ 表示一个 unit 的 feature vector。**<font color = green>由于 CNNs 中的特征是局部相关的，所以在 feature map 中丢弃单个 unit 效果不太好【Dropblock: A regularization method for convolutional networks _NIPS_2018】</font>** 。继之前的 DropBlock 在 feature map 中丢弃连续区域（continuous regions）的工作之后，我们进一步考虑每个局部区域的相关性和丢弃单元块（drop blocks of units）。为了获得一个 structural mask，我们首先通过 local average pooling 收集 feature map 中的局部信息，同时降低 feature map 的维度以加快相关性计算（correlation calculation）。当将 block 的大小设置为 $k$ 时，我们在 feature map 上进行 local average pooling， kernel size 为 $k$，步长为 $k$。具体来说，我们在每个 feature map 中从左到右、从上到下扫描每个大小为 $k \\times k$ 的 block，并计算每个 block 的激活值的平均值，可以描述为：\n>\n> <img src=CorrDrop_e9.png width=50% />\n>\n> 得到的 feature map 是 $V^{(l)'} \\in \\mathcal{R}^{N' \\times C}$，其中，$N' = H' \\times W'$，$H' = ceil(\\frac{H}{k})$，$W' = ceil(\\frac{W}{k})$。丢弃概率 $p$ 相应调整为：\n>\n> <img src=CorrDrop_e10.png width=50% />\n>\n> 通过下采样 feature map $V^{(l)'}$，我们采样 corrdrop mask 为：\n>\n> <img src=CorrDrop_e11.png width=50% />\n>\n> 其中， $\\Phi(.)$ 是如公式(1)-(3) 所示的特征相关性计算函数，$\\Psi(.)$ 表示如公式(4)-(8)所示的 dropout mask sampling operation。为了生成 structural mask，我们采用最近邻上采样的方法将 corrdrop mask $S_s^{(l)'} \\in \\mathcal{R}^{H' \\times W'}$ 上采样到 $S_s^{(l)} \\in \\mathcal{R}^{H \\times W}$。$S_s^{(l)}$ 中的每一个 zero entry 将被扩展为 $k \\times k$ blocks。因此，square regions of units 将被丢弃。最后，将 the spatial-wise corrdrop mask 与 the original feature maps $V^{(l)}$ 的每一个通道相乘，并 masks out 掉部分 feature regions，其表示为：\n>\n> <img src=CorrDrop_e12.png width=50% />\n>\n> 其中，$\\odot$ 表示逐点相乘运算。过程如图 2 所示。采用这种方式，我们根据局部信息来计算 feature correlation，并丢弃具有 small average correlation  的 square of regions。\n\n### 3.4. Channel-wise CorrDrop\n\n> 除了 spatial-wise features 之外，值得注意的是，**<font color = purple>每个 CNN filter 可以检测到输入数据的不同模式，即 channel-wise features 对应不同的语义模式。</font>** **<font color = green>【Weighted channel dropout for regularization of deep convolutional neural network_AAAI_2019】中的工作表明，more semantic feature channels 具有 more class-specific，其中包括一些冗余和较少激活的通道。</font>** 因此，我们尝试基于 channel-wise feature correlation 来丢弃那些不相关的特征通道并提高泛化能力，从而产生我们的 Channel-wise CorrDrop。类似于 Spatial-wise CorrDrop，给定中间第 $l$-$th$ 层的 feature maps 为 $V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}$，其中 $N = H \\times W$ 是 feature map 中的 unit 数，$C$ 是通道数，$H$ 和 $W$ 分别是 feature map 的高和宽。我们首先将第 $l$-$th$ 层 feature map $V^{(l)}$ reshape 为 $U^{(l)} = [u_1^{(l)}, ..., u_C^{(l)}]^T \\in \\mathcal{R}^{C \\times N}$。同理， the channel-wise dropout mask 计算为：\n>\n> <img src=CorrDrop_e13.png width=50% />\n>\n> 其中， $p$ 是 dropout probability，$F_C^{(l)} \\in \\mathcal{R}^C$ 是 correlation map，$S_C^{(l)} \\in \\mathcal{R}^C$ 是 corrdrop mask。按如下方式执行 the channel-wise corrdrop：\n>\n> <img src=CorrDrop_e14.png width=50% />\n>\n> 其中 $\\odot$ 指逐通道相乘，如果 $S_C^{(l)}$ 中的第 $j$-$th$ 个元素为0则 $U^{(l)}$ 的第 $j$-$th$ 个channel 将被置 0。\n\n\n\n\n\n\n\n## 5. Conclusions\n\n> 在本文中，我们提出一种新颖且有效的 structural dropout 来有效地正则化 CNNs。与现有的正则化方法会遇到 CNNs 的 under/over-dropping 问题不同，我们的方法通过基于 spatial and channel dimensions 的feature correlation 丢弃 feature 来解决这些问题。大量实验表明我们的方法在不同的机器视觉额任务，网络架构和数据集上优于其他同类方法。此外， the feature activation map 的可视化让我们了解到我们的方法可以强制模型学习更紧凑的表示（learn more compact representations）。除了图像分类任务以外，我们还验证了我们的方法在弱监督目标定位方面的有效性，并揭示了我们的方法在各种计算机视觉任务中的潜在应用。我们还表明，我们的方法可以很容易地插入普通的 CNNs 架构以正则化 CNNs。我们相信我们提出的 CorrDrop 可以作为计算机视觉社区中地通用正则化技术。\n>\n> 在未来的工作中，我们将进一步研究我们的方法在其他计算机视觉任务中的有效性，例如目标检测，语义分割等等。另一方面，图 8 中的 feature maps  的可视化启发我们继续利用特征的相关性来进一步提高网络的表征能力。\n\n\n\n\n\n# Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification_2021\n\n## Abstract\n\n> 在细粒度视觉分类（FGVC）任务中，从同一超类别（如鸟）中对一个目标的子类别进行分类，**<font color = green>高度依赖于多个判别特征</font>**。**<font color = blue>现有方法主要通过引入  attention mechanisms 来定位判别部分或特征编码方法以弱监督的方式提取高度参数化的特征来解决这个问题</font>**。在这项工作中，我们提出了一种名为 Channel DropBlock（CDB）的轻量级但有效的正则化方法，并结合两个可选的相关度量（alternative correlation metrics）来解决这个问题。**<font color = purple>关键思想是在训练阶段随机屏蔽（mask out）一组相关通道，从协同适应中破坏特征，从而增强特征表示（enhance feature representations）</font>**。在三个基准 FGVC 数据集上的大量实验表明，CDB 有效地提高了性能。\n\n> <img src=CDB_f1.png width=100% />\n>\n> 图1 CDB block 的说明。通道相关矩阵（the channel correlation matrix）是根据不同的度量生成的。然后，通过对 input feature map 应用 drop mask ，将一个通道及其对应的视觉组（its corresponding visual group）随机丢弃，丢弃的元素为 0，否则为1。\n\n\n\n## 1 Introduction\n\n> 本论文贡献总结如下：\n>\n> 1）我们通过提出一种新颖的轻量级正则化结构来**<font color = purple>解决 FGVC 任务中判别特征学习（discriminative feature learning）的挑战</font>**，该结构丢弃一组相关通道来激发网络增强特征表示，从而提取更多的判别模式（discriminative patterns）。\n>\n> 2）我们提出两个指标来度量不同特征通道之间的成对相关性，这可以帮助我们深入了解特征通道。\n>\n> 3）我们在三个流行的细粒度基准数据集上进行了大量实验，结果表明，当应用于基线网络或集成到现有方法时，所提出的 CDB 显著提高了 FGVC 的性能。\n\n\n\n> <img src=CDB_a1.png width=100% />\n\n\n\n\n\n## 3 CDB: Channel DropBlock\n\n> 在本节中，我们介绍了所提出的 Channel DropBlock（CDB）的细节。它是一种基于 dropout 的正则化技术，可以很容易地应用于分类模型的 convolutional feature maps，以改善 feature representations。我们首先描述动机 motivation，并与相关方法进行比较（第 3.1 节）。然后我们描述 Channel DropBlock 算法，该算法基于 channel correlation matrix（第 3.2 和 3.3 节）丢弃 correlated channel groups。\n\n### 3.1 Motivation\n\n> 正如之前的工作【】所示，**<font color = green>卷积特征的每个通道对应一个视觉模式</font>。**然而，由于模式之间的共同适应性，只有部分模式有助于最终预测，这将降低推理准确性，尤其是当子类别接近且难以区分时（例如，在 FGVC 任务中）。虽然 dropout 能有效地破坏特征中的协同适应性，但它对卷积特征通道的效果较差，因为这些通道是成对相关的，并且如果我们单独丢弃通道，关于输入的模式仍然可以发送到下一层。这种直觉建议我们屏蔽一组相关的通道（mask out a correlated group of channels）而不是当个通道（a single channel），以鼓励模型学习更多判别部分（discriminative parts）。**<font color = purple>CDB 的主要动机是破坏协同适应性，诱导模型充分利用更具判别性的特征（more discriminative features）。这是通过随机屏蔽整个相关通道组来实现的，这仅仅有助于最终预测的一个视觉证据。</font>**\n>\n> 我们最初开发 CDB 作为一种 attention-based  的方法，专门从 the input feature 中移除 the most important channel groups。这条线索类似于 ADL 的想法【】，因为我们开发了一个重要的分支和一个 dropout 分支，它们是随机选择的，并以对抗方式突出重要通道（highlight important channels）并移除最大激活的组（remove maximally activated group）。我们将这个实验作为消融研究进行，与随机选择的实验相比，改进有限，因为随机的实验可以给出更多的遮挡组合，并且更有可能破坏通道之间的协同适应（destruct co-adaptations between channels）。**<font color  = orange>我们的所有实验都专注于随机选择的 Channel DropBlock。</font>**\n>\n> 与 MA-CNN【】在 the final feature map 上聚类通道（clusters channels）并为每个聚类设置单独的分类器相比，本文提出的 CDB 被设计作为一个正则化块（a regularization block），更灵活地应用于任何分类模型的 convolutional feature maps。\n>\n> 相比于 SpatialDropout【】，**<font color = purple>CDB 强调通道之间是相互关联的，视觉证据仍然可以通过单独的 dropout 发送到下一层。</font>**\n>\n> 与 DropBlock 【】在空间上丢弃相关单元（drops correlated units）相比，**<font color = purple>提出的 CDB 计算逐通道的相关性（calculates correlations channel-wise），并且可以通过我们提供的两个独特的相关性度量（two unique correlation metrics）来捕获更精确的视觉证据。</font>**\n\n\n\n### 3.2 Channel DropBlock Algorithm\n\n> Algorithm 1 和 Figure 1 展示了  the Channel DropBlock 的主要过程。具体来说，CDB 的输入是 a convolutional feature map $F \\in \\mathcal{R}^{C \\times H \\times W}$，其中，$C$ 是通道的数量，$H$ 和 $W$ 分别表示 $F$ 的高和宽。**<font color = purple>我们通过计算 each feature channel 之间的两两相似度来获得 the correlation matrix $M \\in \\mathcal{R}^{C \\times C}$</font>**(描述在 3.3 节)。为了获得  the drop mask，CDB 首先从 $M$ 中随机选择一行，通过将 top $\\gamma$ 个最相关的元素设置为0，其它元素设置为1，来生成 the drop mask $M_d \\in \\mathcal{R}^{C}$。然后，通过广播乘法（broadcasting multiplication）将 the drop mask 应用于 the input feature map。通过这种方式，连续组中的特征（features in a contiguous group）被一起丢掉，这隐藏了一个特定的判别模式，并鼓励模型学习其他有助于最终预测的判别信息（discriminative information）。与 dropout 类似，所提出的 CDB 仅在具有归一化的训练阶段起作用，在推理阶段不涉及额外的参数和计算成本。\n>\n> CDB 有两个主要的超参数：$insert_pos$ 和 $\\gamma$。参数 $insert_pos$ 表示 CDB 应用的位置，$\\gamma$ 控制 dropped group 中的通道数量。\n>\n> **$insert_pos$ 的影响：**随着 CNN 结构越来越深，高层神经元对整个图像反应强烈，语义丰富，但不可避免地会丢失来自小的判别区域的细节信息（detailed information from small discriminative regions）。由于 $insert_pos$ 的设置不同， the input feature map 的信息也不同。在我们的实验中，我们完成了一项消融实验（图 Table 2 所述），将提出的 CDB block 应用于 CNN 的不同的层。\n>\n> **设置 $\\gamma$ 的值：**另一个超参数涉及到我们何时将 correlated channels 聚合成 group。这里我们将 $\\gamma$ 定义为进行 CDB 时被丢弃的组中（a dropped group）通道的百分比。在实践中，不同的 correlation metrics 会导致不同的簇数 (cluster numbers) 和 each cluster 中的通道数，因此，$\\gamma$ 的设置与我们选择的 correlation metrics 不同。\n>\n> <img src=CDB_f2.png width=100% />\n>\n> 图2：channel correlation metrics 的说明：(a) max activation，将通道分组为有区别的局部区域（discriminative local region）; (b) bilinear pooling metric 双线性池化度量，根据 visual pattern 视觉模式对 channel 进行分组。 \n\n\n\n### 3.3 Channel Correlation\n\n> 理想情况下， a correlation metric 应该是对称的，并且可以将 feature channels 聚集到不同的 visual pattern groups。在本文中，我们研究了两个候选 metric 来评估 channel 之间的 correlation。\n>\n> **Max activation metric.** 为了将 feature channels 分成 group，一个直观的想法是将 feature channels 分成不同的焦点局部区域（different focused local regions）。**<font color  = purple>受 MA-CNN【】思想的启发，我们将最大激活位置接近的通道视为一个 pattern group（we treat channels with close maximal activation position as one pattern group）。</font>** 我们使用 $3 \\times 3$ average pooling 来平滑 feature maps 并使用 argmax(.) 操作来获得 each feature channel 中峰值响应的坐标，这将 the input feature map $F$ 转换为位置矩阵 $P \\in \\mathcal{R}^{C \\times 2}$，由下式给出：\n>\n> <img src=CDB_e1.png width=60% />\n>\n> 其中，$t_x^i$，$t_y^i$ 是第 $i^{th}$ 个 channel 的峰值响应的坐标。然后计算每个激活位置之间逐对的欧氏距离并获得 the correlation matrix $M$：\n>\n> <img src=CDB_e2.png width=60% />\n>\n> 在该度量中，feature channels 被分组成 discriminative local regions 具有区别性的局部区域。此外，它是一个无参数的度量，不涉及可学习的参数。 Figure 2(a) 展示了 the max activation metric 的过程。\n>\n> **Bilinear pooling metric.** 我们还研究了一个基于 bilinear pooling operator 的 correlation metric【】，它计算归一化余弦距离来度量通道相似性（channel similarities）。该方法将 the input feature map $F$ 重构（reshape） 为一个形状为 $C \\times HW$ 的矩阵，记为 $X \\in \\mathcal{R}^{C \\times HW}$。然后通过 a normalization function 和 a bilinear pooling operator 对 reshaped matrix 重构后的矩阵进行输入，得到 channels 之间的 spatial relationship：\n>\n> <img src=CDB_e3.png width=60% />\n>\n> 其中，$\\mathcal{N(.)}$ 表示矩阵第 2 维度上的 L2 normalization function。$XX^T$ 是 the homogeneous bilinear feature 齐次双线性特征。相比于 the max activation metric，bilinear pooling metric 中的 each channel group 表示 one specific visual pattern。同样，在训练阶段和推理阶段都不涉及可训练的参数。Figure 2(b) 展示了 the bilinear pooling metric 的过程。\n\n\n\n\n\n\n\n## 5 Conclusion\n\n> 本文引入了一种新颖的正则化技术，Channel DropBlock（CDB），该技术通过相关性度量对通道进行聚类，并在训练阶段随机丢弃一个相关通道组（a correlated channel group），从而破坏协同适配的特征通道（destructs feature channels from co-adaptations）。我们证明，与现有的 FGVC 方法相比，CDB 在增强特征表示和提取多种判别模式方面更加轻量级和有效。我们在三个经过广泛测试的细粒度数据集上进行了实验，验证了所提出方法的优越性。未来工作的两个特别有趣的方向包括探索具有自适应大小的通道分组方法，以及使用综合指标度量通道相关性。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"CNN-Regularization","published":1,"updated":"2022-08-05T09:14:27.227Z","_id":"cl49vqqoy00001oul5k1wa4n1","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h1 id=\"正则化\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#正则化\"></a> 正则化</h1>\n<blockquote>\n<p>过拟合 --&gt; 在训练集上表现很好，但在测试集上效果不佳。（随着模型复杂度增加，训练误差减小，测试误差不减）</p>\n<p>发生过拟合时，模型的偏差小而方差大。</p>\n</blockquote>\n<blockquote>\n<p>正则化 --&gt; 解决模型过拟合问题！</p>\n<p>正则化通过对学习算法进行微调，使得该模型具有更好的泛化能力，改善模型在未知数据上的表现。</p>\n</blockquote>\n<blockquote>\n<p><strong><a href=\"https://zh.m.wikipedia.org/zh-hans/%E9%81%8E%E9%81%A9\" title=\"什么是过拟合？\">过拟合</a></strong></p>\n<p><strong><font color=\"green\">过拟合的本质是训练算法从统计噪声中获取了信息并表达在了模型结构的参数当中。</font></strong></p>\n<p>过拟合 --&gt; 指过于紧密或精确地匹配训练集数据，以致于无法良好地拟合测试集数据 --&gt; <strong><font color=\"green\">过拟合一般可视为违反奥卡姆剃刀原理（简约法则，若无必要，勿增实体）</font></strong></p>\n<p><strong><font color=\"green\">过拟合存在的原因 --&gt; 选择模型的标准和评价模型的标准不一致导致的。</font></strong> 选择模型时往往选取在训练数据上表现最好的模型；而评价模型时则是观察模型在不可见数据上的表现。当模型尝试“记住”训练数据而非从训练数据中学习规律时，就可能发生过拟合。</p>\n<p>在统计学系和机器学习中，为了避免或减轻过拟合，可使用以下技巧：</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 模型选择 Model Selection</font>  --&gt; 给定数据的情况下，从一组模型中选出最优模型（或具有代表性的模型）的过程。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 交叉验证 Cross-Validation</font>–&gt;  一种预测模型拟合性能的方法。包括 Leave-one-out Cross-Validation 和 K-fold Cross Validation。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  提前停止 Early Stopping</font>  --&gt; 当训练集上的 loss 不再减小（减小的程度小于某个阈值）时停止继续训练，即用于提前停止训练的回调函数callbacks。</p>\n<p><strong><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  正则化 Regularization</font></strong> --&gt;  机器学习和逆问题领域中，<strong><font color=\"green\">正则化</font></strong> 是指为解决适定性问题或过拟合而加入额外信息的过程。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  剪枝 Pruning</font> --&gt;  机器学习和搜索算法中，通过移除决策树中分辨能力较弱的部分而减小决策树大小的方法，其降低了模型的复杂度，因此能够降低过拟合风险，从而降低泛化误差。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 贝叶斯信息量准则 Bayesian Information Criterion / Schwarz Information Criterion</font>  --&gt;  在有限集合中进行模型选择的准则。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  赤池信息量准则 Akaike Information Criterion</font> --&gt;  基于信息熵，用于评估统计模型的复杂度和衡量统计模型拟合资料的优良性的一种标准。</p>\n<p><font color=\"green\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  dropout</font>  --&gt;  Hinton 提出的一种正则化方法，即在神经网络训练过程中，通过随机丢弃部分神经元，来减小神经元之间的协同适应性，从而降低网络过拟合风险。</p>\n</blockquote>\n<blockquote>\n<p><strong><a href=\"https://zhuanlan.zhihu.com/p/37120298\" title=\"正则化？\">深度学习中的正则化策略</a></strong></p>\n<p>正则化 --&gt; 深度学习中，正则化是惩罚每个节点的权重矩阵。</p>\n<p>用于深度学习的正则化技巧：</p>\n<p><font color=\"green\"> L1 &amp; L2 正则化 </font>  --&gt; 均是在损失函数 cost function 中增加一个正则项，即：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mi>a</mi><mi>y</mi><mo separator=\"true\">,</mo><mi>b</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><msub><mi>y</mi><mrow><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi><mrow><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Cost function = Loss(say, binary_{cross entropy}) + Regularization_{term} \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.28055599999999997em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">n</span><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n</blockquote>\n<h1 id=\"regularization-for-deep-learning-a-taxonomy_2017\"><span class=\"post-title-index\">2. </span><a class=\"markdownIt-Anchor\" href=\"#regularization-for-deep-learning-a-taxonomy_2017\"></a> Regularization For Deep Learning: A Taxonomy_2017</h1>\n<blockquote>\n<p>正则化的定义很多，作者提出一个系统的，统一的分类方法将现有的正则化方法进行分类，并为开发人员提供了实用的正则化方法的建议。</p>\n<p>作者将目前的正则化方法分类为 affect data 影响数据、network architectures 网络架构、error terms 错误项、regularization terms 正则化项、optimization procedures 优化过程 这几种方法。</p>\n<p>在<font color=\"green\">传统</font>意义上的优化和<font color=\"green\">较老</font>的神经网络文献中，<font color=\"green\">正则化只用于损失函数中的惩罚项</font>。</p>\n<p>2016年 Goodfellow 等人 将正则化广泛定义为：<font color=\"green\">为减少模型的测试误差，而非训练误差，对学习算法所作的任何修改。</font>即，正则化被定义为：</p>\n<p><strong><font color=\"green\">任何使模型能够更好地泛化的辅助技术，即在测试集上产生更好效果的技术都被称为正则化。</font></strong>–&gt; 该定义更符合机器学习文献，而非逆问题文献。可包括<font color=\"green\">损失函数的各种属性，损失优化算法或其他技术。</font></p>\n</blockquote>\n<blockquote>\n<p>为了为接下来提出的分类法的顶层提供一个证明，作者梳理了机器学习的理论框架。</p>\n<h2 id=\"理论框架\"><span class=\"post-title-index\">2.1. </span><a class=\"markdownIt-Anchor\" href=\"#理论框架\"></a> 理论框架</h2>\n<p>机器学习的中心任务是 <font color=\"green\"> 模型拟合：找到一个函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span>，它能很好地近似从输入 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 到期望输出 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 的期望映射。</font></p>\n<p>很多应用中，<font color=\"green\">神经网络已被证明是一个选择 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的很好的函数族。</font></p>\n<p>一个神经网络是一个具有可训练权值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi><mo>∈</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">w \\in W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>的函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub><mo>:</mo><mi>x</mi><mo>−</mo><mo>−</mo><mo>&gt;</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">f_w : x --&gt; y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>。</p>\n<p><font color=\"green\">训练网络意味着找到一个使损失函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span></span></span></span> 最小的权重配置 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>w</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">w^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.688696em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> ：</font></p><font color=\"green\">\n<img src=\"1.png\" width=\"70%\">\n<p>通常损失函数采用期望风险的形式：</p>\n<img src=\"2.png\" width=\"70%\">\n<p>其中包含两部分：<font color=\"green\">误差函数<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span>和正则化项<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span>。</font></p>\n<p><strong><font color=\"green\">误差函数 --&gt; 依赖于目标，并根据其与目标的一致性对模型预测分配惩罚。</font></strong></p>\n<p><strong><font color=\"green\">正则化项 --&gt; 根据其他标准对模型进行惩罚。这个标准可以是除了目标以外的任何东西，例如权重。</font></strong></p>\n<p>由于数据分布 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 是未知的，所以根据公式（2）期望风险不能直接降到最低。相反，给出了从分布中采样的训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>。<strong><font color=\"green\">期望风险的最小化可以通过最小化经验风险 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi mathvariant=\"script\">L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{L}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathcal\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.11110999999999999em;\">^</span></span></span></span></span></span></span></span></span></span> 得到。</font></strong></p>\n<img src=\"3.png\" width=\"70%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_i, t_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 是来自训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span> 的样本。</p>\n<p>公式（3）给出了最小化经验风险，作者根据公式中的元素，将正则化方法分为以下几类：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>：训练集 --&gt; affect data 影响数据</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span>：选择的模型族 --&gt; network architectures 网络架构</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span></em>：错误函数 --&gt; error terms 错误项</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span></em>：正则化项 --&gt; regularization terms 正则化项</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>   优化过程本身 --&gt; optimization procedures 优化过程</p>\n</font></blockquote><font color=\"green\">\n<h2 id=\"1-通过数据进行正则化\"><span class=\"post-title-index\">2.2. </span><a class=\"markdownIt-Anchor\" href=\"#1-通过数据进行正则化\"></a> 1 通过数据进行正则化</h2>\n<blockquote>\n<p>训练模型的质量很大程度取决于训练数据。</p>\n<p><font color=\"green\">通过对训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> 应用一些变换 生成一个新的数据集，从而实现对数据的正则化。</font></p>\n<p>进行数据正则化可根据以下俩原则：</p>\n<p>1）进行特征提取或预处理，将特征空间或数据分布修改为某种表示，从而简化学习任务；</p>\n<p>2）允许生成新样本来创建更大的、可能是无限的增强数据集。</p>\n<p>这两个原则在某种程度上是独立的，也可相结合。它们均依赖于（随机）参数的转换：</p>\n<img src=\"D2.png\" width=\"70%\">\n<p>作者给出第二个定义：</p>\n<p><strong><font color=\"green\">带有随机参数的变换是一个函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>τ</mi><mi>θ</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.1132em;\">τ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.1132em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，其参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 遵循某种概率分布。</font></strong></p>\n<p>所以，在此情况下，考虑 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>τ</mi><mi>θ</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.1132em;\">τ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.1132em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 可作用于 <font color=\"green\">网络输入、隐层激活或目标。</font>  输入被高斯噪声破坏<font color=\"green\">（给输入数据添加高斯噪声）</font>是随机参数变换的一个例子。</p>\n<img src=\"4.png\" width=\"70%\">\n<p><strong><font color=\"green\">变换参数的随机性带来新样本的产生，即 data augmentation 数据增广。数据增广通常专门指输入变换或隐藏激活。</font></strong></p>\n<p><strong><font color=\"blue\">作者根据变换的性质及其参数的分布对基于数据的正则化方法进行分类。</font></strong></p>\n<h3 id=\"变换参数-theta-的随机性\"><span class=\"post-title-index\">2.2.1. </span><a class=\"markdownIt-Anchor\" href=\"#变换参数-theta-的随机性\"></a> 变换参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的随机性</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\">确定性参数</font></strong>：参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 遵循 delta 分布，数据集大小保持不变。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\">随机性参数</font></strong>：允许生成一个更大的，可能是无限的数据集。 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的采样方法多种多样，有：</p>\n<p>1）<strong><font color=\"blue\">随机</font></strong>：从指定的分布中画一个随机的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></p>\n<p>2）<strong><font color=\"blue\">自适应</font></strong>： <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>的值是一个优化过程的结果，通常目标是一个最大化变换样本上的网络误差（这种具有挑战性的样本被认为是当前训练阶段信息量最大的样本），或最小化网络预测和预定义的假目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">t'</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> 之间的差异。</p>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color=\"blue\">约束优化</font></strong>：通常在硬约束下最大化误差找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>（支持 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的分布控制最强的允许变换）；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color=\"blue\">无约束优化</font></strong>：通过最大化修正误差函数找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>，使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的分布作为权重（为了完整性在此提出，但并未测试）；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color=\"blue\">随机</font></strong>：通过获取固定数量的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 样本并使用产生最高误差的样本来找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>.</p>\n</blockquote>\n<h3 id=\"对数据表示的影响\"><span class=\"post-title-index\">2.2.2. </span><a class=\"markdownIt-Anchor\" href=\"#对数据表示的影响\"></a> 对数据表示的影响</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"green\">保留表示的转换 Representation-preserving transformations</font></strong>：保留特征空间并尝试保留数据分布。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"green\">保留修改的转换 Representation-modifying transformations</font></strong>：将数据映射到不同的表示（不同的分布甚至新的特征空间），这可能会解开原始表示的潜在因素并使学习问题更容易。</p>\n<h3 id=\"转换空间\"><span class=\"post-title-index\">2.2.3. </span><a class=\"markdownIt-Anchor\" href=\"#转换空间\"></a> 转换空间</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\">输入</font></strong>：对输入 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 进行变换；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\">隐藏特征空间</font></strong>：对样本的一些深层表示进行变换（这也使用部分 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 将输入映射到隐藏特征空间；这种变换在网络 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 内部起作用，因此可被认为是架构）</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\">目标</font></strong>：转换应用于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span>（只能在训练阶段使用，因为标签在测试时没有显示给模型）</p>\n<h3 id=\"普遍性\"><span class=\"post-title-index\">2.2.4. </span><a class=\"markdownIt-Anchor\" href=\"#普遍性\"></a> 普遍性</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"green\">通用 Generic</font></strong> ：适用于所有数据域；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"green\">特定域 Domain-specific</font></strong>：针对当前问题的特定（手工制作），例如图像旋转。</p>\n<h3 id=\"theta-分布的依赖关系\"><span class=\"post-title-index\">2.2.5. </span><a class=\"markdownIt-Anchor\" href=\"#theta-分布的依赖关系\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布的依赖关系</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></font></strong>：所有样本的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布相同</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span></span></span></span></font></strong>：不同目标（类别）的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布可能不同</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t')</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|\\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|time)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">e</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|\\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> 以上方法的综合</font></strong>：即 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, \\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t')</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>t</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span></p>\n<h3 id=\"阶段\"><span class=\"post-title-index\">2.2.6. </span><a class=\"markdownIt-Anchor\" href=\"#阶段\"></a> 阶段</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"green\"> 训练</font></strong>：训练样本的转换。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color=\"blue\"> 测试</font></strong>：测试样本的转换，例如对样本的多个增强变体进行分类，并将结果汇总在它们之上。</p>\n<p><strong><font color=\"green\"> 表1回顾了使用通用转换的现有方法</font></strong>：</p>\n<img src=\"t1.png\" width=\"100%\">\n<p><strong><font color=\"blue\"> 表2列出了特定域的方法</font></strong>，特别侧重于图像领域。最常用的方法是：图像的刚性变形和弹性变形。</p>\n<img src=\"t2.png\" width=\"100%\">\n<h3 id=\"目标保留数据增广\"><span class=\"post-title-index\">2.2.7. </span><a class=\"markdownIt-Anchor\" href=\"#目标保留数据增广\"></a> 目标保留数据增广</h3>\n<p>目标保留数据增广 --&gt; 在输入和隐藏特征空间中使用随机变换，同时保持原始目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span></p>\n<p><font color=\"red\"> 未完待续！！！</font></p>\n<h3 id=\"基于数据的正则化方法的总结\"><span class=\"post-title-index\">2.2.8. </span><a class=\"markdownIt-Anchor\" href=\"#基于数据的正则化方法的总结\"></a> 基于数据的正则化方法的总结</h3>\n<p>作者对基于数据的正则化方法进行了形式化，展示了**<font color=\"green\">看似与数据正则化无关的技术，例如保留目标的数据增广、dropout 或 Batch Normalization 等技术在方法上惊人的近似，都可看做是基于数据的正则化方法。</font>**</p>\n</blockquote>\n<h2 id=\"2-通过网络架构进行正则化\"><span class=\"post-title-index\">2.3. </span><a class=\"markdownIt-Anchor\" href=\"#2-通过网络架构进行正则化\"></a> 2 通过网络架构进行正则化</h2>\n<blockquote>\n<p>为了实现正则化效果，可以选择具有特定属性或匹配特定假设的网络架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span>.</p>\n<h3 id=\"关于映射的假设\"><span class=\"post-title-index\">2.3.1. </span><a class=\"markdownIt-Anchor\" href=\"#关于映射的假设\"></a> 关于映射的假设</h3>\n<blockquote>\n<p><font color=\"green\">为了很好地拟合数据 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span>，输入-输出 的映射 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 必须具有某些属性。</font>尽管执行理想映射的精确属性可能很难，但可通过关于映射的简化假设来近似它们。<font color=\"green\">这些属性和假设可以<strong>以硬或软</strong>的方式强加于模型拟合。</font>这限制了模型的搜索空间，并允许找到更好的解决方案。</p>\n<p>**<font color=\"green\">&nbsp;作者讨论的对 输入-输出 映射施加假设的方法是网络架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的选择。</font>**一方面，架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的选择 <strong><font color=\"green\">硬连接</font><strong>了映射的某些属性；此外，在 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 和优化算法之间的相互作用中，某些权重配置比其他配置更可能通过优化获得，从而进一步</strong><font color=\"green\">以软方式限制可能的搜索空间</font></strong>。</p>\n<p><font color=\"green\">对映射施加某些假设的补充方法是<strong>正则化项</strong>，以及**（增广）数据集中存在的不变性**。</font></p>\n<p>假设可以 <strong><font color=\"green\">硬连接</font></strong> 到某些层执行的操作的定义中，和 / 或层之间的连接中。</p>\n<p>**<font color=\"green\">基于网络架构的方法如表三</font>**所示：</p>\n<img src=\"t3_1.png\" width=\"100%\">\n<img src=\"t3_2.png\" width=\"100%\">\n<p>在隐藏特征空间中对数据进行变换的正则化方法可被视为体系结构的一部分。也就是说，<font color=\"green\">在隐藏特征空间中对数据进行变换的正则化方法既属于数据正则化，也属于网络架构正则化。</font></p>\n</blockquote>\n<h3 id=\"blacksquare-权值共享-weight-sharing\"><span class=\"post-title-index\">2.3.2. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-权值共享-weight-sharing\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 权值共享 Weight sharing</h3>\n<blockquote>\n<p>权值共享 --&gt; 在网络的多个部分重复使用某个可训练参数。例如，卷积网络中的**<font color=\"blue\">权值共享不仅减少了需要学习的权重的数量，它还编码了 shift-equivariance 的先验知识和特征提取的局部性。</font>**</p>\n</blockquote>\n<h3 id=\"blacksquare-激活函数-activation-function\"><span class=\"post-title-index\">2.3.3. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-激活函数-activation-function\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 激活函数 Activation function</h3>\n<blockquote>\n<p>选择正确的激活函数非常重要。例如：</p>\n<p>1）<strong>ReLUs ** 在训练时间和准确性方面提高了许多深度架构的性能。</strong><font color=\"green\">ReLUs 的成功既可归因于：ReLUs 可避免梯度消失问题；也可归因于：它们提供了更有表现力的映射家族 more expressive families of mappings</font>**。</p>\n<p><strong><font color=\"blue\"> 一些激活函数是专门为正则化设计的。</font></strong></p>\n<p>2）<strong>Dropout</strong> ，<strong>Maxout</strong> 单元允许在测试时更精确地逼近模型集合预测的几何平均值。</p>\n<p>3）<strong>Stochastic pooling 随机池化</strong> 是最大池化的噪音版本。作者声称，这允许对激活的分布进行建模，而不仅是取最大值。</p>\n</blockquote>\n<h3 id=\"blacksquare-噪声模型-noisy-models\"><span class=\"post-title-index\">2.3.4. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-噪声模型-noisy-models\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 噪声模型 Noisy models</h3>\n<blockquote>\n<p><strong>Stochastic pooling</strong> 随机池化是确定性模型的随机泛化的一个例子。<font color=\"green\">有些模型是通过向模型的各个部分注入随机噪声来实现的。 Dropout 是最常用的噪声模型</font></p>\n</blockquote>\n<h3 id=\"blacksquare-多任务学习-multi-task-learning\"><span class=\"post-title-index\">2.3.5. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-多任务学习-multi-task-learning\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 多任务学习 Multi-task learning</h3>\n<blockquote>\n<p>**多任务学习  --&gt; 是一种特殊类型的正则化。**它可与半监督学习相结合，在辅助任务上利用未标记数据。</p>\n<p><strong>元学习</strong>中也使用了任务之间共享知识的类似概念，其中来自同一领域的多个任务被顺序学习，使用先前获得的知识作为新任务的偏差。</p>\n<p><strong>迁移学习</strong>，将一个领域的只是迁移到另一个领域。</p>\n</blockquote>\n<h3 id=\"blacksquare-模型选择-model-selection\"><span class=\"post-title-index\">2.3.6. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-模型选择-model-selection\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 模型选择 Model selection</h3>\n<blockquote>\n<p>可通过评估验证集上的预测来选择几个经过训练的模型（例如，具有不同的架构）中最好的模型。</p>\n</blockquote>\n</blockquote>\n<h2 id=\"3-通过误差函数进行正则化\"><span class=\"post-title-index\">2.4. </span><a class=\"markdownIt-Anchor\" href=\"#3-通过误差函数进行正则化\"></a> 3 通过误差函数进行正则化</h2>\n<blockquote>\n<p>**<font color=\"green\">理想情况下，误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span>（表示输出与目标的一致性） 反映了适当的质量概念，在某些情况下还反映了一些关于数据分布的假设。</font>**典型的例子是：<strong>均方误差</strong> 或 <strong>交叉熵</strong>。</p>\n<p>**<font color=\"blue\"> 误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 也可以具有正则化效果。</font>**例如，Dice coefficient optimization 系数优化，它对类别不平衡具有鲁棒性。</p>\n</blockquote>\n<h2 id=\"4-通过正则化项进行正则化\"><span class=\"post-title-index\">2.5. </span><a class=\"markdownIt-Anchor\" href=\"#4-通过正则化项进行正则化\"></a> 4 通过正则化项进行正则化</h2>\n<blockquote>\n<p><strong><font color=\"green\">正则化可以通过在损失函数中添加正则化器 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 来实现。</font><strong>与误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> （表示输出与目标的一致性）不同，</strong><font color=\"green\">正则化项独立于目标。</font><strong>相反，</strong><font color=\"blue\">正则化项用于编码所需模型的其他属性，以提供归纳偏差（即关于映射的假设，而不是输出与目标的一致性）。</font></strong> 因此，<strong><font color=\"green\">对于未标记的测试样本，正则化项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 的值能计算出来，而误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 不能计算。</font></strong></p>\n<p><strong>正则化项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 与目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span> 的独立性有一个重要含义：它允许额外使用未标记的样本（半监督学习），根据其符合一些期望的属性来改进学习模型。</strong></p>\n<blockquote>\n<p><strong><font color=\"green\"> 一个经典的正则化方法是 weight decay 权值衰减</font></strong></p><font color=\"green\">\n<img src=\"5.png\" width=\"70%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">λ</span></span></span></span> 是一个加权项，用于控制正则化对一致性的重要性。</p>\n<p><strong>从贝叶斯的角度来看</strong> ，权重衰减对应于使用对称的多元正态分布作为权重的先验：</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mi mathvariant=\"normal\">∣</mi><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>λ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>I</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(w) = \\mathcal{N}(w|0,\\lambda^{-1}I) \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\">∣</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<img src=\"e1.png\" width=\"70%\">\n<p><strong>图4 回顾了现有的通过正则化项进行正则化的方法。权重衰减（L2正则化）似乎仍然是最流行的正则化项。</strong></p>\n<p><strong><font color=\"green\">L2 正则化的目的就是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题，所以权重衰减也叫 L2 正则化。</font></strong></p>\n<img src=\"t4_1.png\" width=\"100%\">\n<img src=\"t4_2.png\" width=\"100%\">\n<img src=\"t4_3.png\" width=\"100%\">\n</font></blockquote><font color=\"green\">\n</font></blockquote><font color=\"green\">\n<h2 id=\"5-通过优化进行正则化\"><span class=\"post-title-index\">2.6. </span><a class=\"markdownIt-Anchor\" href=\"#5-通过优化进行正则化\"></a> 5 通过优化进行正则化</h2>\n<blockquote>\n<p>**<font color=\"green\">随机梯度下降（SGD）及其衍生</font> 是深度神经网络中最常用的优化算法，也是我们关注的中心。**作者也在下文列出了一些替代方法。</p>\n<p><strong><font color=\"green\">随机梯度下降法（SGD）</font> 是一种采用以下更新规则的迭代优化算法 ：</strong></p>\n<img src=\"7.png\" width=\"70%\">\n<p>如果算法在合理的时间内达到较低的训练误差（与训练集的大小呈线性关系，允许多次通过训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>，那么在某些温和的假设下，解决方案的泛化效果很好，从这个意义上来说：</p>\n<p><strong><font color=\"green\"> SGD 作为一个隐性的正则化器：即使没有使用任何额外的正则化器，较短的训练时间也能防止过拟合。</font></strong> --&gt; 这与论文《Understanding deep learning requires rethinking generalization  》中的观点一致：该论文作者在一系列实验中发现，<strong><font color=\"blue\">正则化（例如 Dropout、数据增广和权重衰减）本身既不是良好泛化的必要条件，也不是充分条件。</font></strong></p>\n<p>作者将通过优化进行正则化的方法分为三组：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 初始化 /热启动方法</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 更新方法</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 终止方法</p>\n<h3 id=\"blacksquare-initialization-and-warm-start-methods-初始化热启动方法\"><span class=\"post-title-index\">2.6.1. </span><a class=\"markdownIt-Anchor\" href=\"#blacksquare-initialization-and-warm-start-methods-初始化热启动方法\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> Initialization and warm-start methods 初始化/热启动方法</h3>\n<blockquote></blockquote>\n</blockquote>\n<h2 id=\"建议-讨论-结论\"><span class=\"post-title-index\">2.7. </span><a class=\"markdownIt-Anchor\" href=\"#建议-讨论-结论\"></a> 建议、讨论、结论</h2>\n<blockquote>\n<blockquote>\n<h3 id=\"1-该分类法的优势\"><span class=\"post-title-index\">2.7.1. </span><a class=\"markdownIt-Anchor\" href=\"#1-该分类法的优势\"></a> 1 该分类法的优势：</h3>\n<p>作者认为<font color=\"green\">这样的分类法的优势</font>有两个方面：</p>\n<p>1）它为正则化方法的用户提供了现有技术的概述，并让他们更好地了解如何为他们的问题选择理想的正则化技术组合。</p>\n<p>2）它对于开发新方法很有用，因为它全面概述了可用于正则化模型的主要原则。</p>\n</blockquote>\n<blockquote>\n<h3 id=\"2-作者建议\"><span class=\"post-title-index\">2.7.2. </span><a class=\"markdownIt-Anchor\" href=\"#2-作者建议\"></a> 2 作者建议：</h3>\n<h4 id=\"1-font-color-green对现有正则化方法用户的建议font\"><span class=\"post-title-index\">2.7.2.1. </span><a class=\"markdownIt-Anchor\" href=\"#1-font-color-green对现有正则化方法用户的建议font\"></a> 1. <strong><font color=\"green\">对现有正则化方法用户的建议</font></strong></h4>\n<p>总的来说，<font color=\"green\">尽可能多地使用数据中包含的信息以及先验知识，并主要从流行的方法开始</font>，以下程序可能是有帮助的:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 对于第一步的常见建议：</p>\n<p>1）<font color=\"green\">深度学习就是要把变异的因素分解开来。</font>应该选择一个合适的数据表示；<strong>已知的有意义的数据转换不应该外包给学习。</strong> 在几种表征中，<strong>冗余地提供相同的信息是可以的。</strong></p>\n<p>2）<font color=\"green\">输出非线性和误差函数应该反应学习目标。</font></p>\n<p>3）一个好的起点是通常工作良好的技术（例如，ReLU，成功的架构）。<strong>超参数（和架构）可以联合调优，但是很缓慢</strong>（根据经验进行插值 / 推断，而不是尝试太多的组合）。</p>\n<p>4）通常，<font color=\"green\"> 从一个简化的数据集（例如，更少和/或更简单的样本）和一个简单的网络开始是有帮助的，</font> 在获得有希望的结果后，<font color=\"green\">在调优超参数和尝试正则化方法时逐渐增加数据和网络的复杂性。</font></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 通过数据进行正则化：</p>\n<p>1）当不处理几乎无限 / 丰富的数据时：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> 如果可能的话，收集更多的真实数据（并使用考虑到其属性的方法）是可取的：</p>\n<ul>\n<li><strong>有标记的样本</strong>是最好的，但无标记的样本也可能有用（兼容半监督学习）。</li>\n<li><strong>来自相同领域的样本</strong>是最好的，但来自相似领域的样本也会有帮助（兼容领域适应和迁移学习）。</li>\n<li><strong>可靠的高质量样本</strong>是最好的，但低质量样本也有帮助（它们的信心 / 重要性可以相应地调整）。</li>\n<li><strong>给额外的任务贴上标签</strong>会很有帮助（与多任务学习兼容）。</li>\n<li>**额外的输入特性（来自额外的信息源）和 / 或数据预处理（即特定于领域的数据转换）**可能会有所帮助（网络架构需要相应的调整）。</li>\n</ul>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong>数据增广</strong>（例如，保留目标的手工特定领域转换）可以很好地弥补有限的数据。如果一直增强数据的自然方法（充分模拟自然转换），则可以尝试（并组合）它们。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> 如果增广数据的自然方法未知或被证明是不充分的，如果有足够的数据可用，就有可能从数据中推断出转换（例如学习图像变形字段）。</p>\n<p>2） 流行的泛型方法（例如 Dropout 的高级变体）通常也有帮助。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 架构和正则化项：</p>\n<p>1）关于映射的可能的有意义的属性的知识可以被用来如将不变性（对某些转换）硬连接到架构中，或者被表述为正则化项。</p>\n<p>2）流行的方法也可能有帮助（见表3和表4），但应该选择匹配映射的假设（例如，仅当需要对常规网格数据进行局部和移位等变特征提取时，卷积层才完全合适）。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 优化：</p>\n<p>1）初始化：尽管预训练的现成模型大大加快了原型的制作速度，但良好的随机初始化也应该被考虑。</p>\n<p>2）优化器：尝试一些不同的方法，包括先进的（例如 Nesterov momentum, Adam, ProxProm），可能会带来更好的结果。正确选择的参数，例如学习率，通常会产生很大的不同。</p>\n<h4 id=\"2-font-color-green对新正则化方法的开发人员的建议font\"><span class=\"post-title-index\">2.7.2.2. </span><a class=\"markdownIt-Anchor\" href=\"#2-font-color-green对新正则化方法的开发人员的建议font\"></a> 2. <font color=\"green\">对新正则化方法的开发人员的建议</font></h4>\n<p>了解最佳方法成功的原因是一个很好的基础。有希望的空白领域（分类法属性的某些组合）是可以解决的。强加在模型上的假设可能会对分类法的大多数元素产生强烈的影响。<font color=\"green\">&nbsp;<strong>数据增广比损失项更有表现力</strong>（损失项只在训练样本的无限小的邻域强制属性；数据增广可以使用丰富的转换参数分布）。</font>数据和损失项以相当软的方式强加假设和不变性，并且可以调整它们的影响，而硬连接网络架构是强加假设的更苛刻的方式。施加它们的不同假设和选项具有不同的优点和缺点。</p>\n<h4 id=\"3-font-color-green-基于数据方法的未来方向font\"><span class=\"post-title-index\">2.7.2.3. </span><a class=\"markdownIt-Anchor\" href=\"#3-font-color-green-基于数据方法的未来方向font\"></a> 3. <font color=\"green\"> 基于数据方法的未来方向</font></h4>\n<p>作者认为以下几个有前景的方向值得研究：</p>\n<p>1） <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>  的自适应采样可能会导致更低的误差和更短的训练时间（反过来，更短的训练时间可能会额外起到隐式正则化的作用）。</p>\n<p>2）作者认为学习类依赖变换会导致更可信的样本。</p>\n<p>3）在最近引发了关于真实世界对抗示例及其对摄像机位置变化等变换的鲁棒性 / 不变性的讨论后，对抗示例（以及对它们的网络鲁棒性）领域正获得越来越多的关注。对抗强烈的对抗性例子可能需要更好的正则化技术。</p>\n<h4 id=\"4-总结\"><span class=\"post-title-index\">2.7.2.4. </span><a class=\"markdownIt-Anchor\" href=\"#4-总结\"></a> 4. 总结</h4>\n<p>在这项工作中，<font color=\"green\">作者为深度学习提供了一个广义的的正则化定义，确定了<strong>神经网络训练的五个主要元素（数据，架构，错误项，正则化项，优化程序）</strong>，通过每个元素描述了正则化，包括对每个元素的进一步、更精细的分类，并从这些子类别中提供了示例方法。</font> 我们没有试图详细解释引用的作品，而只是确定它们与我们的分类相关的属性。我们的工作证明了现有方法之间的一些联系。此外，我们的系统方法通过结合现有方法的最佳特性，能够发现新的、改进的正则化方法。</p>\n</blockquote>\n</blockquote>\n<hr>\n<hr>\n<h1 id=\"heuristic-dropout-an-efficient-regularization-method-for-medical-image-segmentation-models_2022tsinghua-university\"><span class=\"post-title-index\">3. </span><a class=\"markdownIt-Anchor\" href=\"#heuristic-dropout-an-efficient-regularization-method-for-medical-image-segmentation-models_2022tsinghua-university\"></a> Heuristic Dropout: An Efficient Regularization Method For Medical Image Segmentation Models_2022,Tsinghua University</h1>\n<h2 id=\"abstract\"><span class=\"post-title-index\">3.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract\"></a> Abstract</h2>\n<blockquote>\n<p>对于真实场景中的医学图像分割，像素级的准确标注数据量通常较少，容易造成过拟合问题。这篇手稿深入研究了 Dropout 算法，该算法常用于神经网络以缓解过拟合问题。这篇手稿<strong>从解决 co-adaptation problem 协同适应问题的角度</strong>出发，解释了 <font color=\"green\">Dropout 算法</font>的基本原理，并讨论了<font color=\"green\">其衍生方法存在的局限性</font>。此外我们提出一种新颖的<strong>Heuristic Dropout启发式 Dropout 算法来解决这些局限</strong>。<strong><font color=\"green\">该算法以信息熵和方差作为启发式规则。</font></strong> 它指导我们的算法更有效地丢弃遭受协同适应问题的特征，从而更好地缓解小规模医学图像分割数据集的过拟合问题。医学图像分割数据集和模型的实验表明，所提出的算法显著提高了这些模型的性能。</p>\n</blockquote>\n<h2 id=\"intex-terms\"><span class=\"post-title-index\">3.2. </span><a class=\"markdownIt-Anchor\" href=\"#intex-terms\"></a> Intex Terms</h2>\n<blockquote>\n<p>医学图像分割，过拟合问题， Dropout 算法，信息熵</p>\n</blockquote>\n<h2 id=\"1-introduction\"><span class=\"post-title-index\">3.3. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction\"></a> 1. Introduction</h2>\n<blockquote>\n<p><strong>医学图像分割</strong>是当前<strong>计算机辅助医学诊断（Computer-aided Medical Diagnosis, CAD）系统</strong>的重要组成部分，其准确性直接影响 CAD 系统的性能。近年来，CAD 系统越来越多地参与到实际的医疗诊断任务中。因此，提高医学图像分割模型的准确性和可靠性具有重要的意义和应用价值。</p>\n<p>在医学图像分割领域， <font color=\"green\">U-Net，nnU-Net，TransUNet 等深度学习模型</font>已经在各种任务中表现出了比传统方法更好的性能。与<strong>自然图像分割</strong>相比，**<font color=\"green\">医学图像分割的数据标定高度依赖于专家知识，需要像素级的准确标定。因此，在专家指导下，像素级的准确标定数据量通常很小。</font>**小尺度的数据集容易出、造成过拟合问题，特别是当分割模型参数量较大时。</p>\n<p>解决过拟合问题的方法有很多， Dropout 算法是其中一种简单而有效的方法。它在训练过程中以一定的概率随机丢弃模型中的神经元，缓解了协同适应问题，从而缓解了深度学习模型的过拟合问题。<strong><font color=\"green\">Co-adaptation 协同适应是指每个神经元学习到的特征通常必须与上下文（即其他特定神经元）相结合的现象，以在训练过程中提供有用的信息。</font></strong> 然而，<strong>从小规模医学图像分割数据集中学习到的这种经验依赖是脆弱的，在面对测试集的分布时可能不可信。</strong> 因此，<font color=\"green\">神经元之间过多的依赖关系往往会引发过拟合问题。</font> Dropout 算法中的 drop 操作减少了深度学习模型中神经元之间的依赖关系，防止了一些神经元过度依赖其他神经元，从而在一定程度上避免了过拟合问题。</p>\n<p><strong><font color=\"green\">根据 drop 过程是否完全随机，Dropout 算法的衍生方法可以分为两类。</font></strong> <strong><font color=\"blue\">第一类是完全随机的方法</font></strong>，例如 <strong><font color=\"blue\">Spatial Dropout </font></strong> 随机丢弃通道维度中的单元，<strong><font color=\"blue\">DropBlock&nbsp;</font></strong> 将 2d blocks 视为单元并随机丢弃它们，<strong><font color=\"blue\">Stochastic Depth</font></strong> 随机丢弃残差连接。<strong><font color=\"purple\">第二类是基于规则的方法&nbsp;</font>，</strong> 例如  <strong><font color=\"purple\">Weighted Channel Dropout</font></strong> 以通道的激活值作为指导规则，<strong><font color=\"purple\">Focused Dropout</font></strong> 以 2d blocks 的激活值作为指导规则。然而，这两类现有方法都不是没有局限的。<strong><font color=\"blue\">第一类，完全随机的方法，缺乏指导规则，因此可能效率低下，丢弃的特征不一定是遭受协同适应问题的特征</font></strong>，<strong><font color=\"purple\">在第二类，基于规则的方法中，现有的指导规则不够准确，丢弃遭受协同适应问题的精确特征的效率仍有提升空间</font></strong> 。因此，<strong><font color=\"green\">本手稿提出了一种结合信息熵和方差的新的指导规则</font></strong>。在此规则的指导下，进一步提高了所提出的算法丢弃遭受协同适应问题的特征的效率。在多个医学图像分割数据集和模型上的实验表明，该算法显著提高了模型精度。</p>\n</blockquote>\n<h2 id=\"2-methodology\"><span class=\"post-title-index\">3.4. </span><a class=\"markdownIt-Anchor\" href=\"#2-methodology\"></a> 2. Methodology</h2>\n<blockquote>\n<p>作者提出了一种新颖的启发式 Dropout 算法，<strong><font color=\"green\">使用信息熵和方差作为指导规则来执行 Dropout 操作</font></strong>。该算法能够有效地丢弃遭受协同适应影响的特征，从而在很大程度上缓解了医学图像分割任务中的过拟合问题。</p>\n</blockquote>\n<h3 id=\"21-heuristic-metric\"><span class=\"post-title-index\">3.4.1. </span><a class=\"markdownIt-Anchor\" href=\"#21-heuristic-metric\"></a> 2.1. Heuristic Metric</h3>\n<blockquote>\n<p>为了有效地丢弃协同适应问题较严重的特征，作者采用信息熵作为启发式规则。<strong><font color=\"green\">信息熵可以衡量一个分布的不确定性。</font></strong></p>\n<img src=\"h1.png\" width=\"35%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 是一个随机变量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 是概率密度函数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">H(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 是关于随机变量 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 的信息熵。</p>\n<img src=\"dropout_f7.png\" width=\"70%\">\n<p>如 Dropout 一文中的图7所示，<strong><font color=\"green\">遭受严重协同适应的特征具有不确定的视觉意义，因此具有较高的信息熵值，而遭受轻微协同适应的特征具有确定的视觉意义</font></strong>，例如，看起来像目标的点、边缘或几何轮廓，这些特征的信息熵值较低。<strong><font color=\"purple\">以信息熵为指导原则，我们以更高的概率丢弃遭受严重协同适应问题的特征。</font><strong>此外，我们还需要</strong><font color=\"purple\">方差作为另一个启发式规则</font></strong>。考虑 <strong><font color=\"blue\">一个极端的情况，当分布接近于常量分布时，已知信息熵将接近最小值。</font><strong>然而，</strong><font color=\"purple\">具有常量分布 constant distribution 的特征对训练几乎不提供什么有用的信息。因此，将方差作为另一个启发式规则，我们以更大的概率丢弃更接近常量分布的特征</font></strong>。</p>\n</blockquote>\n<h3 id=\"22-heuristic-dropout-algorithm\"><span class=\"post-title-index\">3.4.2. </span><a class=\"markdownIt-Anchor\" href=\"#22-heuristic-dropout-algorithm\"></a> 2.2. Heuristic Dropout Algorithm</h3>\n<blockquote>\n<p>结合信息熵和方差两种启发式规则，得到算法1。</p>\n<img src=\"heuristic_a1.png\" width=\"70%\">\n<p>我们计算输入特征图 input feature maps 的每个通道的信息熵 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">e_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和方差 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>。我们使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>+</mo><mfrac><mi>k</mi><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">e_i + \\frac{k}{v_i + \\epsilon}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.325208em;vertical-align:-0.44509999999999994em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mathdefault mtight\">ϵ</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.44509999999999994em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> 作为指导规则。因为 feature maps 的值是连续分布的，所以我们首先要对值进行量化，然后根据直方图计算信息熵，如算法2所示。还发现使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3 \\times 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span>的 Laplace 滤波器代替 all-zero 滤波器作为 drop mask 将为模型性能带来一点提升。</p>\n<img src=\"heuristic_a2.png\" width=\"70%\">\n<p>我们的算法可无缝插入到各种模型中。以  U-Net 为例，我们在 U-Net 的编码器和解码器的每个阶段的两个连续卷积层之间插入所提出的算法，即：在前一个卷积层的激活函数之后，正好在下一个卷积层之前。</p>\n</blockquote>\n<h2 id=\"3-results\"><span class=\"post-title-index\">3.5. </span><a class=\"markdownIt-Anchor\" href=\"#3-results\"></a> 3. Results</h2>\n<h3 id=\"31-datasets\"><span class=\"post-title-index\">3.5.1. </span><a class=\"markdownIt-Anchor\" href=\"#31-datasets\"></a> 3.1. Datasets</h3>\n<blockquote>\n<p>作者在 Pancreas-CT 数据集和 BAGLS 数据集上进行实验。**考虑到在实际应用环境中，由经验丰富的专家标记的训练样本一般很少，我们专门从这些数据集中随机抽取一个子集进行试验。**对于 Pancreas-CT 数据集，我们随机选择12个扫描，然后将其转换为2545个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>512</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">512 \\times 512</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span> 的 2D 切片，以方便训练模型。在这 12幅 3D CT 扫描中，我们随机选择 8幅 作为训练集，2幅 作为验证集，2幅 作为测试集。对于 BAGLS 数据集，我们随机选择 3000个 切片作为训练集，而验证集和测试集的大小保持与原始设置相同。</p>\n</blockquote>\n<h3 id=\"32-evaluation-metrics\"><span class=\"post-title-index\">3.5.2. </span><a class=\"markdownIt-Anchor\" href=\"#32-evaluation-metrics\"></a> 3.2. Evaluation Metrics</h3>\n<blockquote>\n<p>为了对实验结果进行定量分析，我们采用了医学图像分割领域中广泛使用的 <strong><font color=\"green\">DICE 值</font></strong> 和 **<font color=\"green\">IoU 值</font>**作为评价指标。</p>\n<img src=\"heuristic_e1.png\" width=\"50%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 表示模型输出的掩膜 mask，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> 表示输入图像对应的真值 ground truth.</p>\n</blockquote>\n<h3 id=\"33-experimental-settings\"><span class=\"post-title-index\">3.5.3. </span><a class=\"markdownIt-Anchor\" href=\"#33-experimental-settings\"></a> 3.3. Experimental Settings</h3>\n<blockquote>\n<p>我们使用 <strong>Adam optimizer</strong> 来训练所有的模型，学习率为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1 \\times 10^{-3}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">3</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_1 = 0.9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_2 = 0.999</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span></span>， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\epsilon = 10^{-8}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">8</span></span></span></span></span></span></span></span></span></span></span></span>。batch size 的大小设置为可以在 GeForce RTX 2080 Ti 上以混合精度执行的最大值。我们使用 CrossEntropy 并在 Pancreas-CT 数据集上训练100个 epoch。我们使用结合 CrossEntropy, DiceLoss 和 SSIMLoss 的混合损失函数，并在 BAGLS 数据集上训练 30个epoch。我们在训练数据集上使用标准数据增广。不对模型的输出结果进行后处理。我们独立重复所有对比试验5次并报告平均结果。</p>\n</blockquote>\n<h3 id=\"34-comparison-with-dropout-derivative-methods\"><span class=\"post-title-index\">3.5.4. </span><a class=\"markdownIt-Anchor\" href=\"#34-comparison-with-dropout-derivative-methods\"></a> 3.4. Comparison with Dropout Derivative Methods</h3>\n<blockquote>\n<p>为了验证该算法的有效性，我们在 Pancreas-CT 数据集和 BAGLS 数据集上进行了实验。我们将我们的算法和其它 Dropout 的衍生算法加入到几个模型中。图1 为实验结果的箱线图 box plots，表1为定量和整体对比。</p>\n<img src=\"heuristic_f1.png\" width=\"60%\">\n<img src=\"heuristic_t1.png\" width=\"80%\">\n<p>试验结果表明，该算法在 Pancreas-CT 和 BAGLS 两个数据集上的性能都优于其他 Dropout 衍生方法。在 Pancreas-CT 数据集上，我们的算法对 U-Net 和 Attention U-Net 的 DICE 值分别提高了 3.67 和 3.37。在 BAGLS 数据集上，我们的算法对 U-Net 和 UNet3+ 的 IoU 值分别提高了 2.97 和1.12。该算法可以更加有效地提高医学图像分割模型的性能。</p>\n</blockquote>\n<h3 id=\"35-comparison-study-on-hyperparameter-k\"><span class=\"post-title-index\">3.5.5. </span><a class=\"markdownIt-Anchor\" href=\"#35-comparison-study-on-hyperparameter-k\"></a> 3.5. Comparison Study on Hyperparameter <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span></h3>\n<blockquote>\n<p>基于 U-Net 和 Pancreas-CT 数据集，作者研究了超参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的影响。随着 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的增加，性能呈现先增加后衰减的趋势，当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 3 时，性能最好。此外，从方框的方差（the variance of the box）可以看出，当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 2 时，模型性能比 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 3 时更稳定和可预测。</p>\n<img src=\"heuristic_f3.png\" width=\"60%\">\n</blockquote>\n<h3 id=\"36-verify-effectiveness-of-alleviating-co-adaptation\"><span class=\"post-title-index\">3.5.6. </span><a class=\"markdownIt-Anchor\" href=\"#36-verify-effectiveness-of-alleviating-co-adaptation\"></a> 3.6. Verify Effectiveness of Alleviating Co-adaptation</h3>\n<blockquote>\n<p>为了验证我们的算法比传统的 Dropout 更有效地缓解协同适应，我们在 Pancreas-CT 数据集上随机隐蔽了 U-Net 最终输出层之前的一定比例的中间特征图（intermediate feature maps）。<strong><font color=\"green\">对于协同适应较少的模型，由于特征之间的依赖关系较少，掩蔽特征（masked features ）导致的性能下降应该更小。</font></strong> 如图 4 所示。我们的算法在隐蔽后的性能下降明显小于传统的 dropout。实验结果表明，<strong><font color=\"blue\">使用我们的算法可以学习到更多独立特征和更少的依赖关系，因此我们的算法可以比传统的 Dropout 算法更大程度地缓解协同适应。</font></strong></p>\n<img src=\"heuristic_f4.png\" width=\"60%\">\n</blockquote>\n<h3 id=\"37-visualization-of-segmentation-results\"><span class=\"post-title-index\">3.5.7. </span><a class=\"markdownIt-Anchor\" href=\"#37-visualization-of-segmentation-results\"></a> 3.7. Visualization of Segmentation Results</h3>\n<blockquote>\n<img src=\"heuristic_f2.png\" width=\"100%\">\n<p>图 2 演示了定性分析的可视化。从上到下显示三个切片的分割结果。可视化图表明，我们的算法能更准确地分割模型。</p>\n</blockquote>\n<h2 id=\"4-conclusion\"><span class=\"post-title-index\">3.6. </span><a class=\"markdownIt-Anchor\" href=\"#4-conclusion\"></a> 4. Conclusion</h2>\n<blockquote>\n<p>作者提出了一种新的启发式 Dropout 算法来解决小规模医学图像分割数据集的过拟合问题。该算法以信息熵和方差作为启发式规则，更有效地缓解了协同适应现象，从而更好地缓解了过拟合问题。在多个数据集和模型上的实验表明，该算法具有较好的性能。此外，我们将在未来的工作中研究我们的算法与自然图像的兼容性。</p>\n</blockquote>\n<ol>\n<li>\n<blockquote>\n<p><strong><font color=\"green\">启发式</font></strong>：类似于 **<font color=\"green\"> 灵感</font>**一类的东西，可以快速进行判断，不需要逻辑性的思考论证。启发式往往可以让人们 <strong><font color=\"green\">跳出当前思维的局限</font></strong>，但因为缺乏科学依据与缜密的逻辑验证，所以有时也会出错。</p>\n</blockquote>\n</li>\n<li>\n<p><strong><font color=\"green\">医学图像与自然图像的区别：</font></strong></p>\n<blockquote>\n<p>1）医学图像大多数时放射成像，功能性成像，磁共振成像，超声成像这几种方式，而自然图像大多数是自然光成像。自然成像中，光谱比较复杂，有散射的成分，波普宽度比较大，但放射成像例如 DR, CT等，各厂家需要去除人体内的散射，使光谱单一，所以，这导致了一个重要区别，也就是：</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p><strong><font color=\"green\">在自然图像中，噪声分布绝大多数情况下可认为是均匀的，可近似为高斯噪声</font></strong>，因为直射和散射光造成光场分布可认为是均匀的；</p>\n<p>但**<font color=\"blue\">在医学图像中，由于光源单一再加上探测手段，人体厚度的影响往往会导致噪声分布不均匀，往往认为是一种泊松噪声</font>**。</p>\n<p>所以，针对医学图像的算法直接应用于自然图像效果可能不行。</p>\n<p>2）医学图像多是单通道灰度图像，尽管大量医学图像是3D的，但医学图像中没有景深的概念。</p>\n<p>3）同体态的医学图像相似度非常高，<strong>医学图像中的细微结构并不能像自然图像中那样认为是无关紧要的</strong>，在相似度极高的背景组织中的细微变化有可能代表着某种病变。</p>\n</blockquote>\n<ol start=\"3\">\n<li>\n<p><strong><font color=\"green\">Co-adaptation 协同适应</font></strong></p>\n<blockquote>\n<p><strong>过拟合</strong>：在训练集上实现高性能，但没法很好地泛化到看不见的数据（测试集）上。</p>\n<p><strong>在神经网络中，协同适应意味着一些神经元高度依赖其他神经元</strong>。如果那些独立的神经元接收到“坏”的输入，那么依赖的神经元也会受到影响，最终它会显著改变模型的性能，这就是过度拟合可能发生的情况。</p>\n<p>Hinton 提出 Dropout 来防止过拟合：网络中的每个神经元以0和1之间的概率随机丢弃。–&gt; Hinton 认为 Dropout 能防止过拟合的原因在于：<strong><font color=\"green\">通过实施 Dropout 模型被迫拥有可以学习良好特征（或所需数据表示）的神经元，而不依赖于其他神经元。因此，生成的模型对于看不见的数据可能更加鲁棒。</font></strong></p>\n</blockquote>\n<h1 id=\"inproving-neural-networks-by-preventing-co-adaptation-of-feature-detectors_2012_hinton\"><span class=\"post-title-index\">4. </span><a class=\"markdownIt-Anchor\" href=\"#inproving-neural-networks-by-preventing-co-adaptation-of-feature-detectors_2012_hinton\"></a> Inproving Neural Networks By Preventing Co-adaptation of Feature Detectors_2012_Hinton</h1>\n<blockquote>\n<p><strong><font color=\"green\">协同适应 Co-adaptation</font></strong>：一个特征检测器只在其他几个特征检测器的上下文中有用。</p>\n<p>为了阻止复杂的协同适应性，Dropout 通过在训练过程中随机丢弃一半的特征检测器，迫使 <strong><font color=\"green\">每个神经元学习检测一种特征，这种特征通常有助于产生正确的答案，因为它必须在各种内部环境中运作。</font></strong></p>\n</blockquote>\n<h1 id=\"neuron-specific-dropout-a-deterministic-regularization-technique-to-prevent-neural-networks-from-overfitting-reduce-dependence-on-large-training-samples\"><span class=\"post-title-index\">5. </span><a class=\"markdownIt-Anchor\" href=\"#neuron-specific-dropout-a-deterministic-regularization-technique-to-prevent-neural-networks-from-overfitting-reduce-dependence-on-large-training-samples\"></a> Neuron-Specific Dropout: A Deterministic Regularization Technique to Prevent Neural Networks from Overfitting &amp; Reduce Dependence on Large Training Samples</h1>\n<h2 id=\"abstract-2\"><span class=\"post-title-index\">5.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-2\"></a> Abstract</h2>\n<blockquote>\n<p>为了发展输入与输出之间的复杂关系，深度神经网络对大量参数进行训练和调整。为了使这些网络高精度地工作，需要大量数据。<strong><font color=\"green\">然而，有时训练所需的数据量并不存在或无法获得。Neuron-specific dropout (NSDropout) 被提出用来解决该问题。</font></strong>  NSDropout 会同时查看模型中层的训练过程和验证过程。通过比较数据集中每个神经元对每个类别产生的平均值，该网络能够丢弃目标单元。<strong><font color=\"purple\">该层能够预测模型在测试过程中所观察的特征或噪声，而这些特征或噪声在观察验证样本时是不存在的。</font></strong> <strong><font color=\"blue\">与 Dropout 不同的是，“thinned” networks “精简”网络不能 “unthinned” “未精简”用于测试。</font></strong> 与传统方法（包括 dropout 和其他正则化方法）相比，<strong><font color=\"green\">Neuron-specific dropout 被证明可以用更少的数据达到类似的（如果不是更好的话）测试精度。</font></strong> 实验表明， Neuron-specific dropout 减少了网络过拟合的机会，并 <strong><font color=\"green\">减少了图像识别中监督任务对大量训练样本的需要</font></strong>，同时产生了同类最佳（best-in-class）的结果。</p>\n</blockquote>\n<h2 id=\"keywords\"><span class=\"post-title-index\">5.2. </span><a class=\"markdownIt-Anchor\" href=\"#keywords\"></a> Keywords:</h2>\n<blockquote>\n<p>neural networks, regularization, model combination, deep learning, dropout</p>\n</blockquote>\n<h2 id=\"1-introduction-2\"><span class=\"post-title-index\">5.3. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-2\"></a> 1. Introduction</h2>\n<blockquote>\n<p>深度神经网络可以理解为输入与输出之间的复杂关系。通过利用数千甚至数百万个隐藏节点（神经元），这些模型可以生成一套足以预测癌症或驾驶汽车的规则。然而，要做到这一点，需要大量数据来训练并验证模型。<strong><font color=\"green\">当数据量不足时，模型可能会关注训练数据中的缺陷或者采样噪声。</font></strong> 换句话说，该模型将发现训练数据中存在的细节，而这些细节可能在实际应用中并不存在（该模型将发现训练数据中可能并不存在于其实际应用中的细节）。这些最终会导致过拟合，并且因为没办法做出一个完美的数据集，因此已经发展了其他方法来尝试减少模型过拟合的趋势。最流行的方法之一是，当模型的验证精度和训练精度出现偏差时，停止训练。另一个方法是实施权重惩罚，如 L1 和 L2 以及软权重共享（soft weight sharing）。</p>\n<img src=\"NSDropout_f1.png\" width=\"100%\">\n<p><strong><font color=\"green\">现在有几种方法来解决过拟合问题，一种是贝叶斯方法的使用</font></strong>。贝叶斯模型是根据贝叶斯定理构建统计模型。</p>\n<img src=\"NSDropout_e1.png\" width=\"20%\">\n<p>贝叶斯 ML 模型的目标是在给定先验分布 prior distribution <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(\\theta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 和 likely hood <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(x|\\theta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 的情况下估计后验分布 posterior distribution <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(\\theta|x))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span>。这些模型与经典模型的不同之处在于包含了 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> 或先验分布。<strong><font color=\"green\">一种流行的先验分布是高斯过程。</font></strong> 通过取所有参数设置的平均值，并将其值与给定训练数据的后验概率进行加权。有了先验高斯分布，我们可以假设后验分布是正态分布或落在正态钟形曲线上。<strong>假设我们有无限的计算能力，防止过拟合最好的方法是计算一个完美的后验分布。</strong> 然而，<strong><font color=\"green\">逼近后验分布</font></strong> 已经被证明可以在小模型上提供很好的结果。</p>\n<p>对于具有少量隐藏节点的模型，与单个模型相比，对使用不同架构和数据训练的不同模型的值进行平均可以提高性能。然而，对于较大的模型，此过程将过于耗费资源，无法证明回报是合理的。训练多个模型是困难的，因为找到最佳参数可能会耗费大量时间，而且训练多个大网络会占用大量资源。此外，在不同的数据子集上获取足够多的数据来训练多个网络是不可能的。最后，假设你能够使用不同数据子集来训练不同架构的多个网络，在需要快速处理的应用程序中，使用所有这些模型进行测试将花费太多的时间。</p>\n<p>这就引出了防止过拟合的第二种选择。<strong><font color=\"green\">Dropout 是一种简单而有效的方法来限制噪声对模型的影响</font></strong>。它通过“dropping 丢弃”隐藏或可见单元来防止模型过拟合，<strong><font color=\"green\">本质上是同时训练多个模型</font></strong>。通过丢弃一个单元，该单元在该步骤中不再对模型及其决策产生影响。丢弃的神经元数量由概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 决定，即独立于其它单元。</p>\n<img src=\"NSDropout_f2.png\" width=\"100%\">\n<p>图2：左：在训练阶段，假设索引 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> 处的值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">r^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 为1时，unit 出现。假设函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>a</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">a_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的输出在向量函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的输出值中不是最低的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 个百分比，则 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>r</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">r_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的值为1。右：在测试阶段，只有当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>r</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">r_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的最终值为 1 时，unit 才会出现。</p>\n<p>现在我们有了另一种防止过拟合的方法。 <strong><font color=\"green\">Neuron-Specific dropout 采用了从一个层中丢弃隐藏或可见单元的思想，而不是随机的丢弃它们</font></strong> 。与其它流行的层不同， <strong><font color=\"green\">Neurom-Specific Dropout 接受四种输入</font></strong> ：layer input 层输入，the true value of the sample 样本真值，validation layer input 验证层输入，the true value of the validation sample 验证样本真值。通过了解哪些神经元的值与该类样本的验证平均值最远，我们可以找到噪声或训练数据中的伪影在哪些地方影响了我们的模型决策。丢弃的神经元数量取决于比例 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span>。然而，这与 Dropout 不同，因为概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 表示一层中有多少 百分比的 units 将被丢弃。例如，如果在具有 20 个 units 的层中将 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 设置为 0.2，那么总的会有 4个 units 被丢弃。</p>\n<p>通常，神经网络中使用的验证数据不应该在调整超参数之外影响模型的行为，但是 neuron-specific dropout 可以提高准确性，这样就可以分割传统的训练数据集，从而永远不会使用保留的验证数据。对训练数据进行分割，以便为新的验证集保留 20% 似乎时是最佳的。</p>\n<p>类似于 Dropout，应用 neuron-specific dropout 会产生一个 “thinned” 的神经网络。这个 thinned 神经网络保存了从神经元丢弃中幸存下来的神经元的所有值。虽然可以解释为具有 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> 个 units 的神经网络代表 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">2^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.664392em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span> 个可能的 thinned 神经网络，但众所周知，随着训练的进行，从一个步骤到下一个步骤丢弃的不同的 units 的数量会减少。同样，可训练参数的总数仍然是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(n^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> ，或者更少。</p>\n<p>与 dropout 不同的是，如果使用单个的，按比例缩小的神经网络，使用该层的好处不会显示出来。当最后一次使用 mask 时，发现测试结果最好。这是有意义的，因为与 dropout 不同， units 不是随机丢弃的。当模型开始找到受噪声和特征影响的 units 时，它会将它们归零，而把它们带回来则会带回它已经学会的在没有噪声和特征的情况下改进的权重。</p>\n<p>本文结构如下。第 2 节描述了 neuron-specific dropout 的动机。第 3 节描述了之前的相关工作。第 4 节正式描述了 neuron-specific dropout  model 和它如何工作。第 5 节一个训练 neuron-specific dropout 网络的算法，并引入了不可见验证的思想。第 6 节给出了应用 NSDropout 的实验结果，并与其他形式的正则化和模型组合进行了比较。第 7 节讨论了 NSDropout 的显著特征，并分析了 neuron-specific 的影响，以及不同的参数如何改变网络的性能。</p>\n</blockquote>\n<h2 id=\"2-motivation\"><span class=\"post-title-index\">5.4. </span><a class=\"markdownIt-Anchor\" href=\"#2-motivation\"></a> 2. Motivation</h2>\n<blockquote>\n<p>neuron-specific dropout 的动机来自于 dropout。与 neuron-specific dropout 类似， dropout 切断了与神经元的连接。这项研究最初是出于限制数据量的想法，但当发现 neuron-specific dropout 也可以帮助减少过拟合时，这项研究很快改变了主意。在日常生活中，人们学到的信息比需要的更多，无论是从对话，新闻还是课程。当大脑认为学习到的信息以后不会再被使用时，就会失去一部分。这有助于防止大脑变得混乱。</p>\n<p>对于这种现象产生的一个可能的解释是大脑中一种称为干扰的现象。当一个记忆干扰其他记忆时，就会发生干扰。记忆可以定义为大脑中获取，存储，保留和稍后检索信息的过程。干扰可以是主动的或追溯的（事后的）。主动干扰是指大脑由于记忆较旧而无法记住信息。追溯性干扰是指大脑在收到新信息时保留先前学习信息的能力。<strong><font color=\"green\"> Neuron-specific dropout 使用类似于追溯性干扰的方法</font></strong> 。虽然模型本身无法知道哪些信息是有用的（类似于人脑），但验证数据可以让它们了解它们在测试时会看到什么。通过了解验证阶段存在哪些噪声，模型可以关闭或忘记哪些信息对于测试是不必要的。当每个隐藏单元被呈现出新的信息时，即前一层的输出时，它会接收并“学习”这些信息，然后，在激活之前，它会决定哪些信息“干扰”来自验证数据的信息。</p>\n</blockquote>\n<h2 id=\"8-conclusion\"><span class=\"post-title-index\">5.5. </span><a class=\"markdownIt-Anchor\" href=\"#8-conclusion\"></a> 8. Conclusion</h2>\n<blockquote>\n<p>Neuron-specific dropout (NSDropout) 是一种旨在提高神经网络准确性的 <strong><font color=\"green\">确定性正则化技术，重点关注具有少量训练数据的网络</font></strong>。通过传统的学习技术，<strong><font color=\"green\"> 网络在一组数据的输入与输出之间建立了复杂的关系，然而这些复杂的关系往往不能泛化到看不见的未知数据</font></strong>。***<font color=\"purple\">与 Dropout 不同的是， Dropout 可以随机破坏这些复杂的关系， Neuron-specific dropout 可以帮助网络理解这些复杂的关系中哪些导致了网络的过拟合，并关闭隐藏单元，强迫网络在没有这些导致网络过拟合的复杂关系的情况下学习。</font>*** 实验证明，使用 NSDropout 可以提高神经网络在图像分类领域的性能。NSDropout 能够在 MNIST 手写数字，Fashion-MNIST 和 CIFAR-10 中取得最好的（best-in-class）结果。</p>\n<p>此外，为了提高图分类网络的性能，NSDropout 还减少了对大数据集的需求。当对 MNIST 手写数字进行训练时， NSDropout 网络仅使用 750 个训练样本就能达到完美的测试精度（a perfect test accuracy）。在 Fashion-MNIST 中， NSDropout 仅使用 60000 个训练样本中的 10000 个 就能达到近乎完美的准确率（a near-perfect accuracy）. <strong><font color=\"green\">NSDropout 的一个关键特征是能够在训练期间将测试精度和训练精度联系起来。</font></strong> 这有助于限制网络过拟合的机会。</p>\n<p><strong><font color=\"blue\">NSDropout 的一个局限是训练模型所需时间的增加。</font></strong> 一个图像分类 NSDropout 模型的训练时间是相同架构的标准神经网络的 4 倍，并且没有进行优化。它需要比传统的 dropout 模型多 两倍 的时间。<strong><font color=\"blue\">时间增加的一个主要原因是 NSDropout 层中按类排序和无序的多个输入。</font></strong> 虽然排序算法变得更快，并且可以对 NSDropout 进行更多的优化，但它们仍然占用了处理过程中的大部分时间。<strong><font color=\"green\">目前 NSDropout 只是 丢弃（drops）它认为网络过于依赖的单元，但未来的工作可能会着眼于如何调整单元而不是丢弃它</font></strong>，从而在更广泛的应用程序中提高性能。</p>\n</blockquote>\n</li>\n</ol>\n<h1 id=\"structural-dropout-for-model-width-compression_2022\"><span class=\"post-title-index\">6. </span><a class=\"markdownIt-Anchor\" href=\"#structural-dropout-for-model-width-compression_2022\"></a> Structural Dropout for Model Width Compression_2022</h1>\n<h2 id=\"abstract-3\"><span class=\"post-title-index\">6.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-3\"></a> Abstract</h2>\n<blockquote>\n<p>众所周知，现有的 ML 模型是高度过度参数化的（highly over-parameterized），并且使用了比给定任务所需更多的资源。以前的工作已经探索了离线压缩模型（compressing models offline），例如，从较大的模型中提取知识到较小的模型中。这对于压缩是有效的，但没有给出衡量模型可以压缩多少的经验方法，并且需要对每个压缩模型进行额外的训练。<strong><font color=\"green\">我们提出一种只需要对原始模型和一组压缩模型进行一次训练的方法。</font></strong> 所提出的方法是一种 <strong><font color=\"green\">structural dropout</font></strong>，它会在随机选择的索引之上剪枝掉所有处于隐藏状态的元素，从而迫使模型学习其特征的重要性排序。在学习了这种排序之后，在推理阶段可以剪枝掉不重要的特征，同时保持最大的准确性，显著减小参数大小。在这项工作中，我们聚焦于全连接层的 Structural Dropout，但这个概念可以应用于任何类型的具有无序特征的层，如卷积层或 attention layers。Structural Dropout 不需要额外的剪枝 / 重新训练，但需要对每个可能的隐藏大小（each possible hideen sizes）进行额外的验证。在推理阶段，非专业人员可以在广泛的高压缩和更精确的模型之间选择最适合他们需求的内存与精度的权衡。</p>\n</blockquote>\n<h2 id=\"1-introduction-3\"><span class=\"post-title-index\">6.2. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-3\"></a> 1. Introduction</h2>\n<blockquote>\n<p>总结起来，这项工作的贡献如下：</p>\n<ol>\n<li>Dropout  的一种变体，Structural Dropout，它训练一个嵌套网络的集合，以后可以在不进行额外的重新训练（retraining）的情况下将这些网络分离出来进行压缩。</li>\n<li>在 3 个示例任务上验证 Structural Dropout，证明其在保持准确性的同时，各种方法的有效性。</li>\n<li>Structural Dropout 的实现：<a href=\"https://github.com/JulianKnodt/structural_dropout\" title=\"Strucutural Dropout \">An Implementation of Structural Dropout</a></li>\n</ol>\n<img src=\"StructuralDropout_f1.png\" width=\"100%\">\n<p>在训练过程中，Structural Dropout 并不是随机选择要剪枝的索引，而是在统一随机选择索引（a uniformly randomly selected index）后剪枝所有节点，并根据丢弃的特征数量对期望进行归一化。在一定的可能性下，我们运行整个网络，用它间接地监督较小的网络（间接将其用作较小网络的监督）</p>\n</blockquote>\n<h2 id=\"5-discussion\"><span class=\"post-title-index\">6.3. </span><a class=\"markdownIt-Anchor\" href=\"#5-discussion\"></a> 5. Discussion</h2>\n<blockquote>\n<p>Structural Dropout 作为现有架构的最小补充，可以执行超参数搜索和压缩。由于它不需要昂贵的重新训练和额外的领域知识，因此它比特定领域的修改更容易采用，并且与剪枝和量化正交。</p>\n<p>在我们的实验中，很明显存在信息饱和的陡峭悬崖，并且可以以最小的精度变化来修剪 50%-80%之间的重要特征。如在 PointNet 上所见，当使用更高的 dropout rate 进行更积极的剪枝时，可以在不损失精度的情况下剪枝高达 80%。</p>\n<p>Structural Dropout 也可能有助于提高性能，因为纯粹通过对多个模型进行采样，其中一个模型可能在给定任务上表现更好。</p>\n</blockquote>\n<h2 id=\"6-limitation\"><span class=\"post-title-index\">6.4. </span><a class=\"markdownIt-Anchor\" href=\"#6-limitation\"></a> 6. Limitation</h2>\n<blockquote>\n<p>虽然我们的方法可以直接添加到现有的体系架构中，但它也有一些缺点。</p>\n<p><strong><font color=\"green\">一个显著的缺点是 SD 增加了搜索空间，使问题变得更加困难。</font></strong> 由于问题更加困难，尽管使用更少的参数可以加快速度，但训练过程可能需要更长的时间。这个训练时间并不比原来长很多，但是不清楚到底长多少。<strong><font color=\"green\">除了增加的训练时间之外，比较所有通道宽度的验证损失是缓慢的，因为有大量的模型需要测试。</font></strong> 如果资源可用，这可以很容易地并行化，因为与训练不同，模型将是只读的，否则可以执行稀疏搜索。</p>\n<p><strong><font color=\"green\">SD 的另一个缺点是，它在训练过程中更难以验证和跟踪收敛。</font></strong> 由于 low channel width，模型在训练过程中可能随机出现精度较低的情况，因此很难确定模型的收敛性。所以，对于之前训练过的模型使用 SD 是有意义的，并且先验收敛参数已经设置。在这些模型上，它也可以用作对通道宽度执行超参数搜索的一种方式。</p>\n<p>此外，<strong><font color=\"green\">虽然我们假设所有的 SD 尺寸（SD sizes）在推断时应该是相同的，但在一个具有各种层的较大模型中，情况可能并非如此</font></strong>。对所有可能的通道大小选择执行详尽的搜索将是昂贵的，因此有效的搜索策略很重要。一个常见的假设是每个选择都可以独立做出，但我们将这一探索留给未来的工作。</p>\n<p>最后，<strong><font color=\"green\">我们的方法不能剪枝整个层，因为 SD 仅限于改变神经网络的宽度，而不能修改它的深度。</font></strong> 因此，性能瓶颈是深度的网络将无法获得同样多的好处。</p>\n</blockquote>\n<h2 id=\"7-future-work\"><span class=\"post-title-index\">6.5. </span><a class=\"markdownIt-Anchor\" href=\"#7-future-work\"></a> 7. Future Work</h2>\n<blockquote>\n<p>虽然我们展示了 Structural Dropout 在 FC 层上的应用，但同样的原理可以扩展到其他类型的层。例如，同样的思路也适用于 CNN 的 channel dimension，允许对特征进行修剪。另一种可能的扩展是针对 transformers，在 transformers 中选择多个注意力头 （ a number of attention heads）很重要。 Structural dropout 可应用于注意力头的大小和注意力头的数量。我们希望将这项工作扩展到探索在各种架构中使用 Structural Dropout 以实现实用的和高效的压缩。</p>\n</blockquote>\n<h2 id=\"8-conclusion-2\"><span class=\"post-title-index\">6.6. </span><a class=\"markdownIt-Anchor\" href=\"#8-conclusion-2\"></a> 8. Conclusion</h2>\n<blockquote>\n<p>Structural Dropout 是一种用于推测时间压缩（inference-time compression）的方法，可用作现有架构的 drop-in layer，以最小的精度损失实现大量的压缩。这是作者所知道的第一种方法，它允许在单个训练会话（a single training session）中训练许多任意压缩的模型，然后联合部署它们，代价是只部署最大的模型。从我们的实验中，我们希望 Structural Dropout 是一种用于压缩神经网络的强大工具。</p>\n</blockquote>\n<h1 id=\"dropout-regularization-for-automatic-segmented-dental-images_aciids_2021\"><span class=\"post-title-index\">7. </span><a class=\"markdownIt-Anchor\" href=\"#dropout-regularization-for-automatic-segmented-dental-images_aciids_2021\"></a> Dropout Regularization for Automatic Segmented Dental Images_ACIIDS_2021</h1>\n<h2 id=\"abstract-4\"><span class=\"post-title-index\">7.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-4\"></a> Abstract</h2>\n<blockquote>\n<p>深度神经网络是指具有大量参数的网络，是深度学习系统的核心。从这些系统中产生了一个挑战，即它们如何针对训练数据 和 / 或 验证数据集执行。由于所涉及的参数数量众多，网络往往会消耗大量时间，这会导致称为过拟合的情况。<strong><font color=\"green\">这种方法建议在模型的输入和第一个隐藏层之间引入一个 dropout 层</font></strong> 。这是非常特别的，与其他领域使用的传统 Dropout 不同，传统的 Dropout 在网络模型的每个隐藏层中引入 dropout 来解决过拟合。我们的方法涉及预处理步骤，该步骤处理数据增广以处理有限数量的牙科图像和侵蚀形态以消除图像中的噪音。此外，使用 canny 边缘检测方法进行分割以提取基于边缘的特征。除此之外，所使用的神经网路采用了 Keras 的顺序模型，这是为了将边缘分割步骤的迭代合并到一个模型中。对模型进行并行评估，首先没有 Dropout，另一个使用大小为 0.3 的 dropout 输入层。在模型中引入 dropout 作为权重正则化技术（weight regularization technique），提高了评估结果的准确性，无论是 precision 准确率还是 recall values 查全率，没有 dropout 的模型为 89.0%，而有 dropout 的模型为 91.3%。</p>\n</blockquote>\n<h2 id=\"keywords-2\"><span class=\"post-title-index\">7.2. </span><a class=\"markdownIt-Anchor\" href=\"#keywords-2\"></a> Keywords:</h2>\n<blockquote>\n<p>Deep learning,  Over-fitting,  Regularization technique,  Dropout</p>\n</blockquote>\n<h2 id=\"1-introduction-4\"><span class=\"post-title-index\">7.3. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-4\"></a> 1. Introduction</h2>\n<blockquote>\n<p>过拟合是各种深度学习系统中普遍存在的问题。当模型在训练集上训练得太好，而在测试集上训练得不太好时，通常会发生过拟合的情况。或者，欠拟合是指我们的模型在训练集和测试集上都表现不佳。</p>\n<p>因此，这两种情况可以通过几种称为权重正则化技术（weight regularization technique）的方法来处理。这些方法包括 early stopping, L1,L2 regularization 和 Dropout。在我们的方法中，我们使用 Dropout，包括丢弃神经网络模型中隐藏和可见的单元。这是通过在训练阶段忽略在随机选择的特定神经元集（certain set of neurons）的 units 单元来实现的。从技术上讲，在每个训练阶段，单个 units 要么以 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">1-p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 的概率从网络中丢弃，要么以 p 的概率保留，这样剩下的是一个简化的网路（reduced network）.</p>\n<p>Dropout 背后的关键思想是在训练过程中，从神经网络中随机丢弃 units 及其它们的连接，以防止 units 过度自适应（co-adapting）。在训练阶段丢弃不同网络模型的 units 后，这使得测试更容易接近网络平均预测的效果。从 dropout 的过程中，减少了过拟合，并进一步对其他正则化方法进行了重大改进。</p>\n<p>在其他研究（例如：<a href=\"https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\" title=\"Dropout Regularization \">Dropout Regularization in Deep Learning Models with Keras</a> ）中，展示了 dropout 如何应用于深度学习系统。可以通过多种方式在网络模型中引入 Dropout。它可以作为输入和第一个隐藏层之间的一个层来引入。其次，它可以应用于两个隐藏层之间以及最后一个隐藏层和输出层之间。</p>\n<p>我们提出的方法使用了第一个方法，在输入层和第一个隐藏层之间引入 dropout。Dropout 在大型网络中非常有用，它具有各种约束条件，如 learning rate, decay 和 momentum，以调高评估性能。</p>\n</blockquote>\n<h2 id=\"5-conclusion\"><span class=\"post-title-index\">7.4. </span><a class=\"markdownIt-Anchor\" href=\"#5-conclusion\"></a> 5. Conclusion</h2>\n<blockquote>\n<p>Dropout 是一种通过减少过拟合来改进神经网络的技术。相比于模型的隐藏层之间引入一个独立的 dropout layer，在输入可见层中引入该算法得到了很好的结果。深度神经网络模型的训练需要很长时间，从使用我们的方法进行的实验中，我们见证了模型复杂度的降低和训练时间的增加。</p>\n<p>我们的 Dropout 方法可以与其他正则化方法一起使用，以获得更好的性能结果。其他可用于获得更好的性能的权重正则化技术包括 early stopping 以解决在模型图上见证的验证损失变化。</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>神经元特定的 dropout 是针对训练样本不足或无法获得的问题，提出的方法能够在保证精度的同时，减少训练样本的需求。</li>\n<li>用于模型宽度压缩的结构 dropout ，是针对离线压缩模型没有给出衡量模型可以压缩多少的方法且需要对每个压缩模型进行额外的重新训练的问题，作者提出的方法不需要额外的剪枝或者重新训练。</li>\n<li>牙医图像自动分割的 dropout 正则化，传统的dropout是在网络模型的每个隐藏层中引入 dropout 来解决过拟合的，而作者是在模型的输入和第一个隐藏层之间引入 dropout层，也就是研究了针对牙医图像自动分割技术， dropout 通过什么样的方式能更好地应用于深度学习系统。</li>\n</ol>\n</blockquote>\n<h1 id=\"gating-dropout-communication-efficient-regularization-for-sparsely-activated-transformers_icml_2022\"><span class=\"post-title-index\">8. </span><a class=\"markdownIt-Anchor\" href=\"#gating-dropout-communication-efficient-regularization-for-sparsely-activated-transformers_icml_2022\"></a> Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers_ICML_2022</h1>\n<h2 id=\"abstract-5\"><span class=\"post-title-index\">8.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-5\"></a> Abstract</h2>\n<blockquote>\n<p>Sparsely activated transformers, 如 Mixture of Experts(MoE)，由于其惊人的缩放能力而引起了极大的兴趣，这可以不显著增加计算成本的情况下显著增加模型大小。为了实现这一点， MoE 模型用 Transformer 中的 Mixture-of-Experts  sub-layer 替换前馈子层（feed-forward sub-layer），并使用一个 gating network 门控网络将每个令牌路由到其指定的专家。由于有效训练此类模型的常见做法需要将专家和令牌分布在不同的机器上，因此这种路由策略通常会产生巨大的跨机器通信成本，因为令牌及其分配的专家可能位于不同的机器中。在这篇文章中，作者提出 Gating Dropout，它允许令牌忽略 gating network 并停留在它们的本地机器上，从而减少了跨机器通信。与传统的 dropout 类似，我们也表明， Gating Dropout 在训练中有正则化效果，从而提高了正则化性能。我们验证了 Gating Dropout 在多语言机器翻译任务中的有效性。我们的结果表明， Gating Dropout 改进了最先进的 MoE 模型，具有更快的 wallclock 时间收敛率和更好的 BLEU 分数，适用于各种模型大小和数据集。</p>\n</blockquote>\n<h2 id=\"6-conclusion\"><span class=\"post-title-index\">8.2. </span><a class=\"markdownIt-Anchor\" href=\"#6-conclusion\"></a> 6. Conclusion</h2>\n<blockquote>\n<p>我们提出 Gating Dropout 作为一种通讯高效的正则化技术来训练 sparsely activated transformers。我们观察到 sparsely activated transformers，例如 MoE 模型，通常具有非常高的跨机器通信成本，因为它们需要通过 all-to-all 通信操作将令牌发送给指定专家。Gating Dropout 通过随机跳过 all-to-all 操作来降低通信成本。这种随机跳跃在训练期间也具有正则化效果，从而提高了泛化性能。多语言翻译任务的实验证明了该方法在吞吐量（throughput）、泛化性能和收敛速度方面的有效性。</p>\n<p>关于未来的工作，我们正在研究如何通过结合 Gating Dropout 和专家剪枝来提高推理速度。Gating Dropout 目前对推理速度没有影响，因为它只是在推理阶段关闭。此外，我们还对整个训练阶段中不同 dropout rate 的影响感兴趣，因为从探索-利用的角度，探索在训练的早期阶段可能更为重要。</p>\n</blockquote>\n<h1 id=\"dropout-regularization-in-hierarchical-mixture-of-experts_neurocomputing_2021\"><span class=\"post-title-index\">9. </span><a class=\"markdownIt-Anchor\" href=\"#dropout-regularization-in-hierarchical-mixture-of-experts_neurocomputing_2021\"></a> Dropout Regularization in Hierarchical Mixture of Experts_Neurocomputing_2021</h1>\n<p>专家分层混合中的 Dropout 正则化</p>\n<h2 id=\"abstract-6\"><span class=\"post-title-index\">9.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-6\"></a> Abstract</h2>\n<blockquote>\n<p>Dropout 是一种非常有效的防止过拟合的方法，近年来已成为多层神经网络的首选正则器。专家的分层混合是一个分层门控模型（hierarchically gated model），它定义了一个软决策树，其中叶子对应于专家，决策节点对应于在其子项之间软选择的门控模型，因此，该模型定义了输入空间的软分层分区。在这项工作中，我们提出了一种用于专家分层混合的 dropout 变体，它忠实于模型定义的树层次结构，而不是像多层感知器那样具有平面的、单元独立的 dropout 应用程序。我们表明，在合成回归数据以及 MNIST 和 CIFAR-10 数据集上，我们提出的 dropout 机制可以防止在树上的过拟合，并在多个层次上提高泛化能力并提供更平滑的拟合。</p>\n</blockquote>\n<h2 id=\"5-conclusions\"><span class=\"post-title-index\">9.2. </span><a class=\"markdownIt-Anchor\" href=\"#5-conclusions\"></a> 5. Conclusions</h2>\n<blockquote>\n<p>我们提出了一种新的 Dropout 机制，可应用于专家分层混合方法及其扩展。与具有条件独立单元的平面架构上的 dropout 相比，我们的方法忠实于模型树层次结构中存在的门控依赖关系（the gating dependencies）。</p>\n<p>我们展示了我们的方法在一个合成玩具数据集以及用于数字识别和图像分类任务的两个真实数据集上的有效性。在所有的数据集上，我们看到专家的分层混合在有太多级别和叶子时确实会过拟合，但是我们提出的方法可以作为一种有效的正则化器，其中 dropout rate 作为权衡偏差 bias 和方差 variance 的超参数。</p>\n<p>我们还定性地评估 dropout 对模型学习到的表示的影响，这些模型通过提供可视化来可视化 dropout 的影响。由于我们仅不对称地丢弃左子树这一事实，我们的 dropout 方法有效地从具有不同复杂性的树结构模型的集合中采样。这种方法通过充当具有不同复杂性的模型的插值来引入正则化。</p>\n</blockquote>\n<h1 id=\"clustering-based-adaptive-dropout-for-cnn-based-classification_pr_2020\"><span class=\"post-title-index\">10. </span><a class=\"markdownIt-Anchor\" href=\"#clustering-based-adaptive-dropout-for-cnn-based-classification_pr_2020\"></a> Clustering-Based Adaptive Dropout for CNN-Based Classification_PR_2020</h1>\n<h2 id=\"abstract-7\"><span class=\"post-title-index\">10.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-7\"></a> Abstract</h2>\n<blockquote>\n<p>Dropout 被广泛用于提高深度网络的泛化能力，而目前的 dropout 变体很少动态调整网络隐藏单元或权重的 dropout 概率（dropout probabilities）以适应它们对网络优化的贡献。这篇文章提出了一种基于聚类的 dropout（clustering-based dropout），该算法基于特征、权重或它们的衍生物的网络特征，其中这些特征的 dropout 概率根据相应的聚类组自适应更新以区分它们的贡献。在 Fashion-MNIST 和 CIFAR-10 数据库以及 FER2013 和 CK+ 表情数据库上的实验结果表明，所提出的基于聚类的 dropout 比原始的  dropout 和各种 dropout 变体具有更好的准确性，并且与最先进的算法相比具有最具竞争力的性能。</p>\n</blockquote>\n<h2 id=\"keywords-3\"><span class=\"post-title-index\">10.2. </span><a class=\"markdownIt-Anchor\" href=\"#keywords-3\"></a> Keywords:</h2>\n<blockquote>\n<p>Feature and weight clustering,  Feature derivative dropout,  Self-adaptive dropout probability,  Facial expression recognition</p>\n</blockquote>\n<h2 id=\"1-introduction-5\"><span class=\"post-title-index\">10.3. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-5\"></a> 1. Introduction</h2>\n<blockquote>\n<p>为了提高深度网络的正则化能力，提出了 regularizer 正则化器、batch normalization 批归一化 和 sparse deep feature learning 稀疏深度特征学习【Sparse deep feature learning for facial expression recognition _PR_2019 】，来减少过拟合的可能性。Dropout 随机丢弃网络隐藏单元或权重，也被应用于很多目标识别问题。受隐藏单元 dropout 的启发， connection (weight) dropout 被提出来随机丢弃权重元素。Khan【Regularization of deep neural networks with spectral dropout _NN_2019 】 等人提出了对 feature map 的光谱变换进行 dropout，其中引入了与 feature map 的重塑维度（the reshaped dimension of the feature map）相对应的三种不同的变体。</p>\n<p>然而，传统 dropout 中的隐藏单元或权重是逐个元素地抑制的，这可能会忽略元素块 element block 中隐含的结构信息。Tompson 【Efficient object localization using convolutional networks_CVPR_2014】等人提出 spatial dropout 来丢弃一整个 feature map，即同时丢弃或保留（dropped or retained）一个 feature map 的所有隐藏单元。Poernomo 和 Kang 【Biased dropout and crossmap dropout: learning towards effective dropout regularization in convolutional neural network_NN_2018】根据隐藏单元响应【Learning both weights and connections for efficient neural network _NIPS_2015】的大小将特征分为大小相等的两组，并为每一组分配一个 dropout 概率。同时，提出一个额外的 cross-map dropout，其中不同 feature maps 上相同坐标的元素被同时丢弃或保留。然而，两组不足以区分不同特征之间的贡献，应该设计更多的组。Rohit 等人【Guided dropout _AAAI_2019】根据节点的强度（the strength of each node），提出通过删除节点来引导 dropout。Zhang 等人【ML-LocNet: improving object localization with multi-view learning network _ECCV_2018】提出 region dropout，利用显著区域（salient regions）的组合进行训练。但是，区域的相对位置和大小是固定的，不够灵活。Zhang 等人【Image ordinal classification and understanding: grid dropout with masking label _ICME_2018】提出 grid dropout 来减少搜索空间，以方便对全局特征的探索。然而，相同 grid 中的元素可能彼此之间存在显著差异，因此分配给整个网格 grid 相同的 dropout 概率可能不适用于相同 grid 中显著不同的元素。</p>\n<p>对于 dropout 的特征（hidden unit, feature or weight）分组，最先进的 dropout 变体没有以足够的灵活性和多样性来划分这些特征。实际上，对于网络反向传播，即使特征图和权重矩阵中的相邻元素对网络损失的贡献也大有不同。例如，图 1 展示了使用 ResNet18 的表情图像的特征图的活动区域，其中根据 heat maps response 热图影响将不同的 feature maps 分为三个不同的重要性等级，即：insignificant, fair and significant。直观的说，特征元素响应的大小应该与 dropout 概率负相关。然而，传统的 dropout 和最先进的变体无法收集这些 insignificant 无关紧要的 feature maps 或分布在整个 map 上的元素用于 dropout。在这项工作中，在 dropout 中引入了 network element clustering 网路元素聚类，将相似的元素分组，以使它们共享相同的 dropout 概率。因此，利用所提出的聚类方法，可以通过分配一个具有较大 dropout 概率的相应组来同时抑制不重要的元素。</p>\n<p>对于 dropout 概率设置，在整个网络训练过程中保持固定的 dropout 概率可能会忽略不同部分对网络优化的动态影响。 Wager 等人【Dropout training as adaptive regularization _NIPS_2013】将 dropout 训练视为一种具有二阶导数近似的自适应正则化形式。 Ba and Frey 等人【Adaptive dropout for training deep neural networks _NIPS_2013】根据矩阵元素性能 matrix elements performance，提出了一种通过更新 a probability mask matrix 概率掩码矩阵的自适应 dropout 方法。在这项工作中， dropout 概率是根据平均特征响应的聚类组（the clustering group of average characteristic response）动态更新的。</p>\n<img src=\"Clustering-basedAdaptiveDropout_f1.png\" width=\"80%\">\n<p>图1 残差网络（ResNet18）的最后一个卷积层中示例表达式的 512 个 feature maps 中的 6 个。根据感兴趣区域对 RaFD 数据库的影响，feature maps 可以被分为不同的重要性等级（importance levels），即： insignificant, fair and significant。</p>\n<p>为了考虑 dropout 的特征，通常使用深度网络中的全连接层特征（FC layer features，即 layer input）和权重矩阵（weight matrix）作为判别特征来确定识别性能（as the discriminative features to determine the recognition performance）。因此， FC features, the weights 及其 their derivatives 被用作聚类的特征。</p>\n<p>这项工作的主要贡献总结如下：</p>\n<ul>\n<li>提出了一种基于 FC features, weights or their derivatives 聚类的新型 dropout 算法；</li>\n<li>根据每组 feature, weight or derivative clustering 的响应幅度，提出了 dropout 概率的自适应更新方法；</li>\n<li>在 Fashion-MNIST 和 CIFAR10 数据库以及 FER2013 和 CK+ 表情数据库上取得了有竞争力的性能。</li>\n</ul>\n<p>本文分为以下几个部分。第 2 节介绍了提出的 clustering-based dropout。第 3 节给出了实验结果和相应的插图。最后，在第 4 节提出结论和讨论。</p>\n</blockquote>\n<h2 id=\"4-conclusion-2\"><span class=\"post-title-index\">10.4. </span><a class=\"markdownIt-Anchor\" href=\"#4-conclusion-2\"></a> 4. Conclusion</h2>\n<blockquote>\n<p>考虑到全连接特征、权重、特征和权重的衍生物中的元素对网络优化的贡献不同，提出了一种具有自适应 dropout 概率的基于聚类的 dropout 算法。本文提出的 dropout 进一步嵌入到 ResNet18 的 FC 层，用于四个公共数据库，即 Fashion-MNIST, CIFAR-10, FER2013 和  CK+，实验结果验证了所提出的 dropout 相比于其他 dropout 变体和相关的最新算法的竞争力。</p>\n<p>虽然本文提出的基于聚类的 dropout 方法获得了具有竞争力的结果，但仍有进一步改进的空间。首先，引入超参数对网络学习的影响，如簇的数量（the number of clusters），需要进一步研究。其次，深入研究不同模型选择下基于聚类的 dropout 的理论分析。最后，提出的 dropout 应该应用于更多的模型和任务。</p>\n</blockquote>\n<h1 id=\"correlation-based-structural-dropout-for-convolutional-neural-networks_pr_2021\"><span class=\"post-title-index\">11. </span><a class=\"markdownIt-Anchor\" href=\"#correlation-based-structural-dropout-for-convolutional-neural-networks_pr_2021\"></a> Correlation-based structural dropout for convolutional neural networks_PR_2021</h1>\n<h2 id=\"abstract-8\"><span class=\"post-title-index\">11.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-8\"></a> Abstract</h2>\n<blockquote>\n<p>卷积神经网络很容易遭受过拟合问题的影响，因为它们在小型训练数据集的情况下经常被过度参数化（over-parameterized）。<strong><font color=\"green\">传统的 dropout </font></strong> 随机丢弃 feature units 对于全连接网络效果很好，但 <strong><font color=\"green\">由于中间特征的高空间相关性（high spatial correlation of the intermediate features）</font></strong> 而不能很好地正则化 CNNs，这**<font color=\"green\">使得丢弃的信息流过网络，从而导致 under-dropping </font>**问题。为了更好地正则化 CNNs，已经提出了一些 structural dropout methods，例如 <strong><font color=\"blue\">SpatialDropout 和 DropBlock</font></strong>，它们通过在连续区域中随机丢弃 feature units 来实现。然而，这些方法 <strong><font color=\"blue\">可能会因为丢弃关键的判别特征（ critical discriminative features ）而遭受 over-dropping 问题</font></strong> ，从而限制了 CNNs 的性能。为了解决这些问题，我们提出了一种新颖的 structural dropout method，Correlation based Dropout（CorrDrop），通过 <strong><font color=\"purple\">基于 feature correlation 丢弃 feature units</font></strong> 来正则化 CNNs。与之前的 dropout 方法不同，我们的 CorrDrop 可以 <strong><font color=\"purple\">聚焦于判别信息（discriminative information），并以 spatial-wise 或 channel-wise 的方式丢弃 features</font></strong> 。在不同的数据集，网络架构和各种任务（如，图像分类和目标定位）上的广泛实验证明了我们的方法优于其他方法。</p>\n</blockquote>\n<h2 id=\"1-introduction-6\"><span class=\"post-title-index\">11.2. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-6\"></a> 1. Introduction</h2>\n<blockquote>\n<p>卷积神经网络已经广泛应用于机器学习社区和计算机视觉任务中，包括图像识别和目标检测。近年来， ResNet, InceptionNet 和 DenseNet 等很多先进的 CNNs 被设计来提高传统 CNNs 的性能。提出了更深和更宽的深度学习模型，以在各种计算机视觉任务中实现最先进的性能。然而，这些模型有数百万个参数，因此很容易遭受过拟合的问题，尤其在训练数据有限的情况下。因此，开发正则化方法来缓解 CNNs 的过拟合是必不可少的。</p>\n<p>早期提出的正则化方法有很多，如 weight decay, early stopping, data augmentation, batch normalization 和 dropout。这些方法已被采用作为常规的工具来正则化深度神经网络。其中，传统的 dropout 在全连接（FCs）网络中运行良好。然而，这种 <strong><font color=\"green\">dropout 并不能通过在 feature map 中随机丢弃单个 feature unit 来有效地正则化 CNNs，因为空间相关的 features 仍然允许丢弃的信息在网络中流动，从而导致 under-dropping 问题。</font></strong></p>\n<p>为了使 dropout 对 CNNs 更有效，最近提出了一些 structural dropout methods，包括 <strong><font color=\"blue\"> SpatialDropout，Cutout 和 DropBlock</font></strong> ，以提高 CNNs 的正则化能力。这些方式 <strong><font color=\"blue\">试图通过在 input/feature space 中随机丢弃整个 channels 或 square of regions </font></strong> 来正则化 CNNs。然而，由于 the feature units 以随机方式在连续区域中被丢弃，而 <strong><font color=\"blue\">不考虑图像中的语义信息</font></strong> ，因此它们存在 over-dropping 问题。 <strong><font color=\"blue\">这种丢弃 feature unit 的随机方式可能会丢弃 the input/feature maps 中的整个判别区域（the whole of discriminative regions）</font></strong> ，并限制模型的学习能力。如图 1 所示， <strong><font color=\"green\">传统的 dropout 丢弃 feature maps 中的 single unit</font></strong> ，而 <strong><font color=\"blue\">structural DropBlock 直接丢弃 feature maps 中的 a square of feature units</font></strong> ，并且可能会将信息语义区域归零。</p>\n<img src=\"CorrDrop_f1.png\" width=\"80%\">\n<p>图 1. Dropout, DropBlock 和我们的 Spatial-wise CorrDrop masks（前三行）的示例。红色的部分表示要屏蔽的区域（the regions to be masked）。<strong><font color=\"orange\">最后一行表示 CorrDrop 对应的相关热图（the corresponding correlation heatmap）</font></strong> 。<strong><font color=\"purple\">聚焦于主要目标的 feature units 之间的相关性更强。</font></strong> 与 Dropout 和 DropBlock 相比，  <strong><font color=\"purple\">CorrDrop 考虑了判别性信息（discriminative information），自适应地丢弃 feature units </font></strong> 以缓解 under-dropping  和 over-dropping 问题。</p>\n</blockquote>\n<blockquote>\n<p><strong><font color=\"orange\">受观察到的目标的判别区域（discriminative region of an object）将具有更高的特征相关性（feature correlations）的启发</font></strong>（参见图 1 最后一行），我们提出了 Correlation-based Dropout（CorrDrop），这是一种新颖且有效的 CNNs 结构 dropout 方法，该方法考虑到 spatial / channel dimensions 上的 feature correlation，从而丢弃 feature units。</p>\n<p>不同于之前的随机丢弃 feature units 的 structural dropout methods（如 DropBlock），我们的  <strong><font color=\"purple\">CorrDrop 基于判别信息（discriminative information）自适应地丢弃 feature units</font></strong> 。具体来说，我们 <strong><font color=\"purple\">首先计算 feature correlation map 以指示最具辨别力的区域（the most discriminative regions），然后自适应地屏蔽那些辨别力较差的区域（those less discriminative regions），即特征相关值较小的区域（regions with small feature correlation values）</font></strong> 。由于 feature correlation 根据相关性计算的方法可以进一步分为 spatial-wise feature correlation 和 channel-wise feature correlation，我们提出了 CorrDrop 的两种变体： Spatial-wise CorrDrop 和 Channel-wise CorrDrop，它们分别在 spatial dimension 和 channel dimension 自适应地丢弃 features。如图 1 所示，与传统的 dropout 和 DropBlock 遭遇 under-/over-dropping  问题相比，我们的 <strong><font color=\"purple\">CorrDrop 通过丢弃相关性较低的区域（part of less correlated regions）来生成自适应掩膜（adaptive  masks）</font></strong>。图像分类的大量实验表明，在公共数据集上不同的 CNNs 架构下，，我们的 CorrDrop 始终优于 dropout he DropBlock。此外，我们也证明了我们的 CorrDrop 在其他计算机视觉任务（如如目标定位）中也能很好地正则化 CNN 模型。</p>\n<p>这项工作的初步版本已经作为会议版本【Corrdrop: Correlation based dropout for convolutional neural networks _ICASSP_2020】呈现出来。在这个扩展版本中，我们包含了额外的内容，包括 the channel-wise CorrDrop，更多的消融实验，最先进的 CNNs 实验和额外的视觉任务。主要贡献可以总结如下：</p>\n<ul>\n<li>我们提出了 Correlation based structural dropout（CorrDrop），它丢弃了 feature maps 中不太相关的特征（the less correlated features），这缓解了以前的 dropout 变体以随机方式丢弃 features 的 under-/over-dropping 问题。</li>\n<li>针对 feature map 中 spatial-wise 和 channel-wise features，提出了相应的 Spatial-wise CorrDrop(SCD) 和 Channel-wise CorrDrop(CCD)。实验结果表明，<strong><font color=\"purple\">它们的互补性在于 SCD 在简单数据集（如 CIFAR-10 和 SVHN）上表现良好，而 CCD 在复杂数据集（CIFAR-100 和 TinyImageNet）上表现出色。</font></strong></li>\n<li>在各种数据集，架构和视觉任务上的大量实验表明，我们的方法可以得到持续的改进。</li>\n</ul>\n<p>这篇文章剩余部分组织如下。第 2 节简要回顾了深度学习中正则化方法和 注意力机制的相关研究成果。在第 3 节中，我们详细介绍了 CorrDrop 。第 4 节给出了实验结果。最后，我们在第 5 节得出结论。</p>\n</blockquote>\n<h2 id=\"3-methodology\"><span class=\"post-title-index\">11.3. </span><a class=\"markdownIt-Anchor\" href=\"#3-methodology\"></a> 3. Methodology</h2>\n<blockquote>\n<p>由于 under-/over-dropping 问题，大多数现有的基于 dropout 的方法在正则化 CNNs 方面受到限制。通过利用特征的相关性，我们提出一种有效的 structural dropout： correlation-based dropout（CorrDrop），它根据判别信息（discriminative information）自适应地丢弃 feature units，并且可以有效地正则化 CNNs。考虑到 CNNs 的 spatial-wise feature correlation 和 channel-wise feature correlation，我们进一步推导出 CorrDrop 的两种变体，即： Spatial-wise CorrDrop 和 Channel-wise CorrDrop。这两种变体的流程如图 2 和 图 3 所示。在下面的部分中，我们首先描述基于特征正交性（feature orthogonality）的特征相关性（feature correlation）的计算。然后，我们根据 correlation map  来采样 mask。最后，我们说明了 Spatial-wise CorrDrop 和 Channel-wise CorrDrop 的策略。</p>\n</blockquote>\n<h3 id=\"31-feature-correlation-calculation\"><span class=\"post-title-index\">11.3.1. </span><a class=\"markdownIt-Anchor\" href=\"#31-feature-correlation-calculation\"></a> 3.1. Feature correlation calculation</h3>\n<blockquote>\n<p>与以往随机丢弃 feature units  的方法不同，我们试图根据 feature correlation 来自适应地丢弃 feature units，这反映了判别信息（discriminative information）。<strong><font color=\"purple\">最近的研究【Learning deep features for discriminative localization _CVPR_2016】【Grad–cam: Visual explanations from deep networks via gradient-based localization _ICCV_2017】表明，目标的判别区域（discriminative regions）将有更高的特征相关性（feature correlations）。</font></strong> 这些观察让我们做出基本假设，即通过丢弃那些 low-correlated features 可以更有效地正则化 CNNs。  <strong><font color=\"purple\">为了表示 feature correlation，我们使用特征正交性（feature orthogonality）的度量，如之前的工作【Improved training of convolutional filters _CVPR_2019】。</font></strong> 给定 feature matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>N</mi></msub><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A = [a_1, ..., a_N]^T \\in \\mathcal{R}^{N \\times K}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 是 feature units  的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 是 feature dimension。correlation 的计算可以描述如下：</p>\n<img src=\"CorrDrop_e1.png\" width=\"50%\">\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|.|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord\">.</span><span class=\"mord\">∣</span></span></span></span> 表示绝对值运算，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">I</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span></span></span></span> 是一个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N \\times N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的单位矩阵。我们首先对  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span>  的每一行进行归一化（normalize），根据特征正交性（feature orthogonality）计算 correlation scores。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 是一个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N \\times N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的矩阵，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">P_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 行。 a single unit 的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 行的非对角元素表示所有其他 feature units  的投影（Off-diagonal elements of a row of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> for a single unit denote projection of all the other feature units）。每行的平均值表示每个 unit 的 correlation score。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">F_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的值越高该 unit 与其他 unit 高度相关。</p>\n</blockquote>\n<h3 id=\"32-correlation-based-dropout-mask-sampling\"><span class=\"post-title-index\">11.3.2. </span><a class=\"markdownIt-Anchor\" href=\"#32-correlation-based-dropout-mask-sampling\"></a> 3.2. Correlation based dropout mask sampling</h3>\n<blockquote>\n<p>为了根据 feature correlation 自适应地丢弃 units，我们根据 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 中的值为每个 unit 分配 丢弃概率（dropout probability）。一般情况下，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">F_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的值越大，我们的丢弃概率越小。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span> 中第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个 feature unit 的丢弃概率可以表示为：</p>\n<img src=\"CorrDrop_e4.png\" width=\"50%\">\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> 表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 中 feature unit 的索引。为了确保丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>γ</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\gamma_i \\in (0,1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335400000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>，我们将每个 unit 的 correlation score 进行归一化。</p>\n<p>基于丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>γ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\gamma_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> ，从伯努利分布中采样 dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">M \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<img src=\"CorrDrop_e5.png\" width=\"50%\">\n<p>经验上，类似于其他 dropout 变体，一个超参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 被引入以确保我们的 CorrDrop 不会丢弃太多 feature units。利用基于 correlation 的 dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>，我们调整 keep probability 并生成另一个 mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">B \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。当两个 masks 对应的值都为 0 时， the units 被丢弃，并得到 the final dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>S</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">S \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。CorrDrop 的 final dropout mask 可制定为：</p>\n<img src=\"CorrDrop_e6.png\" width=\"50%\">\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">numel(M)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mclose\">)</span></span></span></span> 计算 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 的 units 数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">sum(M)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">m</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mclose\">)</span></span></span></span> 计算值为 1 的 units 数量。</p>\n</blockquote>\n<blockquote>\n<img src=\"CorrDrop_f2.png\" width=\"100%\">\n<p>图 2. Spatial-wise CorrDrop 的过程。1）通过 spatial-wise  local average pooling 对前一层的 feature maps 进行下采样，kernel size 和步长为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>，用于局部特征收集和降维（local features gathering and dimension reduction）。2）基于特征正交性（feature orthogonality） 计算 correlation map，并从具有自适应丢弃概率的伯努利分布中采样 dropout mask。3）通过最近邻上采样生成 CorrDrop mask。4）通过对 CorrDrop mask 和 original feature map 进行逐元素相乘得到 regularized feature 正则化特征。</p>\n<img src=\"CorrDrop_f3.png\" width=\"100%\">\n<p>图 3. Channel-wise CorrDrop  的过程。1）基于 correlation calculation 计算 correlation vector。2）根据 correlation vector 对 CorrDrop mask 进行采样，即相关越少的通道（the less correlated channels）越容易被丢弃。3）将 CorrDrop mask 与 original feature map 进行逐通道相乘。</p>\n</blockquote>\n<h3 id=\"33-spatial-wise-corrdrop\"><span class=\"post-title-index\">11.3.3. </span><a class=\"markdownIt-Anchor\" href=\"#33-spatial-wise-corrdrop\"></a> 3.3 Spatial-wise CorrDrop</h3>\n<blockquote>\n<p>在空间维度，我们假设高度相关的单元（highly correlated units）构成 feature maps 中的判别部分（discriminative parts），这些判别部分应以较高的概率保留。给定中间第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层的 feature maps 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>v</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>v</mi><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N = H \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是 feature map 中的 units 的数量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是 channels 的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别表示 feature map  的高和宽。每一行 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">v_i^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 表示一个 unit 的 feature vector。<strong><font color=\"green\">由于 CNNs 中的特征是局部相关的，所以在 feature map 中丢弃单个 unit 效果不太好【Dropblock: A regularization method for convolutional networks _NIPS_2018】</font></strong> 。继之前的 DropBlock 在 feature map 中丢弃连续区域（continuous regions）的工作之后，我们进一步考虑每个局部区域的相关性和丢弃单元块（drop blocks of units）。为了获得一个 structural mask，我们首先通过 local average pooling 收集 feature map 中的局部信息，同时降低 feature map 的维度以加快相关性计算（correlation calculation）。当将 block 的大小设置为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 时，我们在 feature map 上进行 local average pooling， kernel size 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>，步长为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>。具体来说，我们在每个 feature map 中从左到右、从上到下扫描每个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k \\times k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的 block，并计算每个 block 的激活值的平均值，可以描述为：</p>\n<img src=\"CorrDrop_e9.png\" width=\"50%\">\n<p>得到的 feature map 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><msup><mi>N</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)'} \\in \\mathcal{R}^{N' \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.98158em;vertical-align:-0.0391em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>N</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">N' = H' \\times W'</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.835222em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mfrac><mi>H</mi><mi>k</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">H' = ceil(\\frac{H}{k})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.217331em;vertical-align:-0.345em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mfrac><mi>W</mi><mi>k</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W' = ceil(\\frac{W}{k})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.217331em;vertical-align:-0.345em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span>。丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 相应调整为：</p>\n<img src=\"CorrDrop_e10.png\" width=\"50%\">\n<p>通过下采样 feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)'}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，我们采样 corrdrop mask 为：</p>\n<img src=\"CorrDrop_e11.png\" width=\"50%\">\n<p>其中， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Phi(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span> 是如公式(1)-(3) 所示的特征相关性计算函数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Ψ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Psi(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ψ</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span> 表示如公式(4)-(8)所示的 dropout mask sampling operation。为了生成 structural mask，我们采用最近邻上采样的方法将 corrdrop mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)'} \\in \\mathcal{R}^{H' \\times W'}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2158720000000003em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0992800000000003em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 上采样到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)} \\in \\mathcal{R}^{H \\times W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.161392em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.161392em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span></span></span></span> 中的每一个 zero entry 将被扩展为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k \\times k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> blocks。因此，square regions of units 将被丢弃。最后，将 the spatial-wise corrdrop mask 与 the original feature maps <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的每一个通道相乘，并 masks out 掉部分 feature regions，其表示为：</p>\n<img src=\"CorrDrop_e12.png\" width=\"50%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">⊙</span></span></span></span> 表示逐点相乘运算。过程如图 2 所示。采用这种方式，我们根据局部信息来计算 feature correlation，并丢弃具有 small average correlation  的 square of regions。</p>\n</blockquote>\n<h3 id=\"34-channel-wise-corrdrop\"><span class=\"post-title-index\">11.3.4. </span><a class=\"markdownIt-Anchor\" href=\"#34-channel-wise-corrdrop\"></a> 3.4. Channel-wise CorrDrop</h3>\n<blockquote>\n<p>除了 spatial-wise features 之外，值得注意的是，<strong><font color=\"purple\">每个 CNN filter 可以检测到输入数据的不同模式，即 channel-wise features 对应不同的语义模式。</font></strong> <strong><font color=\"green\">【Weighted channel dropout for regularization of deep convolutional neural network_AAAI_2019】中的工作表明，more semantic feature channels 具有 more class-specific，其中包括一些冗余和较少激活的通道。</font></strong> 因此，我们尝试基于 channel-wise feature correlation 来丢弃那些不相关的特征通道并提高泛化能力，从而产生我们的 Channel-wise CorrDrop。类似于 Spatial-wise CorrDrop，给定中间第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层的 feature maps 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>v</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>v</mi><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N = H \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是 feature map 中的 unit 数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是通道数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别是 feature map 的高和宽。我们首先将第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层 feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> reshape 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>U</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>u</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>u</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">U^{(l)} = [u_1^{(l)}, ..., u_C^{(l)}]^T \\in \\mathcal{R}^{C \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。同理， the channel-wise dropout mask 计算为：</p>\n<img src=\"CorrDrop_e13.png\" width=\"50%\">\n<p>其中， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 是 dropout probability，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>F</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">F_C^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 是 correlation map，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">S_C^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 是 corrdrop mask。按如下方式执行 the channel-wise corrdrop：</p>\n<img src=\"CorrDrop_e14.png\" width=\"50%\">\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">⊙</span></span></span></span> 指逐通道相乘，如果 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">S_C^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span></span></span></span> 中的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个元素为0则 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>U</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">U^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个channel 将被置 0。</p>\n</blockquote>\n<h2 id=\"5-conclusions-2\"><span class=\"post-title-index\">11.4. </span><a class=\"markdownIt-Anchor\" href=\"#5-conclusions-2\"></a> 5. Conclusions</h2>\n<blockquote>\n<p>在本文中，我们提出一种新颖且有效的 structural dropout 来有效地正则化 CNNs。与现有的正则化方法会遇到 CNNs 的 under/over-dropping 问题不同，我们的方法通过基于 spatial and channel dimensions 的feature correlation 丢弃 feature 来解决这些问题。大量实验表明我们的方法在不同的机器视觉额任务，网络架构和数据集上优于其他同类方法。此外， the feature activation map 的可视化让我们了解到我们的方法可以强制模型学习更紧凑的表示（learn more compact representations）。除了图像分类任务以外，我们还验证了我们的方法在弱监督目标定位方面的有效性，并揭示了我们的方法在各种计算机视觉任务中的潜在应用。我们还表明，我们的方法可以很容易地插入普通的 CNNs 架构以正则化 CNNs。我们相信我们提出的 CorrDrop 可以作为计算机视觉社区中地通用正则化技术。</p>\n<p>在未来的工作中，我们将进一步研究我们的方法在其他计算机视觉任务中的有效性，例如目标检测，语义分割等等。另一方面，图 8 中的 feature maps  的可视化启发我们继续利用特征的相关性来进一步提高网络的表征能力。</p>\n</blockquote>\n<h1 id=\"channel-dropblock-an-improved-regularization-method-for-fine-grained-visual-classification_2021\"><span class=\"post-title-index\">12. </span><a class=\"markdownIt-Anchor\" href=\"#channel-dropblock-an-improved-regularization-method-for-fine-grained-visual-classification_2021\"></a> Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification_2021</h1>\n<h2 id=\"abstract-9\"><span class=\"post-title-index\">12.1. </span><a class=\"markdownIt-Anchor\" href=\"#abstract-9\"></a> Abstract</h2>\n<blockquote>\n<p>在细粒度视觉分类（FGVC）任务中，从同一超类别（如鸟）中对一个目标的子类别进行分类，<strong><font color=\"green\">高度依赖于多个判别特征</font></strong>。<strong><font color=\"blue\">现有方法主要通过引入  attention mechanisms 来定位判别部分或特征编码方法以弱监督的方式提取高度参数化的特征来解决这个问题</font></strong>。在这项工作中，我们提出了一种名为 Channel DropBlock（CDB）的轻量级但有效的正则化方法，并结合两个可选的相关度量（alternative correlation metrics）来解决这个问题。<strong><font color=\"purple\">关键思想是在训练阶段随机屏蔽（mask out）一组相关通道，从协同适应中破坏特征，从而增强特征表示（enhance feature representations）</font></strong>。在三个基准 FGVC 数据集上的大量实验表明，CDB 有效地提高了性能。</p>\n</blockquote>\n<blockquote>\n<img src=\"CDB_f1.png\" width=\"100%\">\n<p>图1 CDB block 的说明。通道相关矩阵（the channel correlation matrix）是根据不同的度量生成的。然后，通过对 input feature map 应用 drop mask ，将一个通道及其对应的视觉组（its corresponding visual group）随机丢弃，丢弃的元素为 0，否则为1。</p>\n</blockquote>\n<h2 id=\"1-introduction-7\"><span class=\"post-title-index\">12.2. </span><a class=\"markdownIt-Anchor\" href=\"#1-introduction-7\"></a> 1 Introduction</h2>\n<blockquote>\n<p>本论文贡献总结如下：</p>\n<p>1）我们通过提出一种新颖的轻量级正则化结构来**<font color=\"purple\">解决 FGVC 任务中判别特征学习（discriminative feature learning）的挑战</font>**，该结构丢弃一组相关通道来激发网络增强特征表示，从而提取更多的判别模式（discriminative patterns）。</p>\n<p>2）我们提出两个指标来度量不同特征通道之间的成对相关性，这可以帮助我们深入了解特征通道。</p>\n<p>3）我们在三个流行的细粒度基准数据集上进行了大量实验，结果表明，当应用于基线网络或集成到现有方法时，所提出的 CDB 显著提高了 FGVC 的性能。</p>\n</blockquote>\n<blockquote>\n<img src=\"CDB_a1.png\" width=\"100%\">\n</blockquote>\n<h2 id=\"3-cdb-channel-dropblock\"><span class=\"post-title-index\">12.3. </span><a class=\"markdownIt-Anchor\" href=\"#3-cdb-channel-dropblock\"></a> 3 CDB: Channel DropBlock</h2>\n<blockquote>\n<p>在本节中，我们介绍了所提出的 Channel DropBlock（CDB）的细节。它是一种基于 dropout 的正则化技术，可以很容易地应用于分类模型的 convolutional feature maps，以改善 feature representations。我们首先描述动机 motivation，并与相关方法进行比较（第 3.1 节）。然后我们描述 Channel DropBlock 算法，该算法基于 channel correlation matrix（第 3.2 和 3.3 节）丢弃 correlated channel groups。</p>\n</blockquote>\n<h3 id=\"31-motivation\"><span class=\"post-title-index\">12.3.1. </span><a class=\"markdownIt-Anchor\" href=\"#31-motivation\"></a> 3.1 Motivation</h3>\n<blockquote>\n<p>正如之前的工作【】所示，<strong><font color=\"green\">卷积特征的每个通道对应一个视觉模式</font>。<strong>然而，由于模式之间的共同适应性，只有部分模式有助于最终预测，这将降低推理准确性，尤其是当子类别接近且难以区分时（例如，在 FGVC 任务中）。虽然 dropout 能有效地破坏特征中的协同适应性，但它对卷积特征通道的效果较差，因为这些通道是成对相关的，并且如果我们单独丢弃通道，关于输入的模式仍然可以发送到下一层。这种直觉建议我们屏蔽一组相关的通道（mask out a correlated group of channels）而不是当个通道（a single channel），以鼓励模型学习更多判别部分（discriminative parts）。</strong><font color=\"purple\">CDB 的主要动机是破坏协同适应性，诱导模型充分利用更具判别性的特征（more discriminative features）。这是通过随机屏蔽整个相关通道组来实现的，这仅仅有助于最终预测的一个视觉证据。</font></strong></p>\n<p>我们最初开发 CDB 作为一种 attention-based  的方法，专门从 the input feature 中移除 the most important channel groups。这条线索类似于 ADL 的想法【】，因为我们开发了一个重要的分支和一个 dropout 分支，它们是随机选择的，并以对抗方式突出重要通道（highlight important channels）并移除最大激活的组（remove maximally activated group）。我们将这个实验作为消融研究进行，与随机选择的实验相比，改进有限，因为随机的实验可以给出更多的遮挡组合，并且更有可能破坏通道之间的协同适应（destruct co-adaptations between channels）。<strong><font color=\"orange\">我们的所有实验都专注于随机选择的 Channel DropBlock。</font></strong></p>\n<p>与 MA-CNN【】在 the final feature map 上聚类通道（clusters channels）并为每个聚类设置单独的分类器相比，本文提出的 CDB 被设计作为一个正则化块（a regularization block），更灵活地应用于任何分类模型的 convolutional feature maps。</p>\n<p>相比于 SpatialDropout【】，<strong><font color=\"purple\">CDB 强调通道之间是相互关联的，视觉证据仍然可以通过单独的 dropout 发送到下一层。</font></strong></p>\n<p>与 DropBlock 【】在空间上丢弃相关单元（drops correlated units）相比，<strong><font color=\"purple\">提出的 CDB 计算逐通道的相关性（calculates correlations channel-wise），并且可以通过我们提供的两个独特的相关性度量（two unique correlation metrics）来捕获更精确的视觉证据。</font></strong></p>\n</blockquote>\n<h3 id=\"32-channel-dropblock-algorithm\"><span class=\"post-title-index\">12.3.2. </span><a class=\"markdownIt-Anchor\" href=\"#32-channel-dropblock-algorithm\"></a> 3.2 Channel DropBlock Algorithm</h3>\n<blockquote>\n<p>Algorithm 1 和 Figure 1 展示了  the Channel DropBlock 的主要过程。具体来说，CDB 的输入是 a convolutional feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F \\in \\mathcal{R}^{C \\times H \\times W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是通道的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 的高和宽。<strong><font color=\"purple\">我们通过计算 each feature channel 之间的两两相似度来获得 the correlation matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">M \\in \\mathcal{R}^{C \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span></font></strong>(描述在 3.3 节)。为了获得  the drop mask，CDB 首先从 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 中随机选择一行，通过将 top <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 个最相关的元素设置为0，其它元素设置为1，来生成 the drop mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>M</mi><mi>d</mi></msub><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">M_d \\in \\mathcal{R}^{C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>。然后，通过广播乘法（broadcasting multiplication）将 the drop mask 应用于 the input feature map。通过这种方式，连续组中的特征（features in a contiguous group）被一起丢掉，这隐藏了一个特定的判别模式，并鼓励模型学习其他有助于最终预测的判别信息（discriminative information）。与 dropout 类似，所提出的 CDB 仅在具有归一化的训练阶段起作用，在推理阶段不涉及额外的参数和计算成本。</p>\n<p>CDB 有两个主要的超参数：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>。参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 表示 CDB 应用的位置，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 控制 dropped group 中的通道数量。</p>\n<p>**<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 的影响：**随着 CNN 结构越来越深，高层神经元对整个图像反应强烈，语义丰富，但不可避免地会丢失来自小的判别区域的细节信息（detailed information from small discriminative regions）。由于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 的设置不同， the input feature map 的信息也不同。在我们的实验中，我们完成了一项消融实验（图 Table 2 所述），将提出的 CDB block 应用于 CNN 的不同的层。</p>\n<p>**设置 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 的值：**另一个超参数涉及到我们何时将 correlated channels 聚合成 group。这里我们将 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 定义为进行 CDB 时被丢弃的组中（a dropped group）通道的百分比。在实践中，不同的 correlation metrics 会导致不同的簇数 (cluster numbers) 和 each cluster 中的通道数，因此，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 的设置与我们选择的 correlation metrics 不同。</p>\n<img src=\"CDB_f2.png\" width=\"100%\">\n<p>图2：channel correlation metrics 的说明：(a) max activation，将通道分组为有区别的局部区域（discriminative local region）; (b) bilinear pooling metric 双线性池化度量，根据 visual pattern 视觉模式对 channel 进行分组。</p>\n</blockquote>\n<h3 id=\"33-channel-correlation\"><span class=\"post-title-index\">12.3.3. </span><a class=\"markdownIt-Anchor\" href=\"#33-channel-correlation\"></a> 3.3 Channel Correlation</h3>\n<blockquote>\n<p>理想情况下， a correlation metric 应该是对称的，并且可以将 feature channels 聚集到不同的 visual pattern groups。在本文中，我们研究了两个候选 metric 来评估 channel 之间的 correlation。</p>\n<p><strong>Max activation metric.</strong> 为了将 feature channels 分成 group，一个直观的想法是将 feature channels 分成不同的焦点局部区域（different focused local regions）。<strong><font color=\"purple\">受 MA-CNN【】思想的启发，我们将最大激活位置接近的通道视为一个 pattern group（we treat channels with close maximal activation position as one pattern group）。</font></strong> 我们使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3 \\times 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span> average pooling 来平滑 feature maps 并使用 argmax(.) 操作来获得 each feature channel 中峰值响应的坐标，这将 the input feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 转换为位置矩阵 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P \\in \\mathcal{R}^{C \\times 2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span>，由下式给出：</p>\n<img src=\"CDB_e1.png\" width=\"60%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>t</mi><mi>x</mi><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">t_x^i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.071664em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.824664em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">x</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>t</mi><mi>y</mi><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">t_y^i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2077719999999998em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.824664em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span></span></span></span> 是第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.849108em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.849108em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\">h</span></span></span></span></span></span></span></span></span></span></span></span> 个 channel 的峰值响应的坐标。然后计算每个激活位置之间逐对的欧氏距离并获得 the correlation matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>：</p>\n<img src=\"CDB_e2.png\" width=\"60%\">\n<p>在该度量中，feature channels 被分组成 discriminative local regions 具有区别性的局部区域。此外，它是一个无参数的度量，不涉及可学习的参数。 Figure 2(a) 展示了 the max activation metric 的过程。</p>\n<p><strong>Bilinear pooling metric.</strong> 我们还研究了一个基于 bilinear pooling operator 的 correlation metric【】，它计算归一化余弦距离来度量通道相似性（channel similarities）。该方法将 the input feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 重构（reshape） 为一个形状为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">C \\times HW</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 的矩阵，记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X \\in \\mathcal{R}^{C \\times HW}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>。然后通过 a normalization function 和 a bilinear pooling operator 对 reshaped matrix 重构后的矩阵进行输入，得到 channels 之间的 spatial relationship：</p>\n<img src=\"CDB_e3.png\" width=\"60%\">\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N(.)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span></span> 表示矩阵第 2 维度上的 L2 normalization function。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">XX^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span> 是 the homogeneous bilinear feature 齐次双线性特征。相比于 the max activation metric，bilinear pooling metric 中的 each channel group 表示 one specific visual pattern。同样，在训练阶段和推理阶段都不涉及可训练的参数。Figure 2(b) 展示了 the bilinear pooling metric 的过程。</p>\n</blockquote>\n<h2 id=\"5-conclusion-2\"><span class=\"post-title-index\">12.4. </span><a class=\"markdownIt-Anchor\" href=\"#5-conclusion-2\"></a> 5 Conclusion</h2>\n<blockquote>\n<p>本文引入了一种新颖的正则化技术，Channel DropBlock（CDB），该技术通过相关性度量对通道进行聚类，并在训练阶段随机丢弃一个相关通道组（a correlated channel group），从而破坏协同适配的特征通道（destructs feature channels from co-adaptations）。我们证明，与现有的 FGVC 方法相比，CDB 在增强特征表示和提取多种判别模式方面更加轻量级和有效。我们在三个经过广泛测试的细粒度数据集上进行了实验，验证了所提出方法的优越性。未来工作的两个特别有趣的方向包括探索具有自适应大小的通道分组方法，以及使用综合指标度量通道相关性。</p>\n</blockquote>\n</font></font></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"正则化\"><a class=\"markdownIt-Anchor\" href=\"#正则化\"></a> 正则化</h1>\n<blockquote>\n<p>过拟合 --&gt; 在训练集上表现很好，但在测试集上效果不佳。（随着模型复杂度增加，训练误差减小，测试误差不减）</p>\n<p>发生过拟合时，模型的偏差小而方差大。</p>\n</blockquote>\n<blockquote>\n<p>正则化 --&gt; 解决模型过拟合问题！</p>\n<p>正则化通过对学习算法进行微调，使得该模型具有更好的泛化能力，改善模型在未知数据上的表现。</p>\n</blockquote>\n<blockquote>\n<p><strong><a href=\"https://zh.m.wikipedia.org/zh-hans/%E9%81%8E%E9%81%A9\" title=\"什么是过拟合？\">过拟合</a></strong></p>\n<p><strong><font color = green>过拟合的本质是训练算法从统计噪声中获取了信息并表达在了模型结构的参数当中。</font></strong></p>\n<p>过拟合 --&gt; 指过于紧密或精确地匹配训练集数据，以致于无法良好地拟合测试集数据 --&gt; <strong><font color = green>过拟合一般可视为违反奥卡姆剃刀原理（简约法则，若无必要，勿增实体）</font></strong></p>\n<p><strong><font color = green>过拟合存在的原因 --&gt; 选择模型的标准和评价模型的标准不一致导致的。</font></strong> 选择模型时往往选取在训练数据上表现最好的模型；而评价模型时则是观察模型在不可见数据上的表现。当模型尝试“记住”训练数据而非从训练数据中学习规律时，就可能发生过拟合。</p>\n<p>在统计学系和机器学习中，为了避免或减轻过拟合，可使用以下技巧：</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 模型选择 Model Selection</font>  --&gt; 给定数据的情况下，从一组模型中选出最优模型（或具有代表性的模型）的过程。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 交叉验证 Cross-Validation</font>–&gt;  一种预测模型拟合性能的方法。包括 Leave-one-out Cross-Validation 和 K-fold Cross Validation。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  提前停止 Early Stopping</font>  --&gt; 当训练集上的 loss 不再减小（减小的程度小于某个阈值）时停止继续训练，即用于提前停止训练的回调函数callbacks。</p>\n<p><strong><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  正则化 Regularization</font></strong> --&gt;  机器学习和逆问题领域中，<strong><font color = green>正则化</font></strong> 是指为解决适定性问题或过拟合而加入额外信息的过程。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  剪枝 Pruning</font> --&gt;  机器学习和搜索算法中，通过移除决策树中分辨能力较弱的部分而减小决策树大小的方法，其降低了模型的复杂度，因此能够降低过拟合风险，从而降低泛化误差。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 贝叶斯信息量准则 Bayesian Information Criterion / Schwarz Information Criterion</font>  --&gt;  在有限集合中进行模型选择的准则。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  赤池信息量准则 Akaike Information Criterion</font> --&gt;  基于信息熵，用于评估统计模型的复杂度和衡量统计模型拟合资料的优良性的一种标准。</p>\n<p><font color = green><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>  dropout</font>  --&gt;  Hinton 提出的一种正则化方法，即在神经网络训练过程中，通过随机丢弃部分神经元，来减小神经元之间的协同适应性，从而降低网络过拟合风险。</p>\n</blockquote>\n<blockquote>\n<p><strong><a href=\"https://zhuanlan.zhihu.com/p/37120298\" title=\"正则化？\">深度学习中的正则化策略</a></strong></p>\n<p>正则化 --&gt; 深度学习中，正则化是惩罚每个节点的权重矩阵。</p>\n<p>用于深度学习的正则化技巧：</p>\n<p><font color = green> L1 &amp; L2 正则化 </font>  --&gt; 均是在损失函数 cost function 中增加一个正则项，即：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mi>a</mi><mi>y</mi><mo separator=\"true\">,</mo><mi>b</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>r</mi><msub><mi>y</mi><mrow><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi><mrow><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Cost function = Loss(say, binary_{cross entropy}) + Regularization_{term} \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.28055599999999997em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">n</span><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></p>\n</blockquote>\n<h1 id=\"regularization-for-deep-learning-a-taxonomy_2017\"><a class=\"markdownIt-Anchor\" href=\"#regularization-for-deep-learning-a-taxonomy_2017\"></a> Regularization For Deep Learning: A Taxonomy_2017</h1>\n<blockquote>\n<p>正则化的定义很多，作者提出一个系统的，统一的分类方法将现有的正则化方法进行分类，并为开发人员提供了实用的正则化方法的建议。</p>\n<p>作者将目前的正则化方法分类为 affect data 影响数据、network architectures 网络架构、error terms 错误项、regularization terms 正则化项、optimization procedures 优化过程 这几种方法。</p>\n<p>在<font color = green>传统</font>意义上的优化和<font color = green>较老</font>的神经网络文献中，<font color = green>正则化只用于损失函数中的惩罚项</font>。</p>\n<p>2016年 Goodfellow 等人 将正则化广泛定义为：<font color = green>为减少模型的测试误差，而非训练误差，对学习算法所作的任何修改。</font>即，正则化被定义为：</p>\n<p><strong><font color = green>任何使模型能够更好地泛化的辅助技术，即在测试集上产生更好效果的技术都被称为正则化。</font></strong>–&gt; 该定义更符合机器学习文献，而非逆问题文献。可包括<font color = green>损失函数的各种属性，损失优化算法或其他技术。</font></p>\n</blockquote>\n<blockquote>\n<p>为了为接下来提出的分类法的顶层提供一个证明，作者梳理了机器学习的理论框架。</p>\n<h2 id=\"理论框架\"><a class=\"markdownIt-Anchor\" href=\"#理论框架\"></a> 理论框架</h2>\n<p>机器学习的中心任务是 <font color = green> 模型拟合：找到一个函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span>，它能很好地近似从输入 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 到期望输出 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 的期望映射。</font></p>\n<p>很多应用中，<font color = green>神经网络已被证明是一个选择 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的很好的函数族。</font></p>\n<p>一个神经网络是一个具有可训练权值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi><mo>∈</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">w \\in W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>的函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub><mo>:</mo><mi>x</mi><mo>−</mo><mo>−</mo><mo>&gt;</mo><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">f_w : x --&gt; y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>。</p>\n<p><font color = green>训练网络意味着找到一个使损失函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span></span></span></span> 最小的权重配置 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>w</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">w^*</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.688696em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.688696em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mbin mtight\">∗</span></span></span></span></span></span></span></span></span></span></span> ：</p>\n<img src=1.png width=70% />\n<p>通常损失函数采用期望风险的形式：</p>\n<img src=2.png width=70% />\n<p>其中包含两部分：<font color = green>误差函数<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span>和正则化项<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span>。</font></p>\n<p><strong><font color = green>误差函数 --&gt; 依赖于目标，并根据其与目标的一致性对模型预测分配惩罚。</font></strong></p>\n<p><strong><font color = green>正则化项 --&gt; 根据其他标准对模型进行惩罚。这个标准可以是除了目标以外的任何东西，例如权重。</font></strong></p>\n<p>由于数据分布 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 是未知的，所以根据公式（2）期望风险不能直接降到最低。相反，给出了从分布中采样的训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>。<strong><font color = green>期望风险的最小化可以通过最小化经验风险 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi mathvariant=\"script\">L</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{L}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathcal\">L</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.11110999999999999em;\">^</span></span></span></span></span></span></span></span></span></span> 得到。</font></strong></p>\n<img src=3.png width=70% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_i, t_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 是来自训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span> 的样本。</p>\n<p>公式（3）给出了最小化经验风险，作者根据公式中的元素，将正则化方法分为以下几类：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>：训练集 --&gt; affect data 影响数据</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{f}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span>：选择的模型族 --&gt; network architectures 网络架构</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span></em>：错误函数 --&gt; error terms 错误项</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <em><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span></em>：正则化项 --&gt; regularization terms 正则化项</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span>   优化过程本身 --&gt; optimization procedures 优化过程</p>\n</blockquote>\n<h2 id=\"1-通过数据进行正则化\"><a class=\"markdownIt-Anchor\" href=\"#1-通过数据进行正则化\"></a> 1 通过数据进行正则化</h2>\n<blockquote>\n<p>训练模型的质量很大程度取决于训练数据。</p>\n<p><font color = green>通过对训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span> 应用一些变换 生成一个新的数据集，从而实现对数据的正则化。</font></p>\n<p>进行数据正则化可根据以下俩原则：</p>\n<p>1）进行特征提取或预处理，将特征空间或数据分布修改为某种表示，从而简化学习任务；</p>\n<p>2）允许生成新样本来创建更大的、可能是无限的增强数据集。</p>\n<p>这两个原则在某种程度上是独立的，也可相结合。它们均依赖于（随机）参数的转换：</p>\n<img src=D2.png width=70% />\n<p>作者给出第二个定义：</p>\n<p><strong><font color = green>带有随机参数的变换是一个函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>τ</mi><mi>θ</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.1132em;\">τ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.1132em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，其参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 遵循某种概率分布。</font></strong></p>\n<p>所以，在此情况下，考虑 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>τ</mi><mi>θ</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tau_{\\theta}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.1132em;\">τ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.1132em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 可作用于 <font color = green>网络输入、隐层激活或目标。</font>  输入被高斯噪声破坏<font color = green>（给输入数据添加高斯噪声）</font>是随机参数变换的一个例子。</p>\n<img src=4.png width=70% />\n<p><strong><font color = green>变换参数的随机性带来新样本的产生，即 data augmentation 数据增广。数据增广通常专门指输入变换或隐藏激活。</font></strong></p>\n<p><strong><font color = blue>作者根据变换的性质及其参数的分布对基于数据的正则化方法进行分类。</font></strong></p>\n<h3 id=\"变换参数-theta-的随机性\"><a class=\"markdownIt-Anchor\" href=\"#变换参数-theta-的随机性\"></a> 变换参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的随机性</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue>确定性参数</font></strong>：参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 遵循 delta 分布，数据集大小保持不变。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue>随机性参数</font></strong>：允许生成一个更大的，可能是无限的数据集。 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的采样方法多种多样，有：</p>\n<p>1）<strong><font color = blue>随机</font></strong>：从指定的分布中画一个随机的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></p>\n<p>2）<strong><font color = blue>自适应</font></strong>： <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>的值是一个优化过程的结果，通常目标是一个最大化变换样本上的网络误差（这种具有挑战性的样本被认为是当前训练阶段信息量最大的样本），或最小化网络预测和预定义的假目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">t&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> 之间的差异。</p>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color = blue>约束优化</font></strong>：通常在硬约束下最大化误差找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>（支持 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的分布控制最强的允许变换）；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color = blue>无约束优化</font></strong>：通过最大化修正误差函数找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>，使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的分布作为权重（为了完整性在此提出，但并未测试）；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong><font color = blue>随机</font></strong>：通过获取固定数量的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 样本并使用产生最高误差的样本来找到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>.</p>\n</blockquote>\n<h3 id=\"对数据表示的影响\"><a class=\"markdownIt-Anchor\" href=\"#对数据表示的影响\"></a> 对数据表示的影响</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = green>保留表示的转换 Representation-preserving transformations</font></strong>：保留特征空间并尝试保留数据分布。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = green>保留修改的转换 Representation-modifying transformations</font></strong>：将数据映射到不同的表示（不同的分布甚至新的特征空间），这可能会解开原始表示的潜在因素并使学习问题更容易。</p>\n<h3 id=\"转换空间\"><a class=\"markdownIt-Anchor\" href=\"#转换空间\"></a> 转换空间</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue>输入</font></strong>：对输入 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span> 进行变换；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue>隐藏特征空间</font></strong>：对样本的一些深层表示进行变换（这也使用部分 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 将输入映射到隐藏特征空间；这种变换在网络 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 内部起作用，因此可被认为是架构）</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue>目标</font></strong>：转换应用于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span>（只能在训练阶段使用，因为标签在测试时没有显示给模型）</p>\n<h3 id=\"普遍性\"><a class=\"markdownIt-Anchor\" href=\"#普遍性\"></a> 普遍性</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = green>通用 Generic</font></strong> ：适用于所有数据域；</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = green>特定域 Domain-specific</font></strong>：针对当前问题的特定（手工制作），例如图像旋转。</p>\n<h3 id=\"theta-分布的依赖关系\"><a class=\"markdownIt-Anchor\" href=\"#theta-分布的依赖关系\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布的依赖关系</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></font></strong>：所有样本的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布相同</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span></span></span></span></font></strong>：不同目标（类别）的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 分布可能不同</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t&#x27;)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|\\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|time)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">e</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|\\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span></font></strong>：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> 以上方法的综合</font></strong>：即 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>π</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, \\pi)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><msup><mi>t</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t&#x27;)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|t, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>t</mi><mo separator=\"true\">,</mo><mi mathvariant=\"script\">D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta|x, t, \\mathcal{D})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span><span class=\"mclose\">)</span></span></span></span></p>\n<h3 id=\"阶段\"><a class=\"markdownIt-Anchor\" href=\"#阶段\"></a> 阶段</h3>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = green> 训练</font></strong>：训练样本的转换。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> <strong><font color = blue> 测试</font></strong>：测试样本的转换，例如对样本的多个增强变体进行分类，并将结果汇总在它们之上。</p>\n<p><strong><font color = green> 表1回顾了使用通用转换的现有方法</font></strong>：</p>\n<img src=t1.png width=100% />\n<p><strong><font color = blue> 表2列出了特定域的方法</font></strong>，特别侧重于图像领域。最常用的方法是：图像的刚性变形和弹性变形。</p>\n<img src=t2.png width=100% />\n<h3 id=\"目标保留数据增广\"><a class=\"markdownIt-Anchor\" href=\"#目标保留数据增广\"></a> 目标保留数据增广</h3>\n<p>目标保留数据增广 --&gt; 在输入和隐藏特征空间中使用随机变换，同时保持原始目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span></p>\n<p><font color = red> 未完待续！！！</font></p>\n<h3 id=\"基于数据的正则化方法的总结\"><a class=\"markdownIt-Anchor\" href=\"#基于数据的正则化方法的总结\"></a> 基于数据的正则化方法的总结</h3>\n<p>作者对基于数据的正则化方法进行了形式化，展示了**<font color = green>看似与数据正则化无关的技术，例如保留目标的数据增广、dropout 或 Batch Normalization 等技术在方法上惊人的近似，都可看做是基于数据的正则化方法。</font>**</p>\n</blockquote>\n<h2 id=\"2-通过网络架构进行正则化\"><a class=\"markdownIt-Anchor\" href=\"#2-通过网络架构进行正则化\"></a> 2 通过网络架构进行正则化</h2>\n<blockquote>\n<p>为了实现正则化效果，可以选择具有特定属性或匹配特定假设的网络架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span>.</p>\n<h3 id=\"关于映射的假设\"><a class=\"markdownIt-Anchor\" href=\"#关于映射的假设\"></a> 关于映射的假设</h3>\n<blockquote>\n<p><font color =green>为了很好地拟合数据 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span>，输入-输出 的映射 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>f</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 必须具有某些属性。</font>尽管执行理想映射的精确属性可能很难，但可通过关于映射的简化假设来近似它们。<font color = green>这些属性和假设可以<strong>以硬或软</strong>的方式强加于模型拟合。</font>这限制了模型的搜索空间，并允许找到更好的解决方案。</p>\n<p>**<font color = green> 作者讨论的对 输入-输出 映射施加假设的方法是网络架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的选择。</font>**一方面，架构 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的选择 <strong><font color = green>硬连接</font><strong>了映射的某些属性；此外，在 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 和优化算法之间的相互作用中，某些权重配置比其他配置更可能通过优化获得，从而进一步</strong><font color = green>以软方式限制可能的搜索空间</font></strong>。</p>\n<p><font color = green>对映射施加某些假设的补充方法是<strong>正则化项</strong>，以及**（增广）数据集中存在的不变性**。</font></p>\n<p>假设可以 <strong><font color = green>硬连接</font></strong> 到某些层执行的操作的定义中，和 / 或层之间的连接中。</p>\n<p>**<font color = green>基于网络架构的方法如表三</font>**所示：</p>\n<img src=t3_1.png width=100% />\n<img src=t3_2.png width=100% />\n<p>在隐藏特征空间中对数据进行变换的正则化方法可被视为体系结构的一部分。也就是说，<font color =green>在隐藏特征空间中对数据进行变换的正则化方法既属于数据正则化，也属于网络架构正则化。</font></p>\n</blockquote>\n<h3 id=\"blacksquare-权值共享-weight-sharing\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-权值共享-weight-sharing\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 权值共享 Weight sharing</h3>\n<blockquote>\n<p>权值共享 --&gt; 在网络的多个部分重复使用某个可训练参数。例如，卷积网络中的**<font color = blue>权值共享不仅减少了需要学习的权重的数量，它还编码了 shift-equivariance 的先验知识和特征提取的局部性。</font>**</p>\n</blockquote>\n<h3 id=\"blacksquare-激活函数-activation-function\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-激活函数-activation-function\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 激活函数 Activation function</h3>\n<blockquote>\n<p>选择正确的激活函数非常重要。例如：</p>\n<p>1）<strong>ReLUs ** 在训练时间和准确性方面提高了许多深度架构的性能。</strong><font color = green>ReLUs 的成功既可归因于：ReLUs 可避免梯度消失问题；也可归因于：它们提供了更有表现力的映射家族 more expressive families of mappings</font>**。</p>\n<p><strong><font color = blue> 一些激活函数是专门为正则化设计的。</font></strong></p>\n<p>2）<strong>Dropout</strong> ，<strong>Maxout</strong> 单元允许在测试时更精确地逼近模型集合预测的几何平均值。</p>\n<p>3）<strong>Stochastic pooling 随机池化</strong> 是最大池化的噪音版本。作者声称，这允许对激活的分布进行建模，而不仅是取最大值。</p>\n</blockquote>\n<h3 id=\"blacksquare-噪声模型-noisy-models\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-噪声模型-noisy-models\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 噪声模型 Noisy models</h3>\n<blockquote>\n<p><strong>Stochastic pooling</strong> 随机池化是确定性模型的随机泛化的一个例子。<font color = green>有些模型是通过向模型的各个部分注入随机噪声来实现的。 Dropout 是最常用的噪声模型</font></p>\n</blockquote>\n<h3 id=\"blacksquare-多任务学习-multi-task-learning\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-多任务学习-multi-task-learning\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 多任务学习 Multi-task learning</h3>\n<blockquote>\n<p>**多任务学习  --&gt; 是一种特殊类型的正则化。**它可与半监督学习相结合，在辅助任务上利用未标记数据。</p>\n<p><strong>元学习</strong>中也使用了任务之间共享知识的类似概念，其中来自同一领域的多个任务被顺序学习，使用先前获得的知识作为新任务的偏差。</p>\n<p><strong>迁移学习</strong>，将一个领域的只是迁移到另一个领域。</p>\n</blockquote>\n<h3 id=\"blacksquare-模型选择-model-selection\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-模型选择-model-selection\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 模型选择 Model selection</h3>\n<blockquote>\n<p>可通过评估验证集上的预测来选择几个经过训练的模型（例如，具有不同的架构）中最好的模型。</p>\n</blockquote>\n</blockquote>\n<h2 id=\"3-通过误差函数进行正则化\"><a class=\"markdownIt-Anchor\" href=\"#3-通过误差函数进行正则化\"></a> 3 通过误差函数进行正则化</h2>\n<blockquote>\n<p>**<font color = green>理想情况下，误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span>（表示输出与目标的一致性） 反映了适当的质量概念，在某些情况下还反映了一些关于数据分布的假设。</font>**典型的例子是：<strong>均方误差</strong> 或 <strong>交叉熵</strong>。</p>\n<p>**<font color = blue> 误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 也可以具有正则化效果。</font>**例如，Dice coefficient optimization 系数优化，它对类别不平衡具有鲁棒性。</p>\n</blockquote>\n<h2 id=\"4-通过正则化项进行正则化\"><a class=\"markdownIt-Anchor\" href=\"#4-通过正则化项进行正则化\"></a> 4 通过正则化项进行正则化</h2>\n<blockquote>\n<p><strong><font color = green>正则化可以通过在损失函数中添加正则化器 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 来实现。</font><strong>与误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> （表示输出与目标的一致性）不同，</strong><font color = green>正则化项独立于目标。</font><strong>相反，</strong><font color = blue>正则化项用于编码所需模型的其他属性，以提供归纳偏差（即关于映射的假设，而不是输出与目标的一致性）。</font></strong> 因此，<strong><font color = green>对于未标记的测试样本，正则化项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 的值能计算出来，而误差函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span></span></span></span> 不能计算。</font></strong></p>\n<p><strong>正则化项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span> 与目标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span> 的独立性有一个重要含义：它允许额外使用未标记的样本（半监督学习），根据其符合一些期望的属性来改进学习模型。</strong></p>\n<blockquote>\n<p><strong><font color = green> 一个经典的正则化方法是 weight decay 权值衰减</strong></p>\n<img src=5.png width=70% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">λ</span></span></span></span> 是一个加权项，用于控制正则化对一致性的重要性。</p>\n<p><strong>从贝叶斯的角度来看</strong> ，权重衰减对应于使用对称的多元正态分布作为权重的先验：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mi mathvariant=\"normal\">∣</mi><mn>0</mn><mo separator=\"true\">,</mo><msup><mi>λ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>I</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(w) = \\mathcal{N}(w|0,\\lambda^{-1}I) \n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord\">∣</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">λ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<img src=e1.png width=70% />\n<p><strong>图4 回顾了现有的通过正则化项进行正则化的方法。权重衰减（L2正则化）似乎仍然是最流行的正则化项。</strong></p>\n<p><strong><font color = green>L2 正则化的目的就是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题，所以权重衰减也叫 L2 正则化。</font></strong></p>\n<img src=t4_1.png width=100% />\n<img src=t4_2.png width=100% />\n<img src=t4_3.png width=100% />\n</blockquote>\n</blockquote>\n<h2 id=\"5-通过优化进行正则化\"><a class=\"markdownIt-Anchor\" href=\"#5-通过优化进行正则化\"></a> 5 通过优化进行正则化</h2>\n<blockquote>\n<p>**<font color = green>随机梯度下降（SGD）及其衍生</font> 是深度神经网络中最常用的优化算法，也是我们关注的中心。**作者也在下文列出了一些替代方法。</p>\n<p><strong><font color = green>随机梯度下降法（SGD）</font> 是一种采用以下更新规则的迭代优化算法 ：</strong></p>\n<img src=7.png width=70% />\n<p>如果算法在合理的时间内达到较低的训练误差（与训练集的大小呈线性关系，允许多次通过训练集 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">D</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>，那么在某些温和的假设下，解决方案的泛化效果很好，从这个意义上来说：</p>\n<p><strong><font color = green> SGD 作为一个隐性的正则化器：即使没有使用任何额外的正则化器，较短的训练时间也能防止过拟合。</font></strong> --&gt; 这与论文《Understanding deep learning requires rethinking generalization  》中的观点一致：该论文作者在一系列实验中发现，<strong><font color = blue>正则化（例如 Dropout、数据增广和权重衰减）本身既不是良好泛化的必要条件，也不是充分条件。</font></strong></p>\n<p>作者将通过优化进行正则化的方法分为三组：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 初始化 /热启动方法</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 更新方法</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 终止方法</p>\n<h3 id=\"blacksquare-initialization-and-warm-start-methods-初始化热启动方法\"><a class=\"markdownIt-Anchor\" href=\"#blacksquare-initialization-and-warm-start-methods-初始化热启动方法\"></a> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> Initialization and warm-start methods 初始化/热启动方法</h3>\n<blockquote></blockquote>\n</blockquote>\n<h2 id=\"建议-讨论-结论\"><a class=\"markdownIt-Anchor\" href=\"#建议-讨论-结论\"></a> 建议、讨论、结论</h2>\n<blockquote>\n<blockquote>\n<h3 id=\"1-该分类法的优势\"><a class=\"markdownIt-Anchor\" href=\"#1-该分类法的优势\"></a> 1 该分类法的优势：</h3>\n<p>作者认为<font color = green>这样的分类法的优势</font>有两个方面：</p>\n<p>1）它为正则化方法的用户提供了现有技术的概述，并让他们更好地了解如何为他们的问题选择理想的正则化技术组合。</p>\n<p>2）它对于开发新方法很有用，因为它全面概述了可用于正则化模型的主要原则。</p>\n</blockquote>\n<blockquote>\n<h3 id=\"2-作者建议\"><a class=\"markdownIt-Anchor\" href=\"#2-作者建议\"></a> 2 作者建议：</h3>\n<h4 id=\"1-font-color-green对现有正则化方法用户的建议font\"><a class=\"markdownIt-Anchor\" href=\"#1-font-color-green对现有正则化方法用户的建议font\"></a> 1. <strong><font color = green>对现有正则化方法用户的建议</font></strong></h4>\n<p>总的来说，<font color = green>尽可能多地使用数据中包含的信息以及先验知识，并主要从流行的方法开始</font>，以下程序可能是有帮助的:</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 对于第一步的常见建议：</p>\n<p>1）<font color = green>深度学习就是要把变异的因素分解开来。</font>应该选择一个合适的数据表示；<strong>已知的有意义的数据转换不应该外包给学习。</strong> 在几种表征中，<strong>冗余地提供相同的信息是可以的。</strong></p>\n<p>2）<font color = green>输出非线性和误差函数应该反应学习目标。</font></p>\n<p>3）一个好的起点是通常工作良好的技术（例如，ReLU，成功的架构）。<strong>超参数（和架构）可以联合调优，但是很缓慢</strong>（根据经验进行插值 / 推断，而不是尝试太多的组合）。</p>\n<p>4）通常，<font color = green> 从一个简化的数据集（例如，更少和/或更简单的样本）和一个简单的网络开始是有帮助的，</font> 在获得有希望的结果后，<font color = green>在调优超参数和尝试正则化方法时逐渐增加数据和网络的复杂性。</font></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 通过数据进行正则化：</p>\n<p>1）当不处理几乎无限 / 丰富的数据时：</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> 如果可能的话，收集更多的真实数据（并使用考虑到其属性的方法）是可取的：</p>\n<ul>\n<li><strong>有标记的样本</strong>是最好的，但无标记的样本也可能有用（兼容半监督学习）。</li>\n<li><strong>来自相同领域的样本</strong>是最好的，但来自相似领域的样本也会有帮助（兼容领域适应和迁移学习）。</li>\n<li><strong>可靠的高质量样本</strong>是最好的，但低质量样本也有帮助（它们的信心 / 重要性可以相应地调整）。</li>\n<li><strong>给额外的任务贴上标签</strong>会很有帮助（与多任务学习兼容）。</li>\n<li>**额外的输入特性（来自额外的信息源）和 / 或数据预处理（即特定于领域的数据转换）**可能会有所帮助（网络架构需要相应的调整）。</li>\n</ul>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> <strong>数据增广</strong>（例如，保留目标的手工特定领域转换）可以很好地弥补有限的数据。如果一直增强数据的自然方法（充分模拟自然转换），则可以尝试（并组合）它们。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⋆</mo></mrow><annotation encoding=\"application/x-tex\">\\star</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.46528em;vertical-align:0em;\"></span><span class=\"mord\">⋆</span></span></span></span> 如果增广数据的自然方法未知或被证明是不充分的，如果有足够的数据可用，就有可能从数据中推断出转换（例如学习图像变形字段）。</p>\n<p>2） 流行的泛型方法（例如 Dropout 的高级变体）通常也有帮助。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 架构和正则化项：</p>\n<p>1）关于映射的可能的有意义的属性的知识可以被用来如将不变性（对某些转换）硬连接到架构中，或者被表述为正则化项。</p>\n<p>2）流行的方法也可能有帮助（见表3和表4），但应该选择匹配映射的假设（例如，仅当需要对常规网格数据进行局部和移位等变特征提取时，卷积层才完全合适）。</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 优化：</p>\n<p>1）初始化：尽管预训练的现成模型大大加快了原型的制作速度，但良好的随机初始化也应该被考虑。</p>\n<p>2）优化器：尝试一些不同的方法，包括先进的（例如 Nesterov momentum, Adam, ProxProm），可能会带来更好的结果。正确选择的参数，例如学习率，通常会产生很大的不同。</p>\n<h4 id=\"2-font-color-green对新正则化方法的开发人员的建议font\"><a class=\"markdownIt-Anchor\" href=\"#2-font-color-green对新正则化方法的开发人员的建议font\"></a> 2. <font color = green>对新正则化方法的开发人员的建议</font></h4>\n<p>了解最佳方法成功的原因是一个很好的基础。有希望的空白领域（分类法属性的某些组合）是可以解决的。强加在模型上的假设可能会对分类法的大多数元素产生强烈的影响。<font color = green> <strong>数据增广比损失项更有表现力</strong>（损失项只在训练样本的无限小的邻域强制属性；数据增广可以使用丰富的转换参数分布）。</font>数据和损失项以相当软的方式强加假设和不变性，并且可以调整它们的影响，而硬连接网络架构是强加假设的更苛刻的方式。施加它们的不同假设和选项具有不同的优点和缺点。</p>\n<h4 id=\"3-font-color-green-基于数据方法的未来方向font\"><a class=\"markdownIt-Anchor\" href=\"#3-font-color-green-基于数据方法的未来方向font\"></a> 3. <font color = green> 基于数据方法的未来方向</font></h4>\n<p>作者认为以下几个有前景的方向值得研究：</p>\n<p>1） <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>  的自适应采样可能会导致更低的误差和更短的训练时间（反过来，更短的训练时间可能会额外起到隐式正则化的作用）。</p>\n<p>2）作者认为学习类依赖变换会导致更可信的样本。</p>\n<p>3）在最近引发了关于真实世界对抗示例及其对摄像机位置变化等变换的鲁棒性 / 不变性的讨论后，对抗示例（以及对它们的网络鲁棒性）领域正获得越来越多的关注。对抗强烈的对抗性例子可能需要更好的正则化技术。</p>\n<h4 id=\"4-总结\"><a class=\"markdownIt-Anchor\" href=\"#4-总结\"></a> 4. 总结</h4>\n<p>在这项工作中，<font color = green>作者为深度学习提供了一个广义的的正则化定义，确定了<strong>神经网络训练的五个主要元素（数据，架构，错误项，正则化项，优化程序）</strong>，通过每个元素描述了正则化，包括对每个元素的进一步、更精细的分类，并从这些子类别中提供了示例方法。</font> 我们没有试图详细解释引用的作品，而只是确定它们与我们的分类相关的属性。我们的工作证明了现有方法之间的一些联系。此外，我们的系统方法通过结合现有方法的最佳特性，能够发现新的、改进的正则化方法。</p>\n</blockquote>\n</blockquote>\n<hr />\n<hr />\n<h1 id=\"heuristic-dropout-an-efficient-regularization-method-for-medical-image-segmentation-models_2022tsinghua-university\"><a class=\"markdownIt-Anchor\" href=\"#heuristic-dropout-an-efficient-regularization-method-for-medical-image-segmentation-models_2022tsinghua-university\"></a> Heuristic Dropout: An Efficient Regularization Method For Medical Image Segmentation Models_2022,Tsinghua University</h1>\n<h2 id=\"abstract\"><a class=\"markdownIt-Anchor\" href=\"#abstract\"></a> Abstract</h2>\n<blockquote>\n<p>对于真实场景中的医学图像分割，像素级的准确标注数据量通常较少，容易造成过拟合问题。这篇手稿深入研究了 Dropout 算法，该算法常用于神经网络以缓解过拟合问题。这篇手稿<strong>从解决 co-adaptation problem 协同适应问题的角度</strong>出发，解释了 <font color = green>Dropout 算法</font>的基本原理，并讨论了<font color =green>其衍生方法存在的局限性</font>。此外我们提出一种新颖的<strong>Heuristic Dropout启发式 Dropout 算法来解决这些局限</strong>。<strong><font color = green>该算法以信息熵和方差作为启发式规则。</font></strong> 它指导我们的算法更有效地丢弃遭受协同适应问题的特征，从而更好地缓解小规模医学图像分割数据集的过拟合问题。医学图像分割数据集和模型的实验表明，所提出的算法显著提高了这些模型的性能。</p>\n</blockquote>\n<h2 id=\"intex-terms\"><a class=\"markdownIt-Anchor\" href=\"#intex-terms\"></a> Intex Terms</h2>\n<blockquote>\n<p>医学图像分割，过拟合问题， Dropout 算法，信息熵</p>\n</blockquote>\n<h2 id=\"1-introduction\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction\"></a> 1. Introduction</h2>\n<blockquote>\n<p><strong>医学图像分割</strong>是当前<strong>计算机辅助医学诊断（Computer-aided Medical Diagnosis, CAD）系统</strong>的重要组成部分，其准确性直接影响 CAD 系统的性能。近年来，CAD 系统越来越多地参与到实际的医疗诊断任务中。因此，提高医学图像分割模型的准确性和可靠性具有重要的意义和应用价值。</p>\n<p>在医学图像分割领域， <font color = green>U-Net，nnU-Net，TransUNet 等深度学习模型</font>已经在各种任务中表现出了比传统方法更好的性能。与<strong>自然图像分割</strong>相比，**<font color = green>医学图像分割的数据标定高度依赖于专家知识，需要像素级的准确标定。因此，在专家指导下，像素级的准确标定数据量通常很小。</font>**小尺度的数据集容易出、造成过拟合问题，特别是当分割模型参数量较大时。</p>\n<p>解决过拟合问题的方法有很多， Dropout 算法是其中一种简单而有效的方法。它在训练过程中以一定的概率随机丢弃模型中的神经元，缓解了协同适应问题，从而缓解了深度学习模型的过拟合问题。<strong><font color = green>Co-adaptation 协同适应是指每个神经元学习到的特征通常必须与上下文（即其他特定神经元）相结合的现象，以在训练过程中提供有用的信息。</font></strong> 然而，<strong>从小规模医学图像分割数据集中学习到的这种经验依赖是脆弱的，在面对测试集的分布时可能不可信。</strong> 因此，<font color = green>神经元之间过多的依赖关系往往会引发过拟合问题。</font> Dropout 算法中的 drop 操作减少了深度学习模型中神经元之间的依赖关系，防止了一些神经元过度依赖其他神经元，从而在一定程度上避免了过拟合问题。</p>\n<p><strong><font color = green>根据 drop 过程是否完全随机，Dropout 算法的衍生方法可以分为两类。</font></strong> <strong><font color = blue>第一类是完全随机的方法</font></strong>，例如 <strong><font color = blue>Spatial Dropout </font></strong> 随机丢弃通道维度中的单元，<strong><font color = blue>DropBlock </font></strong> 将 2d blocks 视为单元并随机丢弃它们，<strong><font color = blue>Stochastic Depth</font></strong> 随机丢弃残差连接。<strong><font color = purple>第二类是基于规则的方法 </font>，</strong> 例如  <strong><font color = purple>Weighted Channel Dropout</font></strong> 以通道的激活值作为指导规则，<strong><font color = purple>Focused Dropout</font></strong> 以 2d blocks 的激活值作为指导规则。然而，这两类现有方法都不是没有局限的。<strong><font color = blue>第一类，完全随机的方法，缺乏指导规则，因此可能效率低下，丢弃的特征不一定是遭受协同适应问题的特征</font></strong>，<strong><font color = purple>在第二类，基于规则的方法中，现有的指导规则不够准确，丢弃遭受协同适应问题的精确特征的效率仍有提升空间</font></strong> 。因此，<strong><font color = green>本手稿提出了一种结合信息熵和方差的新的指导规则</font></strong>。在此规则的指导下，进一步提高了所提出的算法丢弃遭受协同适应问题的特征的效率。在多个医学图像分割数据集和模型上的实验表明，该算法显著提高了模型精度。</p>\n</blockquote>\n<h2 id=\"2-methodology\"><a class=\"markdownIt-Anchor\" href=\"#2-methodology\"></a> 2. Methodology</h2>\n<blockquote>\n<p>作者提出了一种新颖的启发式 Dropout 算法，<strong><font color = green>使用信息熵和方差作为指导规则来执行 Dropout 操作</font></strong>。该算法能够有效地丢弃遭受协同适应影响的特征，从而在很大程度上缓解了医学图像分割任务中的过拟合问题。</p>\n</blockquote>\n<h3 id=\"21-heuristic-metric\"><a class=\"markdownIt-Anchor\" href=\"#21-heuristic-metric\"></a> 2.1. Heuristic Metric</h3>\n<blockquote>\n<p>为了有效地丢弃协同适应问题较严重的特征，作者采用信息熵作为启发式规则。<strong><font color = green>信息熵可以衡量一个分布的不确定性。</font></strong></p>\n<img src=h1.png width=35% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 是一个随机变量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 是概率密度函数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">H(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> 是关于随机变量 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 的信息熵。</p>\n<img src=dropout_f7.png width=70% />\n<p>如 Dropout 一文中的图7所示，<strong><font color = green>遭受严重协同适应的特征具有不确定的视觉意义，因此具有较高的信息熵值，而遭受轻微协同适应的特征具有确定的视觉意义</font></strong>，例如，看起来像目标的点、边缘或几何轮廓，这些特征的信息熵值较低。<strong><font color = purple>以信息熵为指导原则，我们以更高的概率丢弃遭受严重协同适应问题的特征。</font><strong>此外，我们还需要</strong><font color = purple>方差作为另一个启发式规则</font></strong>。考虑 <strong><font color = blue>一个极端的情况，当分布接近于常量分布时，已知信息熵将接近最小值。</font><strong>然而，</strong><font color = purple>具有常量分布 constant distribution 的特征对训练几乎不提供什么有用的信息。因此，将方差作为另一个启发式规则，我们以更大的概率丢弃更接近常量分布的特征</font></strong>。</p>\n</blockquote>\n<h3 id=\"22-heuristic-dropout-algorithm\"><a class=\"markdownIt-Anchor\" href=\"#22-heuristic-dropout-algorithm\"></a> 2.2. Heuristic Dropout Algorithm</h3>\n<blockquote>\n<p>结合信息熵和方差两种启发式规则，得到算法1。</p>\n<img src=heuristic_a1.png width=70% />\n<p>我们计算输入特征图 input feature maps 的每个通道的信息熵 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">e_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和方差 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">v_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>。我们使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>+</mo><mfrac><mi>k</mi><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">e_i + \\frac{k}{v_i + \\epsilon}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.325208em;vertical-align:-0.44509999999999994em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3280857142857143em;\"><span style=\"top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mathdefault mtight\">ϵ</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.44509999999999994em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> 作为指导规则。因为 feature maps 的值是连续分布的，所以我们首先要对值进行量化，然后根据直方图计算信息熵，如算法2所示。还发现使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3 \\times 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span>的 Laplace 滤波器代替 all-zero 滤波器作为 drop mask 将为模型性能带来一点提升。</p>\n<img src=heuristic_a2.png width=70% />\n<p>我们的算法可无缝插入到各种模型中。以  U-Net 为例，我们在 U-Net 的编码器和解码器的每个阶段的两个连续卷积层之间插入所提出的算法，即：在前一个卷积层的激活函数之后，正好在下一个卷积层之前。</p>\n</blockquote>\n<h2 id=\"3-results\"><a class=\"markdownIt-Anchor\" href=\"#3-results\"></a> 3. Results</h2>\n<h3 id=\"31-datasets\"><a class=\"markdownIt-Anchor\" href=\"#31-datasets\"></a> 3.1. Datasets</h3>\n<blockquote>\n<p>作者在 Pancreas-CT 数据集和 BAGLS 数据集上进行实验。**考虑到在实际应用环境中，由经验丰富的专家标记的训练样本一般很少，我们专门从这些数据集中随机抽取一个子集进行试验。**对于 Pancreas-CT 数据集，我们随机选择12个扫描，然后将其转换为2545个 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>512</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">512 \\times 512</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span> 的 2D 切片，以方便训练模型。在这 12幅 3D CT 扫描中，我们随机选择 8幅 作为训练集，2幅 作为验证集，2幅 作为测试集。对于 BAGLS 数据集，我们随机选择 3000个 切片作为训练集，而验证集和测试集的大小保持与原始设置相同。</p>\n</blockquote>\n<h3 id=\"32-evaluation-metrics\"><a class=\"markdownIt-Anchor\" href=\"#32-evaluation-metrics\"></a> 3.2. Evaluation Metrics</h3>\n<blockquote>\n<p>为了对实验结果进行定量分析，我们采用了医学图像分割领域中广泛使用的 <strong><font color = green>DICE 值</font></strong> 和 **<font color = green>IoU 值</font>**作为评价指标。</p>\n<img src=heuristic_e1.png width=50% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 表示模型输出的掩膜 mask，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> 表示输入图像对应的真值 ground truth.</p>\n</blockquote>\n<h3 id=\"33-experimental-settings\"><a class=\"markdownIt-Anchor\" href=\"#33-experimental-settings\"></a> 3.3. Experimental Settings</h3>\n<blockquote>\n<p>我们使用 <strong>Adam optimizer</strong> 来训练所有的模型，学习率为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1 \\times 10^{-3}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">3</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_1 = 0.9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_2 = 0.999</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\">9</span><span class=\"mord\">9</span></span></span></span>， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\epsilon = 10^{-8}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">8</span></span></span></span></span></span></span></span></span></span></span></span>。batch size 的大小设置为可以在 GeForce RTX 2080 Ti 上以混合精度执行的最大值。我们使用 CrossEntropy 并在 Pancreas-CT 数据集上训练100个 epoch。我们使用结合 CrossEntropy, DiceLoss 和 SSIMLoss 的混合损失函数，并在 BAGLS 数据集上训练 30个epoch。我们在训练数据集上使用标准数据增广。不对模型的输出结果进行后处理。我们独立重复所有对比试验5次并报告平均结果。</p>\n</blockquote>\n<h3 id=\"34-comparison-with-dropout-derivative-methods\"><a class=\"markdownIt-Anchor\" href=\"#34-comparison-with-dropout-derivative-methods\"></a> 3.4. Comparison with Dropout Derivative Methods</h3>\n<blockquote>\n<p>为了验证该算法的有效性，我们在 Pancreas-CT 数据集和 BAGLS 数据集上进行了实验。我们将我们的算法和其它 Dropout 的衍生算法加入到几个模型中。图1 为实验结果的箱线图 box plots，表1为定量和整体对比。</p>\n<img src=heuristic_f1.png width=60% />\n<img src=heuristic_t1.png width=80% />\n<p>试验结果表明，该算法在 Pancreas-CT 和 BAGLS 两个数据集上的性能都优于其他 Dropout 衍生方法。在 Pancreas-CT 数据集上，我们的算法对 U-Net 和 Attention U-Net 的 DICE 值分别提高了 3.67 和 3.37。在 BAGLS 数据集上，我们的算法对 U-Net 和 UNet3+ 的 IoU 值分别提高了 2.97 和1.12。该算法可以更加有效地提高医学图像分割模型的性能。</p>\n</blockquote>\n<h3 id=\"35-comparison-study-on-hyperparameter-k\"><a class=\"markdownIt-Anchor\" href=\"#35-comparison-study-on-hyperparameter-k\"></a> 3.5. Comparison Study on Hyperparameter <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span></h3>\n<blockquote>\n<p>基于 U-Net 和 Pancreas-CT 数据集，作者研究了超参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的影响。随着 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的增加，性能呈现先增加后衰减的趋势，当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 3 时，性能最好。此外，从方框的方差（the variance of the box）可以看出，当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 2 时，模型性能比 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 为 3 时更稳定和可预测。</p>\n<img src=heuristic_f3.png width=60% />\n</blockquote>\n<h3 id=\"36-verify-effectiveness-of-alleviating-co-adaptation\"><a class=\"markdownIt-Anchor\" href=\"#36-verify-effectiveness-of-alleviating-co-adaptation\"></a> 3.6. Verify Effectiveness of Alleviating Co-adaptation</h3>\n<blockquote>\n<p>为了验证我们的算法比传统的 Dropout 更有效地缓解协同适应，我们在 Pancreas-CT 数据集上随机隐蔽了 U-Net 最终输出层之前的一定比例的中间特征图（intermediate feature maps）。<strong><font color = green>对于协同适应较少的模型，由于特征之间的依赖关系较少，掩蔽特征（masked features ）导致的性能下降应该更小。</font></strong> 如图 4 所示。我们的算法在隐蔽后的性能下降明显小于传统的 dropout。实验结果表明，<strong><font color = blue>使用我们的算法可以学习到更多独立特征和更少的依赖关系，因此我们的算法可以比传统的 Dropout 算法更大程度地缓解协同适应。</font></strong></p>\n<img src=heuristic_f4.png width=60% />\n</blockquote>\n<h3 id=\"37-visualization-of-segmentation-results\"><a class=\"markdownIt-Anchor\" href=\"#37-visualization-of-segmentation-results\"></a> 3.7. Visualization of Segmentation Results</h3>\n<blockquote>\n<img src=heuristic_f2.png width=100% />\n<p>图 2 演示了定性分析的可视化。从上到下显示三个切片的分割结果。可视化图表明，我们的算法能更准确地分割模型。</p>\n</blockquote>\n<h2 id=\"4-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#4-conclusion\"></a> 4. Conclusion</h2>\n<blockquote>\n<p>作者提出了一种新的启发式 Dropout 算法来解决小规模医学图像分割数据集的过拟合问题。该算法以信息熵和方差作为启发式规则，更有效地缓解了协同适应现象，从而更好地缓解了过拟合问题。在多个数据集和模型上的实验表明，该算法具有较好的性能。此外，我们将在未来的工作中研究我们的算法与自然图像的兼容性。</p>\n</blockquote>\n<ol>\n<li>\n<blockquote>\n<p><strong><font color = green>启发式</font></strong>：类似于 **<font color = green> 灵感</font>**一类的东西，可以快速进行判断，不需要逻辑性的思考论证。启发式往往可以让人们 <strong><font color = green>跳出当前思维的局限</font></strong>，但因为缺乏科学依据与缜密的逻辑验证，所以有时也会出错。</p>\n</blockquote>\n</li>\n<li>\n<p><strong><font color = green>医学图像与自然图像的区别：</font></strong></p>\n<blockquote>\n<p>1）医学图像大多数时放射成像，功能性成像，磁共振成像，超声成像这几种方式，而自然图像大多数是自然光成像。自然成像中，光谱比较复杂，有散射的成分，波普宽度比较大，但放射成像例如 DR, CT等，各厂家需要去除人体内的散射，使光谱单一，所以，这导致了一个重要区别，也就是：</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p><strong><font color = green>在自然图像中，噪声分布绝大多数情况下可认为是均匀的，可近似为高斯噪声</font></strong>，因为直射和散射光造成光场分布可认为是均匀的；</p>\n<p>但**<font color = blue>在医学图像中，由于光源单一再加上探测手段，人体厚度的影响往往会导致噪声分布不均匀，往往认为是一种泊松噪声</font>**。</p>\n<p>所以，针对医学图像的算法直接应用于自然图像效果可能不行。</p>\n<p>2）医学图像多是单通道灰度图像，尽管大量医学图像是3D的，但医学图像中没有景深的概念。</p>\n<p>3）同体态的医学图像相似度非常高，<strong>医学图像中的细微结构并不能像自然图像中那样认为是无关紧要的</strong>，在相似度极高的背景组织中的细微变化有可能代表着某种病变。</p>\n</blockquote>\n<ol start=\"3\">\n<li>\n<p><strong><font color = green>Co-adaptation 协同适应</font></strong></p>\n<blockquote>\n<p><strong>过拟合</strong>：在训练集上实现高性能，但没法很好地泛化到看不见的数据（测试集）上。</p>\n<p><strong>在神经网络中，协同适应意味着一些神经元高度依赖其他神经元</strong>。如果那些独立的神经元接收到“坏”的输入，那么依赖的神经元也会受到影响，最终它会显著改变模型的性能，这就是过度拟合可能发生的情况。</p>\n<p>Hinton 提出 Dropout 来防止过拟合：网络中的每个神经元以0和1之间的概率随机丢弃。–&gt; Hinton 认为 Dropout 能防止过拟合的原因在于：<strong><font color = green>通过实施 Dropout 模型被迫拥有可以学习良好特征（或所需数据表示）的神经元，而不依赖于其他神经元。因此，生成的模型对于看不见的数据可能更加鲁棒。</font></strong></p>\n</blockquote>\n<h1 id=\"inproving-neural-networks-by-preventing-co-adaptation-of-feature-detectors_2012_hinton\"><a class=\"markdownIt-Anchor\" href=\"#inproving-neural-networks-by-preventing-co-adaptation-of-feature-detectors_2012_hinton\"></a> Inproving Neural Networks By Preventing Co-adaptation of Feature Detectors_2012_Hinton</h1>\n<blockquote>\n<p><strong><font color = green>协同适应 Co-adaptation</font></strong>：一个特征检测器只在其他几个特征检测器的上下文中有用。</p>\n<p>为了阻止复杂的协同适应性，Dropout 通过在训练过程中随机丢弃一半的特征检测器，迫使 <strong><font color = green>每个神经元学习检测一种特征，这种特征通常有助于产生正确的答案，因为它必须在各种内部环境中运作。</font></strong></p>\n</blockquote>\n<h1 id=\"neuron-specific-dropout-a-deterministic-regularization-technique-to-prevent-neural-networks-from-overfitting-reduce-dependence-on-large-training-samples\"><a class=\"markdownIt-Anchor\" href=\"#neuron-specific-dropout-a-deterministic-regularization-technique-to-prevent-neural-networks-from-overfitting-reduce-dependence-on-large-training-samples\"></a> Neuron-Specific Dropout: A Deterministic Regularization Technique to Prevent Neural Networks from Overfitting &amp; Reduce Dependence on Large Training Samples</h1>\n<h2 id=\"abstract-2\"><a class=\"markdownIt-Anchor\" href=\"#abstract-2\"></a> Abstract</h2>\n<blockquote>\n<p>为了发展输入与输出之间的复杂关系，深度神经网络对大量参数进行训练和调整。为了使这些网络高精度地工作，需要大量数据。<strong><font color = green>然而，有时训练所需的数据量并不存在或无法获得。Neuron-specific dropout (NSDropout) 被提出用来解决该问题。</font></strong>  NSDropout 会同时查看模型中层的训练过程和验证过程。通过比较数据集中每个神经元对每个类别产生的平均值，该网络能够丢弃目标单元。<strong><font color = purple>该层能够预测模型在测试过程中所观察的特征或噪声，而这些特征或噪声在观察验证样本时是不存在的。</font></strong> <strong><font color = blue>与 Dropout 不同的是，“thinned” networks “精简”网络不能 “unthinned” “未精简”用于测试。</font></strong> 与传统方法（包括 dropout 和其他正则化方法）相比，<strong><font color = green>Neuron-specific dropout 被证明可以用更少的数据达到类似的（如果不是更好的话）测试精度。</font></strong> 实验表明， Neuron-specific dropout 减少了网络过拟合的机会，并 <strong><font color = green>减少了图像识别中监督任务对大量训练样本的需要</font></strong>，同时产生了同类最佳（best-in-class）的结果。</p>\n</blockquote>\n<h2 id=\"keywords\"><a class=\"markdownIt-Anchor\" href=\"#keywords\"></a> Keywords:</h2>\n<blockquote>\n<p>neural networks, regularization, model combination, deep learning, dropout</p>\n</blockquote>\n<h2 id=\"1-introduction-2\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-2\"></a> 1. Introduction</h2>\n<blockquote>\n<p>深度神经网络可以理解为输入与输出之间的复杂关系。通过利用数千甚至数百万个隐藏节点（神经元），这些模型可以生成一套足以预测癌症或驾驶汽车的规则。然而，要做到这一点，需要大量数据来训练并验证模型。<strong><font color = green>当数据量不足时，模型可能会关注训练数据中的缺陷或者采样噪声。</font></strong> 换句话说，该模型将发现训练数据中存在的细节，而这些细节可能在实际应用中并不存在（该模型将发现训练数据中可能并不存在于其实际应用中的细节）。这些最终会导致过拟合，并且因为没办法做出一个完美的数据集，因此已经发展了其他方法来尝试减少模型过拟合的趋势。最流行的方法之一是，当模型的验证精度和训练精度出现偏差时，停止训练。另一个方法是实施权重惩罚，如 L1 和 L2 以及软权重共享（soft weight sharing）。</p>\n<img src=NSDropout_f1.png width=100% />\n<p><strong><font color = green>现在有几种方法来解决过拟合问题，一种是贝叶斯方法的使用</font></strong>。贝叶斯模型是根据贝叶斯定理构建统计模型。</p>\n<img src=NSDropout_e1.png width=20% />\n<p>贝叶斯 ML 模型的目标是在给定先验分布 prior distribution <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(\\theta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 和 likely hood <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(x|\\theta))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> 的情况下估计后验分布 posterior distribution <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p(\\theta|x))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span>。这些模型与经典模型的不同之处在于包含了 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> 或先验分布。<strong><font color = green>一种流行的先验分布是高斯过程。</font></strong> 通过取所有参数设置的平均值，并将其值与给定训练数据的后验概率进行加权。有了先验高斯分布，我们可以假设后验分布是正态分布或落在正态钟形曲线上。<strong>假设我们有无限的计算能力，防止过拟合最好的方法是计算一个完美的后验分布。</strong> 然而，<strong><font color = green>逼近后验分布</font></strong> 已经被证明可以在小模型上提供很好的结果。</p>\n<p>对于具有少量隐藏节点的模型，与单个模型相比，对使用不同架构和数据训练的不同模型的值进行平均可以提高性能。然而，对于较大的模型，此过程将过于耗费资源，无法证明回报是合理的。训练多个模型是困难的，因为找到最佳参数可能会耗费大量时间，而且训练多个大网络会占用大量资源。此外，在不同的数据子集上获取足够多的数据来训练多个网络是不可能的。最后，假设你能够使用不同数据子集来训练不同架构的多个网络，在需要快速处理的应用程序中，使用所有这些模型进行测试将花费太多的时间。</p>\n<p>这就引出了防止过拟合的第二种选择。<strong><font color = green>Dropout 是一种简单而有效的方法来限制噪声对模型的影响</font></strong>。它通过“dropping 丢弃”隐藏或可见单元来防止模型过拟合，<strong><font color = green>本质上是同时训练多个模型</font></strong>。通过丢弃一个单元，该单元在该步骤中不再对模型及其决策产生影响。丢弃的神经元数量由概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 决定，即独立于其它单元。</p>\n<img src=NSDropout_f2.png width=100% />\n<p>图2：左：在训练阶段，假设索引 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> 处的值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">r^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 为1时，unit 出现。假设函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>a</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">a_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的输出在向量函数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的输出值中不是最低的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 个百分比，则 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>r</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">r_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的值为1。右：在测试阶段，只有当 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>r</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">r_i^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span> 的最终值为 1 时，unit 才会出现。</p>\n<p>现在我们有了另一种防止过拟合的方法。 <strong><font color = green>Neuron-Specific dropout 采用了从一个层中丢弃隐藏或可见单元的思想，而不是随机的丢弃它们</font></strong> 。与其它流行的层不同， <strong><font color = green>Neurom-Specific Dropout 接受四种输入</font></strong> ：layer input 层输入，the true value of the sample 样本真值，validation layer input 验证层输入，the true value of the validation sample 验证样本真值。通过了解哪些神经元的值与该类样本的验证平均值最远，我们可以找到噪声或训练数据中的伪影在哪些地方影响了我们的模型决策。丢弃的神经元数量取决于比例 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span>。然而，这与 Dropout 不同，因为概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 表示一层中有多少 百分比的 units 将被丢弃。例如，如果在具有 20 个 units 的层中将 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 设置为 0.2，那么总的会有 4个 units 被丢弃。</p>\n<p>通常，神经网络中使用的验证数据不应该在调整超参数之外影响模型的行为，但是 neuron-specific dropout 可以提高准确性，这样就可以分割传统的训练数据集，从而永远不会使用保留的验证数据。对训练数据进行分割，以便为新的验证集保留 20% 似乎时是最佳的。</p>\n<p>类似于 Dropout，应用 neuron-specific dropout 会产生一个 “thinned” 的神经网络。这个 thinned 神经网络保存了从神经元丢弃中幸存下来的神经元的所有值。虽然可以解释为具有 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> 个 units 的神经网络代表 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">2^n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.664392em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span></span></span></span></span></span></span> 个可能的 thinned 神经网络，但众所周知，随着训练的进行，从一个步骤到下一个步骤丢弃的不同的 units 的数量会减少。同样，可训练参数的总数仍然是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(n^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> ，或者更少。</p>\n<p>与 dropout 不同的是，如果使用单个的，按比例缩小的神经网络，使用该层的好处不会显示出来。当最后一次使用 mask 时，发现测试结果最好。这是有意义的，因为与 dropout 不同， units 不是随机丢弃的。当模型开始找到受噪声和特征影响的 units 时，它会将它们归零，而把它们带回来则会带回它已经学会的在没有噪声和特征的情况下改进的权重。</p>\n<p>本文结构如下。第 2 节描述了 neuron-specific dropout 的动机。第 3 节描述了之前的相关工作。第 4 节正式描述了 neuron-specific dropout  model 和它如何工作。第 5 节一个训练 neuron-specific dropout 网络的算法，并引入了不可见验证的思想。第 6 节给出了应用 NSDropout 的实验结果，并与其他形式的正则化和模型组合进行了比较。第 7 节讨论了 NSDropout 的显著特征，并分析了 neuron-specific 的影响，以及不同的参数如何改变网络的性能。</p>\n</blockquote>\n<h2 id=\"2-motivation\"><a class=\"markdownIt-Anchor\" href=\"#2-motivation\"></a> 2. Motivation</h2>\n<blockquote>\n<p>neuron-specific dropout 的动机来自于 dropout。与 neuron-specific dropout 类似， dropout 切断了与神经元的连接。这项研究最初是出于限制数据量的想法，但当发现 neuron-specific dropout 也可以帮助减少过拟合时，这项研究很快改变了主意。在日常生活中，人们学到的信息比需要的更多，无论是从对话，新闻还是课程。当大脑认为学习到的信息以后不会再被使用时，就会失去一部分。这有助于防止大脑变得混乱。</p>\n<p>对于这种现象产生的一个可能的解释是大脑中一种称为干扰的现象。当一个记忆干扰其他记忆时，就会发生干扰。记忆可以定义为大脑中获取，存储，保留和稍后检索信息的过程。干扰可以是主动的或追溯的（事后的）。主动干扰是指大脑由于记忆较旧而无法记住信息。追溯性干扰是指大脑在收到新信息时保留先前学习信息的能力。<strong><font color = green> Neuron-specific dropout 使用类似于追溯性干扰的方法</font></strong> 。虽然模型本身无法知道哪些信息是有用的（类似于人脑），但验证数据可以让它们了解它们在测试时会看到什么。通过了解验证阶段存在哪些噪声，模型可以关闭或忘记哪些信息对于测试是不必要的。当每个隐藏单元被呈现出新的信息时，即前一层的输出时，它会接收并“学习”这些信息，然后，在激活之前，它会决定哪些信息“干扰”来自验证数据的信息。</p>\n</blockquote>\n<h2 id=\"8-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#8-conclusion\"></a> 8. Conclusion</h2>\n<blockquote>\n<p>Neuron-specific dropout (NSDropout) 是一种旨在提高神经网络准确性的 <strong><font color = green>确定性正则化技术，重点关注具有少量训练数据的网络</font></strong>。通过传统的学习技术，<strong><font color = green> 网络在一组数据的输入与输出之间建立了复杂的关系，然而这些复杂的关系往往不能泛化到看不见的未知数据</font></strong>。***<font color = purple>与 Dropout 不同的是， Dropout 可以随机破坏这些复杂的关系， Neuron-specific dropout 可以帮助网络理解这些复杂的关系中哪些导致了网络的过拟合，并关闭隐藏单元，强迫网络在没有这些导致网络过拟合的复杂关系的情况下学习。</font>*** 实验证明，使用 NSDropout 可以提高神经网络在图像分类领域的性能。NSDropout 能够在 MNIST 手写数字，Fashion-MNIST 和 CIFAR-10 中取得最好的（best-in-class）结果。</p>\n<p>此外，为了提高图分类网络的性能，NSDropout 还减少了对大数据集的需求。当对 MNIST 手写数字进行训练时， NSDropout 网络仅使用 750 个训练样本就能达到完美的测试精度（a perfect test accuracy）。在 Fashion-MNIST 中， NSDropout 仅使用 60000 个训练样本中的 10000 个 就能达到近乎完美的准确率（a near-perfect accuracy）. <strong><font color = green>NSDropout 的一个关键特征是能够在训练期间将测试精度和训练精度联系起来。</font></strong> 这有助于限制网络过拟合的机会。</p>\n<p><strong><font color = blue>NSDropout 的一个局限是训练模型所需时间的增加。</font></strong> 一个图像分类 NSDropout 模型的训练时间是相同架构的标准神经网络的 4 倍，并且没有进行优化。它需要比传统的 dropout 模型多 两倍 的时间。<strong><font color = blue>时间增加的一个主要原因是 NSDropout 层中按类排序和无序的多个输入。</font></strong> 虽然排序算法变得更快，并且可以对 NSDropout 进行更多的优化，但它们仍然占用了处理过程中的大部分时间。<strong><font color = green>目前 NSDropout 只是 丢弃（drops）它认为网络过于依赖的单元，但未来的工作可能会着眼于如何调整单元而不是丢弃它</font></strong>，从而在更广泛的应用程序中提高性能。</p>\n</blockquote>\n</li>\n</ol>\n<h1 id=\"structural-dropout-for-model-width-compression_2022\"><a class=\"markdownIt-Anchor\" href=\"#structural-dropout-for-model-width-compression_2022\"></a> Structural Dropout for Model Width Compression_2022</h1>\n<h2 id=\"abstract-3\"><a class=\"markdownIt-Anchor\" href=\"#abstract-3\"></a> Abstract</h2>\n<blockquote>\n<p>众所周知，现有的 ML 模型是高度过度参数化的（highly over-parameterized），并且使用了比给定任务所需更多的资源。以前的工作已经探索了离线压缩模型（compressing models offline），例如，从较大的模型中提取知识到较小的模型中。这对于压缩是有效的，但没有给出衡量模型可以压缩多少的经验方法，并且需要对每个压缩模型进行额外的训练。<strong><font color = green>我们提出一种只需要对原始模型和一组压缩模型进行一次训练的方法。</font></strong> 所提出的方法是一种 <strong><font color = green>structural dropout</font></strong>，它会在随机选择的索引之上剪枝掉所有处于隐藏状态的元素，从而迫使模型学习其特征的重要性排序。在学习了这种排序之后，在推理阶段可以剪枝掉不重要的特征，同时保持最大的准确性，显著减小参数大小。在这项工作中，我们聚焦于全连接层的 Structural Dropout，但这个概念可以应用于任何类型的具有无序特征的层，如卷积层或 attention layers。Structural Dropout 不需要额外的剪枝 / 重新训练，但需要对每个可能的隐藏大小（each possible hideen sizes）进行额外的验证。在推理阶段，非专业人员可以在广泛的高压缩和更精确的模型之间选择最适合他们需求的内存与精度的权衡。</p>\n</blockquote>\n<h2 id=\"1-introduction-3\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-3\"></a> 1. Introduction</h2>\n<blockquote>\n<p>总结起来，这项工作的贡献如下：</p>\n<ol>\n<li>Dropout  的一种变体，Structural Dropout，它训练一个嵌套网络的集合，以后可以在不进行额外的重新训练（retraining）的情况下将这些网络分离出来进行压缩。</li>\n<li>在 3 个示例任务上验证 Structural Dropout，证明其在保持准确性的同时，各种方法的有效性。</li>\n<li>Structural Dropout 的实现：<a href=\"https://github.com/JulianKnodt/structural_dropout\" title=\"Strucutural Dropout \">An Implementation of Structural Dropout</a></li>\n</ol>\n<img src=StructuralDropout_f1.png width=100% />\n<p>在训练过程中，Structural Dropout 并不是随机选择要剪枝的索引，而是在统一随机选择索引（a uniformly randomly selected index）后剪枝所有节点，并根据丢弃的特征数量对期望进行归一化。在一定的可能性下，我们运行整个网络，用它间接地监督较小的网络（间接将其用作较小网络的监督）</p>\n</blockquote>\n<h2 id=\"5-discussion\"><a class=\"markdownIt-Anchor\" href=\"#5-discussion\"></a> 5. Discussion</h2>\n<blockquote>\n<p>Structural Dropout 作为现有架构的最小补充，可以执行超参数搜索和压缩。由于它不需要昂贵的重新训练和额外的领域知识，因此它比特定领域的修改更容易采用，并且与剪枝和量化正交。</p>\n<p>在我们的实验中，很明显存在信息饱和的陡峭悬崖，并且可以以最小的精度变化来修剪 50%-80%之间的重要特征。如在 PointNet 上所见，当使用更高的 dropout rate 进行更积极的剪枝时，可以在不损失精度的情况下剪枝高达 80%。</p>\n<p>Structural Dropout 也可能有助于提高性能，因为纯粹通过对多个模型进行采样，其中一个模型可能在给定任务上表现更好。</p>\n</blockquote>\n<h2 id=\"6-limitation\"><a class=\"markdownIt-Anchor\" href=\"#6-limitation\"></a> 6. Limitation</h2>\n<blockquote>\n<p>虽然我们的方法可以直接添加到现有的体系架构中，但它也有一些缺点。</p>\n<p><strong><font color = green>一个显著的缺点是 SD 增加了搜索空间，使问题变得更加困难。</font></strong> 由于问题更加困难，尽管使用更少的参数可以加快速度，但训练过程可能需要更长的时间。这个训练时间并不比原来长很多，但是不清楚到底长多少。<strong><font color = green>除了增加的训练时间之外，比较所有通道宽度的验证损失是缓慢的，因为有大量的模型需要测试。</font></strong> 如果资源可用，这可以很容易地并行化，因为与训练不同，模型将是只读的，否则可以执行稀疏搜索。</p>\n<p><strong><font color = green>SD 的另一个缺点是，它在训练过程中更难以验证和跟踪收敛。</font></strong> 由于 low channel width，模型在训练过程中可能随机出现精度较低的情况，因此很难确定模型的收敛性。所以，对于之前训练过的模型使用 SD 是有意义的，并且先验收敛参数已经设置。在这些模型上，它也可以用作对通道宽度执行超参数搜索的一种方式。</p>\n<p>此外，<strong><font color = green>虽然我们假设所有的 SD 尺寸（SD sizes）在推断时应该是相同的，但在一个具有各种层的较大模型中，情况可能并非如此</font></strong>。对所有可能的通道大小选择执行详尽的搜索将是昂贵的，因此有效的搜索策略很重要。一个常见的假设是每个选择都可以独立做出，但我们将这一探索留给未来的工作。</p>\n<p>最后，<strong><font color = green>我们的方法不能剪枝整个层，因为 SD 仅限于改变神经网络的宽度，而不能修改它的深度。</font></strong> 因此，性能瓶颈是深度的网络将无法获得同样多的好处。</p>\n</blockquote>\n<h2 id=\"7-future-work\"><a class=\"markdownIt-Anchor\" href=\"#7-future-work\"></a> 7. Future Work</h2>\n<blockquote>\n<p>虽然我们展示了 Structural Dropout 在 FC 层上的应用，但同样的原理可以扩展到其他类型的层。例如，同样的思路也适用于 CNN 的 channel dimension，允许对特征进行修剪。另一种可能的扩展是针对 transformers，在 transformers 中选择多个注意力头 （ a number of attention heads）很重要。 Structural dropout 可应用于注意力头的大小和注意力头的数量。我们希望将这项工作扩展到探索在各种架构中使用 Structural Dropout 以实现实用的和高效的压缩。</p>\n</blockquote>\n<h2 id=\"8-conclusion-2\"><a class=\"markdownIt-Anchor\" href=\"#8-conclusion-2\"></a> 8. Conclusion</h2>\n<blockquote>\n<p>Structural Dropout 是一种用于推测时间压缩（inference-time compression）的方法，可用作现有架构的 drop-in layer，以最小的精度损失实现大量的压缩。这是作者所知道的第一种方法，它允许在单个训练会话（a single training session）中训练许多任意压缩的模型，然后联合部署它们，代价是只部署最大的模型。从我们的实验中，我们希望 Structural Dropout 是一种用于压缩神经网络的强大工具。</p>\n</blockquote>\n<h1 id=\"dropout-regularization-for-automatic-segmented-dental-images_aciids_2021\"><a class=\"markdownIt-Anchor\" href=\"#dropout-regularization-for-automatic-segmented-dental-images_aciids_2021\"></a> Dropout Regularization for Automatic Segmented Dental Images_ACIIDS_2021</h1>\n<h2 id=\"abstract-4\"><a class=\"markdownIt-Anchor\" href=\"#abstract-4\"></a> Abstract</h2>\n<blockquote>\n<p>深度神经网络是指具有大量参数的网络，是深度学习系统的核心。从这些系统中产生了一个挑战，即它们如何针对训练数据 和 / 或 验证数据集执行。由于所涉及的参数数量众多，网络往往会消耗大量时间，这会导致称为过拟合的情况。<strong><font color = green>这种方法建议在模型的输入和第一个隐藏层之间引入一个 dropout 层</font></strong> 。这是非常特别的，与其他领域使用的传统 Dropout 不同，传统的 Dropout 在网络模型的每个隐藏层中引入 dropout 来解决过拟合。我们的方法涉及预处理步骤，该步骤处理数据增广以处理有限数量的牙科图像和侵蚀形态以消除图像中的噪音。此外，使用 canny 边缘检测方法进行分割以提取基于边缘的特征。除此之外，所使用的神经网路采用了 Keras 的顺序模型，这是为了将边缘分割步骤的迭代合并到一个模型中。对模型进行并行评估，首先没有 Dropout，另一个使用大小为 0.3 的 dropout 输入层。在模型中引入 dropout 作为权重正则化技术（weight regularization technique），提高了评估结果的准确性，无论是 precision 准确率还是 recall values 查全率，没有 dropout 的模型为 89.0%，而有 dropout 的模型为 91.3%。</p>\n</blockquote>\n<h2 id=\"keywords-2\"><a class=\"markdownIt-Anchor\" href=\"#keywords-2\"></a> Keywords:</h2>\n<blockquote>\n<p>Deep learning,  Over-fitting,  Regularization technique,  Dropout</p>\n</blockquote>\n<h2 id=\"1-introduction-4\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-4\"></a> 1. Introduction</h2>\n<blockquote>\n<p>过拟合是各种深度学习系统中普遍存在的问题。当模型在训练集上训练得太好，而在测试集上训练得不太好时，通常会发生过拟合的情况。或者，欠拟合是指我们的模型在训练集和测试集上都表现不佳。</p>\n<p>因此，这两种情况可以通过几种称为权重正则化技术（weight regularization technique）的方法来处理。这些方法包括 early stopping, L1,L2 regularization 和 Dropout。在我们的方法中，我们使用 Dropout，包括丢弃神经网络模型中隐藏和可见的单元。这是通过在训练阶段忽略在随机选择的特定神经元集（certain set of neurons）的 units 单元来实现的。从技术上讲，在每个训练阶段，单个 units 要么以 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">1-p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 的概率从网络中丢弃，要么以 p 的概率保留，这样剩下的是一个简化的网路（reduced network）.</p>\n<p>Dropout 背后的关键思想是在训练过程中，从神经网络中随机丢弃 units 及其它们的连接，以防止 units 过度自适应（co-adapting）。在训练阶段丢弃不同网络模型的 units 后，这使得测试更容易接近网络平均预测的效果。从 dropout 的过程中，减少了过拟合，并进一步对其他正则化方法进行了重大改进。</p>\n<p>在其他研究（例如：<a href=\"https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\" title=\"Dropout Regularization \">Dropout Regularization in Deep Learning Models with Keras</a> ）中，展示了 dropout 如何应用于深度学习系统。可以通过多种方式在网络模型中引入 Dropout。它可以作为输入和第一个隐藏层之间的一个层来引入。其次，它可以应用于两个隐藏层之间以及最后一个隐藏层和输出层之间。</p>\n<p>我们提出的方法使用了第一个方法，在输入层和第一个隐藏层之间引入 dropout。Dropout 在大型网络中非常有用，它具有各种约束条件，如 learning rate, decay 和 momentum，以调高评估性能。</p>\n</blockquote>\n<h2 id=\"5-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#5-conclusion\"></a> 5. Conclusion</h2>\n<blockquote>\n<p>Dropout 是一种通过减少过拟合来改进神经网络的技术。相比于模型的隐藏层之间引入一个独立的 dropout layer，在输入可见层中引入该算法得到了很好的结果。深度神经网络模型的训练需要很长时间，从使用我们的方法进行的实验中，我们见证了模型复杂度的降低和训练时间的增加。</p>\n<p>我们的 Dropout 方法可以与其他正则化方法一起使用，以获得更好的性能结果。其他可用于获得更好的性能的权重正则化技术包括 early stopping 以解决在模型图上见证的验证损失变化。</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>神经元特定的 dropout 是针对训练样本不足或无法获得的问题，提出的方法能够在保证精度的同时，减少训练样本的需求。</li>\n<li>用于模型宽度压缩的结构 dropout ，是针对离线压缩模型没有给出衡量模型可以压缩多少的方法且需要对每个压缩模型进行额外的重新训练的问题，作者提出的方法不需要额外的剪枝或者重新训练。</li>\n<li>牙医图像自动分割的 dropout 正则化，传统的dropout是在网络模型的每个隐藏层中引入 dropout 来解决过拟合的，而作者是在模型的输入和第一个隐藏层之间引入 dropout层，也就是研究了针对牙医图像自动分割技术， dropout 通过什么样的方式能更好地应用于深度学习系统。</li>\n</ol>\n</blockquote>\n<h1 id=\"gating-dropout-communication-efficient-regularization-for-sparsely-activated-transformers_icml_2022\"><a class=\"markdownIt-Anchor\" href=\"#gating-dropout-communication-efficient-regularization-for-sparsely-activated-transformers_icml_2022\"></a> Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers_ICML_2022</h1>\n<h2 id=\"abstract-5\"><a class=\"markdownIt-Anchor\" href=\"#abstract-5\"></a> Abstract</h2>\n<blockquote>\n<p>Sparsely activated transformers, 如 Mixture of Experts(MoE)，由于其惊人的缩放能力而引起了极大的兴趣，这可以不显著增加计算成本的情况下显著增加模型大小。为了实现这一点， MoE 模型用 Transformer 中的 Mixture-of-Experts  sub-layer 替换前馈子层（feed-forward sub-layer），并使用一个 gating network 门控网络将每个令牌路由到其指定的专家。由于有效训练此类模型的常见做法需要将专家和令牌分布在不同的机器上，因此这种路由策略通常会产生巨大的跨机器通信成本，因为令牌及其分配的专家可能位于不同的机器中。在这篇文章中，作者提出 Gating Dropout，它允许令牌忽略 gating network 并停留在它们的本地机器上，从而减少了跨机器通信。与传统的 dropout 类似，我们也表明， Gating Dropout 在训练中有正则化效果，从而提高了正则化性能。我们验证了 Gating Dropout 在多语言机器翻译任务中的有效性。我们的结果表明， Gating Dropout 改进了最先进的 MoE 模型，具有更快的 wallclock 时间收敛率和更好的 BLEU 分数，适用于各种模型大小和数据集。</p>\n</blockquote>\n<h2 id=\"6-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#6-conclusion\"></a> 6. Conclusion</h2>\n<blockquote>\n<p>我们提出 Gating Dropout 作为一种通讯高效的正则化技术来训练 sparsely activated transformers。我们观察到 sparsely activated transformers，例如 MoE 模型，通常具有非常高的跨机器通信成本，因为它们需要通过 all-to-all 通信操作将令牌发送给指定专家。Gating Dropout 通过随机跳过 all-to-all 操作来降低通信成本。这种随机跳跃在训练期间也具有正则化效果，从而提高了泛化性能。多语言翻译任务的实验证明了该方法在吞吐量（throughput）、泛化性能和收敛速度方面的有效性。</p>\n<p>关于未来的工作，我们正在研究如何通过结合 Gating Dropout 和专家剪枝来提高推理速度。Gating Dropout 目前对推理速度没有影响，因为它只是在推理阶段关闭。此外，我们还对整个训练阶段中不同 dropout rate 的影响感兴趣，因为从探索-利用的角度，探索在训练的早期阶段可能更为重要。</p>\n</blockquote>\n<h1 id=\"dropout-regularization-in-hierarchical-mixture-of-experts_neurocomputing_2021\"><a class=\"markdownIt-Anchor\" href=\"#dropout-regularization-in-hierarchical-mixture-of-experts_neurocomputing_2021\"></a> Dropout Regularization in Hierarchical Mixture of Experts_Neurocomputing_2021</h1>\n<p>专家分层混合中的 Dropout 正则化</p>\n<h2 id=\"abstract-6\"><a class=\"markdownIt-Anchor\" href=\"#abstract-6\"></a> Abstract</h2>\n<blockquote>\n<p>Dropout 是一种非常有效的防止过拟合的方法，近年来已成为多层神经网络的首选正则器。专家的分层混合是一个分层门控模型（hierarchically gated model），它定义了一个软决策树，其中叶子对应于专家，决策节点对应于在其子项之间软选择的门控模型，因此，该模型定义了输入空间的软分层分区。在这项工作中，我们提出了一种用于专家分层混合的 dropout 变体，它忠实于模型定义的树层次结构，而不是像多层感知器那样具有平面的、单元独立的 dropout 应用程序。我们表明，在合成回归数据以及 MNIST 和 CIFAR-10 数据集上，我们提出的 dropout 机制可以防止在树上的过拟合，并在多个层次上提高泛化能力并提供更平滑的拟合。</p>\n</blockquote>\n<h2 id=\"5-conclusions\"><a class=\"markdownIt-Anchor\" href=\"#5-conclusions\"></a> 5. Conclusions</h2>\n<blockquote>\n<p>我们提出了一种新的 Dropout 机制，可应用于专家分层混合方法及其扩展。与具有条件独立单元的平面架构上的 dropout 相比，我们的方法忠实于模型树层次结构中存在的门控依赖关系（the gating dependencies）。</p>\n<p>我们展示了我们的方法在一个合成玩具数据集以及用于数字识别和图像分类任务的两个真实数据集上的有效性。在所有的数据集上，我们看到专家的分层混合在有太多级别和叶子时确实会过拟合，但是我们提出的方法可以作为一种有效的正则化器，其中 dropout rate 作为权衡偏差 bias 和方差 variance 的超参数。</p>\n<p>我们还定性地评估 dropout 对模型学习到的表示的影响，这些模型通过提供可视化来可视化 dropout 的影响。由于我们仅不对称地丢弃左子树这一事实，我们的 dropout 方法有效地从具有不同复杂性的树结构模型的集合中采样。这种方法通过充当具有不同复杂性的模型的插值来引入正则化。</p>\n</blockquote>\n<h1 id=\"clustering-based-adaptive-dropout-for-cnn-based-classification_pr_2020\"><a class=\"markdownIt-Anchor\" href=\"#clustering-based-adaptive-dropout-for-cnn-based-classification_pr_2020\"></a> Clustering-Based Adaptive Dropout for CNN-Based Classification_PR_2020</h1>\n<h2 id=\"abstract-7\"><a class=\"markdownIt-Anchor\" href=\"#abstract-7\"></a> Abstract</h2>\n<blockquote>\n<p>Dropout 被广泛用于提高深度网络的泛化能力，而目前的 dropout 变体很少动态调整网络隐藏单元或权重的 dropout 概率（dropout probabilities）以适应它们对网络优化的贡献。这篇文章提出了一种基于聚类的 dropout（clustering-based dropout），该算法基于特征、权重或它们的衍生物的网络特征，其中这些特征的 dropout 概率根据相应的聚类组自适应更新以区分它们的贡献。在 Fashion-MNIST 和 CIFAR-10 数据库以及 FER2013 和 CK+ 表情数据库上的实验结果表明，所提出的基于聚类的 dropout 比原始的  dropout 和各种 dropout 变体具有更好的准确性，并且与最先进的算法相比具有最具竞争力的性能。</p>\n</blockquote>\n<h2 id=\"keywords-3\"><a class=\"markdownIt-Anchor\" href=\"#keywords-3\"></a> Keywords:</h2>\n<blockquote>\n<p>Feature and weight clustering,  Feature derivative dropout,  Self-adaptive dropout probability,  Facial expression recognition</p>\n</blockquote>\n<h2 id=\"1-introduction-5\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-5\"></a> 1. Introduction</h2>\n<blockquote>\n<p>为了提高深度网络的正则化能力，提出了 regularizer 正则化器、batch normalization 批归一化 和 sparse deep feature learning 稀疏深度特征学习【Sparse deep feature learning for facial expression recognition _PR_2019 】，来减少过拟合的可能性。Dropout 随机丢弃网络隐藏单元或权重，也被应用于很多目标识别问题。受隐藏单元 dropout 的启发， connection (weight) dropout 被提出来随机丢弃权重元素。Khan【Regularization of deep neural networks with spectral dropout _NN_2019 】 等人提出了对 feature map 的光谱变换进行 dropout，其中引入了与 feature map 的重塑维度（the reshaped dimension of the feature map）相对应的三种不同的变体。</p>\n<p>然而，传统 dropout 中的隐藏单元或权重是逐个元素地抑制的，这可能会忽略元素块 element block 中隐含的结构信息。Tompson 【Efficient object localization using convolutional networks_CVPR_2014】等人提出 spatial dropout 来丢弃一整个 feature map，即同时丢弃或保留（dropped or retained）一个 feature map 的所有隐藏单元。Poernomo 和 Kang 【Biased dropout and crossmap dropout: learning towards effective dropout regularization in convolutional neural network_NN_2018】根据隐藏单元响应【Learning both weights and connections for efficient neural network _NIPS_2015】的大小将特征分为大小相等的两组，并为每一组分配一个 dropout 概率。同时，提出一个额外的 cross-map dropout，其中不同 feature maps 上相同坐标的元素被同时丢弃或保留。然而，两组不足以区分不同特征之间的贡献，应该设计更多的组。Rohit 等人【Guided dropout _AAAI_2019】根据节点的强度（the strength of each node），提出通过删除节点来引导 dropout。Zhang 等人【ML-LocNet: improving object localization with multi-view learning network _ECCV_2018】提出 region dropout，利用显著区域（salient regions）的组合进行训练。但是，区域的相对位置和大小是固定的，不够灵活。Zhang 等人【Image ordinal classification and understanding: grid dropout with masking label _ICME_2018】提出 grid dropout 来减少搜索空间，以方便对全局特征的探索。然而，相同 grid 中的元素可能彼此之间存在显著差异，因此分配给整个网格 grid 相同的 dropout 概率可能不适用于相同 grid 中显著不同的元素。</p>\n<p>对于 dropout 的特征（hidden unit, feature or weight）分组，最先进的 dropout 变体没有以足够的灵活性和多样性来划分这些特征。实际上，对于网络反向传播，即使特征图和权重矩阵中的相邻元素对网络损失的贡献也大有不同。例如，图 1 展示了使用 ResNet18 的表情图像的特征图的活动区域，其中根据 heat maps response 热图影响将不同的 feature maps 分为三个不同的重要性等级，即：insignificant, fair and significant。直观的说，特征元素响应的大小应该与 dropout 概率负相关。然而，传统的 dropout 和最先进的变体无法收集这些 insignificant 无关紧要的 feature maps 或分布在整个 map 上的元素用于 dropout。在这项工作中，在 dropout 中引入了 network element clustering 网路元素聚类，将相似的元素分组，以使它们共享相同的 dropout 概率。因此，利用所提出的聚类方法，可以通过分配一个具有较大 dropout 概率的相应组来同时抑制不重要的元素。</p>\n<p>对于 dropout 概率设置，在整个网络训练过程中保持固定的 dropout 概率可能会忽略不同部分对网络优化的动态影响。 Wager 等人【Dropout training as adaptive regularization _NIPS_2013】将 dropout 训练视为一种具有二阶导数近似的自适应正则化形式。 Ba and Frey 等人【Adaptive dropout for training deep neural networks _NIPS_2013】根据矩阵元素性能 matrix elements performance，提出了一种通过更新 a probability mask matrix 概率掩码矩阵的自适应 dropout 方法。在这项工作中， dropout 概率是根据平均特征响应的聚类组（the clustering group of average characteristic response）动态更新的。</p>\n<img src=Clustering-basedAdaptiveDropout_f1.png width=80% />\n<p>图1 残差网络（ResNet18）的最后一个卷积层中示例表达式的 512 个 feature maps 中的 6 个。根据感兴趣区域对 RaFD 数据库的影响，feature maps 可以被分为不同的重要性等级（importance levels），即： insignificant, fair and significant。</p>\n<p>为了考虑 dropout 的特征，通常使用深度网络中的全连接层特征（FC layer features，即 layer input）和权重矩阵（weight matrix）作为判别特征来确定识别性能（as the discriminative features to determine the recognition performance）。因此， FC features, the weights 及其 their derivatives 被用作聚类的特征。</p>\n<p>这项工作的主要贡献总结如下：</p>\n<ul>\n<li>提出了一种基于 FC features, weights or their derivatives 聚类的新型 dropout 算法；</li>\n<li>根据每组 feature, weight or derivative clustering 的响应幅度，提出了 dropout 概率的自适应更新方法；</li>\n<li>在 Fashion-MNIST 和 CIFAR10 数据库以及 FER2013 和 CK+ 表情数据库上取得了有竞争力的性能。</li>\n</ul>\n<p>本文分为以下几个部分。第 2 节介绍了提出的 clustering-based dropout。第 3 节给出了实验结果和相应的插图。最后，在第 4 节提出结论和讨论。</p>\n</blockquote>\n<h2 id=\"4-conclusion-2\"><a class=\"markdownIt-Anchor\" href=\"#4-conclusion-2\"></a> 4. Conclusion</h2>\n<blockquote>\n<p>考虑到全连接特征、权重、特征和权重的衍生物中的元素对网络优化的贡献不同，提出了一种具有自适应 dropout 概率的基于聚类的 dropout 算法。本文提出的 dropout 进一步嵌入到 ResNet18 的 FC 层，用于四个公共数据库，即 Fashion-MNIST, CIFAR-10, FER2013 和  CK+，实验结果验证了所提出的 dropout 相比于其他 dropout 变体和相关的最新算法的竞争力。</p>\n<p>虽然本文提出的基于聚类的 dropout 方法获得了具有竞争力的结果，但仍有进一步改进的空间。首先，引入超参数对网络学习的影响，如簇的数量（the number of clusters），需要进一步研究。其次，深入研究不同模型选择下基于聚类的 dropout 的理论分析。最后，提出的 dropout 应该应用于更多的模型和任务。</p>\n</blockquote>\n<h1 id=\"correlation-based-structural-dropout-for-convolutional-neural-networks_pr_2021\"><a class=\"markdownIt-Anchor\" href=\"#correlation-based-structural-dropout-for-convolutional-neural-networks_pr_2021\"></a> Correlation-based structural dropout for convolutional neural networks_PR_2021</h1>\n<h2 id=\"abstract-8\"><a class=\"markdownIt-Anchor\" href=\"#abstract-8\"></a> Abstract</h2>\n<blockquote>\n<p>卷积神经网络很容易遭受过拟合问题的影响，因为它们在小型训练数据集的情况下经常被过度参数化（over-parameterized）。<strong><font color = green>传统的 dropout </font></strong> 随机丢弃 feature units 对于全连接网络效果很好，但 <strong><font color = green>由于中间特征的高空间相关性（high spatial correlation of the intermediate features）</font></strong> 而不能很好地正则化 CNNs，这**<font color = green>使得丢弃的信息流过网络，从而导致 under-dropping </font>**问题。为了更好地正则化 CNNs，已经提出了一些 structural dropout methods，例如 <strong><font color = blue>SpatialDropout 和 DropBlock</font></strong>，它们通过在连续区域中随机丢弃 feature units 来实现。然而，这些方法 <strong><font color = blue>可能会因为丢弃关键的判别特征（ critical discriminative features ）而遭受 over-dropping 问题</font></strong> ，从而限制了 CNNs 的性能。为了解决这些问题，我们提出了一种新颖的 structural dropout method，Correlation based Dropout（CorrDrop），通过 <strong><font color = purple>基于 feature correlation 丢弃 feature units</font></strong> 来正则化 CNNs。与之前的 dropout 方法不同，我们的 CorrDrop 可以 <strong><font color = purple>聚焦于判别信息（discriminative information），并以 spatial-wise 或 channel-wise 的方式丢弃 features</font></strong> 。在不同的数据集，网络架构和各种任务（如，图像分类和目标定位）上的广泛实验证明了我们的方法优于其他方法。</p>\n</blockquote>\n<h2 id=\"1-introduction-6\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-6\"></a> 1. Introduction</h2>\n<blockquote>\n<p>卷积神经网络已经广泛应用于机器学习社区和计算机视觉任务中，包括图像识别和目标检测。近年来， ResNet, InceptionNet 和 DenseNet 等很多先进的 CNNs 被设计来提高传统 CNNs 的性能。提出了更深和更宽的深度学习模型，以在各种计算机视觉任务中实现最先进的性能。然而，这些模型有数百万个参数，因此很容易遭受过拟合的问题，尤其在训练数据有限的情况下。因此，开发正则化方法来缓解 CNNs 的过拟合是必不可少的。</p>\n<p>早期提出的正则化方法有很多，如 weight decay, early stopping, data augmentation, batch normalization 和 dropout。这些方法已被采用作为常规的工具来正则化深度神经网络。其中，传统的 dropout 在全连接（FCs）网络中运行良好。然而，这种 <strong><font color = green>dropout 并不能通过在 feature map 中随机丢弃单个 feature unit 来有效地正则化 CNNs，因为空间相关的 features 仍然允许丢弃的信息在网络中流动，从而导致 under-dropping 问题。</font></strong></p>\n<p>为了使 dropout 对 CNNs 更有效，最近提出了一些 structural dropout methods，包括 <strong><font color = blue> SpatialDropout，Cutout 和 DropBlock</font></strong> ，以提高 CNNs 的正则化能力。这些方式 <strong><font color = blue>试图通过在 input/feature space 中随机丢弃整个 channels 或 square of regions </font></strong> 来正则化 CNNs。然而，由于 the feature units 以随机方式在连续区域中被丢弃，而 <strong><font color = blue>不考虑图像中的语义信息</font></strong> ，因此它们存在 over-dropping 问题。 <strong><font color = blue>这种丢弃 feature unit 的随机方式可能会丢弃 the input/feature maps 中的整个判别区域（the whole of discriminative regions）</font></strong> ，并限制模型的学习能力。如图 1 所示， <strong><font color = green>传统的 dropout 丢弃 feature maps 中的 single unit</font></strong> ，而 <strong><font color = blue>structural DropBlock 直接丢弃 feature maps 中的 a square of feature units</font></strong> ，并且可能会将信息语义区域归零。</p>\n<img src=CorrDrop_f1.png width=80% />\n<p>图 1. Dropout, DropBlock 和我们的 Spatial-wise CorrDrop masks（前三行）的示例。红色的部分表示要屏蔽的区域（the regions to be masked）。<strong><font color = orange>最后一行表示 CorrDrop 对应的相关热图（the corresponding correlation heatmap）</font></strong> 。<strong><font color = purple>聚焦于主要目标的 feature units 之间的相关性更强。</font></strong> 与 Dropout 和 DropBlock 相比，  <strong><font color = purple>CorrDrop 考虑了判别性信息（discriminative information），自适应地丢弃 feature units </font></strong> 以缓解 under-dropping  和 over-dropping 问题。</p>\n</blockquote>\n<blockquote>\n<p><strong><font color = orange>受观察到的目标的判别区域（discriminative region of an object）将具有更高的特征相关性（feature correlations）的启发</font></strong>（参见图 1 最后一行），我们提出了 Correlation-based Dropout（CorrDrop），这是一种新颖且有效的 CNNs 结构 dropout 方法，该方法考虑到 spatial / channel dimensions 上的 feature correlation，从而丢弃 feature units。</p>\n<p>不同于之前的随机丢弃 feature units 的 structural dropout methods（如 DropBlock），我们的  <strong><font color = purple>CorrDrop 基于判别信息（discriminative information）自适应地丢弃 feature units</font></strong> 。具体来说，我们 <strong><font color = purple>首先计算 feature correlation map 以指示最具辨别力的区域（the most discriminative regions），然后自适应地屏蔽那些辨别力较差的区域（those less discriminative regions），即特征相关值较小的区域（regions with small feature correlation values）</font></strong> 。由于 feature correlation 根据相关性计算的方法可以进一步分为 spatial-wise feature correlation 和 channel-wise feature correlation，我们提出了 CorrDrop 的两种变体： Spatial-wise CorrDrop 和 Channel-wise CorrDrop，它们分别在 spatial dimension 和 channel dimension 自适应地丢弃 features。如图 1 所示，与传统的 dropout 和 DropBlock 遭遇 under-/over-dropping  问题相比，我们的 <strong><font color = purple>CorrDrop 通过丢弃相关性较低的区域（part of less correlated regions）来生成自适应掩膜（adaptive  masks）</font></strong>。图像分类的大量实验表明，在公共数据集上不同的 CNNs 架构下，，我们的 CorrDrop 始终优于 dropout he DropBlock。此外，我们也证明了我们的 CorrDrop 在其他计算机视觉任务（如如目标定位）中也能很好地正则化 CNN 模型。</p>\n<p>这项工作的初步版本已经作为会议版本【Corrdrop: Correlation based dropout for convolutional neural networks _ICASSP_2020】呈现出来。在这个扩展版本中，我们包含了额外的内容，包括 the channel-wise CorrDrop，更多的消融实验，最先进的 CNNs 实验和额外的视觉任务。主要贡献可以总结如下：</p>\n<ul>\n<li>我们提出了 Correlation based structural dropout（CorrDrop），它丢弃了 feature maps 中不太相关的特征（the less correlated features），这缓解了以前的 dropout 变体以随机方式丢弃 features 的 under-/over-dropping 问题。</li>\n<li>针对 feature map 中 spatial-wise 和 channel-wise features，提出了相应的 Spatial-wise CorrDrop(SCD) 和 Channel-wise CorrDrop(CCD)。实验结果表明，<strong><font color = purple>它们的互补性在于 SCD 在简单数据集（如 CIFAR-10 和 SVHN）上表现良好，而 CCD 在复杂数据集（CIFAR-100 和 TinyImageNet）上表现出色。</font></strong></li>\n<li>在各种数据集，架构和视觉任务上的大量实验表明，我们的方法可以得到持续的改进。</li>\n</ul>\n<p>这篇文章剩余部分组织如下。第 2 节简要回顾了深度学习中正则化方法和 注意力机制的相关研究成果。在第 3 节中，我们详细介绍了 CorrDrop 。第 4 节给出了实验结果。最后，我们在第 5 节得出结论。</p>\n</blockquote>\n<h2 id=\"3-methodology\"><a class=\"markdownIt-Anchor\" href=\"#3-methodology\"></a> 3. Methodology</h2>\n<blockquote>\n<p>由于 under-/over-dropping 问题，大多数现有的基于 dropout 的方法在正则化 CNNs 方面受到限制。通过利用特征的相关性，我们提出一种有效的 structural dropout： correlation-based dropout（CorrDrop），它根据判别信息（discriminative information）自适应地丢弃 feature units，并且可以有效地正则化 CNNs。考虑到 CNNs 的 spatial-wise feature correlation 和 channel-wise feature correlation，我们进一步推导出 CorrDrop 的两种变体，即： Spatial-wise CorrDrop 和 Channel-wise CorrDrop。这两种变体的流程如图 2 和 图 3 所示。在下面的部分中，我们首先描述基于特征正交性（feature orthogonality）的特征相关性（feature correlation）的计算。然后，我们根据 correlation map  来采样 mask。最后，我们说明了 Spatial-wise CorrDrop 和 Channel-wise CorrDrop 的策略。</p>\n</blockquote>\n<h3 id=\"31-feature-correlation-calculation\"><a class=\"markdownIt-Anchor\" href=\"#31-feature-correlation-calculation\"></a> 3.1. Feature correlation calculation</h3>\n<blockquote>\n<p>与以往随机丢弃 feature units  的方法不同，我们试图根据 feature correlation 来自适应地丢弃 feature units，这反映了判别信息（discriminative information）。<strong><font color = purple>最近的研究【Learning deep features for discriminative localization _CVPR_2016】【Grad–cam: Visual explanations from deep networks via gradient-based localization _ICCV_2017】表明，目标的判别区域（discriminative regions）将有更高的特征相关性（feature correlations）。</font></strong> 这些观察让我们做出基本假设，即通过丢弃那些 low-correlated features 可以更有效地正则化 CNNs。  <strong><font color = purple>为了表示 feature correlation，我们使用特征正交性（feature orthogonality）的度量，如之前的工作【Improved training of convolutional filters _CVPR_2019】。</font></strong> 给定 feature matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>a</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>a</mi><mi>N</mi></msub><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A = [a_1, ..., a_N]^T \\in \\mathcal{R}^{N \\times K}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 是 feature units  的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span> 是 feature dimension。correlation 的计算可以描述如下：</p>\n<img src=CorrDrop_e1.png width=50% />\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|.|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord\">.</span><span class=\"mord\">∣</span></span></span></span> 表示绝对值运算，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">I</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">I</span></span></span></span> 是一个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N \\times N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的单位矩阵。我们首先对  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span>  的每一行进行归一化（normalize），根据特征正交性（feature orthogonality）计算 correlation scores。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 是一个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N \\times N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 的矩阵，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">P_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 行。 a single unit 的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> 行的非对角元素表示所有其他 feature units  的投影（Off-diagonal elements of a row of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span> for a single unit denote projection of all the other feature units）。每行的平均值表示每个 unit 的 correlation score。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">F_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的值越高该 unit 与其他 unit 高度相关。</p>\n</blockquote>\n<h3 id=\"32-correlation-based-dropout-mask-sampling\"><a class=\"markdownIt-Anchor\" href=\"#32-correlation-based-dropout-mask-sampling\"></a> 3.2. Correlation based dropout mask sampling</h3>\n<blockquote>\n<p>为了根据 feature correlation 自适应地丢弃 units，我们根据 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 中的值为每个 unit 分配 丢弃概率（dropout probability）。一般情况下，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">F_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 的值越大，我们的丢弃概率越小。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span></span></span></span> 中第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个 feature unit 的丢弃概率可以表示为：</p>\n<img src=CorrDrop_e4.png width=50% />\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> 表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 中 feature unit 的索引。为了确保丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>γ</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\gamma_i \\in (0,1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335400000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>，我们将每个 unit 的 correlation score 进行归一化。</p>\n<p>基于丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>γ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\gamma_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> ，从伯努利分布中采样 dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">M \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<img src=CorrDrop_e5.png width=50% />\n<p>经验上，类似于其他 dropout 变体，一个超参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 被引入以确保我们的 CorrDrop 不会丢弃太多 feature units。利用基于 correlation 的 dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>，我们调整 keep probability 并生成另一个 mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">B \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。当两个 masks 对应的值都为 0 时， the units 被丢弃，并得到 the final dropout mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>S</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">S \\in \\mathcal{R}^{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。CorrDrop 的 final dropout mask 可制定为：</p>\n<img src=CorrDrop_e6.png width=50% />\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">numel(M)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mclose\">)</span></span></span></span> 计算 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 的 units 数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">sum(M)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">m</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mclose\">)</span></span></span></span> 计算值为 1 的 units 数量。</p>\n</blockquote>\n<blockquote>\n<img src=CorrDrop_f2.png width=100% />\n<p>图 2. Spatial-wise CorrDrop 的过程。1）通过 spatial-wise  local average pooling 对前一层的 feature maps 进行下采样，kernel size 和步长为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>，用于局部特征收集和降维（local features gathering and dimension reduction）。2）基于特征正交性（feature orthogonality） 计算 correlation map，并从具有自适应丢弃概率的伯努利分布中采样 dropout mask。3）通过最近邻上采样生成 CorrDrop mask。4）通过对 CorrDrop mask 和 original feature map 进行逐元素相乘得到 regularized feature 正则化特征。</p>\n<img src=CorrDrop_f3.png width=100% />\n<p>图 3. Channel-wise CorrDrop  的过程。1）基于 correlation calculation 计算 correlation vector。2）根据 correlation vector 对 CorrDrop mask 进行采样，即相关越少的通道（the less correlated channels）越容易被丢弃。3）将 CorrDrop mask 与 original feature map 进行逐通道相乘。</p>\n</blockquote>\n<h3 id=\"33-spatial-wise-corrdrop\"><a class=\"markdownIt-Anchor\" href=\"#33-spatial-wise-corrdrop\"></a> 3.3 Spatial-wise CorrDrop</h3>\n<blockquote>\n<p>在空间维度，我们假设高度相关的单元（highly correlated units）构成 feature maps 中的判别部分（discriminative parts），这些判别部分应以较高的概率保留。给定中间第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层的 feature maps 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>v</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>v</mi><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N = H \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是 feature map 中的 units 的数量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是 channels 的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别表示 feature map  的高和宽。每一行 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">v_i^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 表示一个 unit 的 feature vector。<strong><font color = green>由于 CNNs 中的特征是局部相关的，所以在 feature map 中丢弃单个 unit 效果不太好【Dropblock: A regularization method for convolutional networks _NIPS_2018】</font></strong> 。继之前的 DropBlock 在 feature map 中丢弃连续区域（continuous regions）的工作之后，我们进一步考虑每个局部区域的相关性和丢弃单元块（drop blocks of units）。为了获得一个 structural mask，我们首先通过 local average pooling 收集 feature map 中的局部信息，同时降低 feature map 的维度以加快相关性计算（correlation calculation）。当将 block 的大小设置为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 时，我们在 feature map 上进行 local average pooling， kernel size 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>，步长为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>。具体来说，我们在每个 feature map 中从左到右、从上到下扫描每个大小为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k \\times k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> 的 block，并计算每个 block 的激活值的平均值，可以描述为：</p>\n<img src=CorrDrop_e9.png width=50% />\n<p>得到的 feature map 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><msup><mi>N</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)&#x27;} \\in \\mathcal{R}^{N&#x27; \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.98158em;vertical-align:-0.0391em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>N</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">N&#x27; = H&#x27; \\times W&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.835222em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mfrac><mi>H</mi><mi>k</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">H&#x27; = ceil(\\frac{H}{k})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.217331em;vertical-align:-0.345em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>c</mi><mi>e</mi><mi>i</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mfrac><mi>W</mi><mi>k</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">W&#x27; = ceil(\\frac{W}{k})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.217331em;vertical-align:-0.345em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.872331em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span>。丢弃概率 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 相应调整为：</p>\n<img src=CorrDrop_e10.png width=50% />\n<p>通过下采样 feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)&#x27;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，我们采样 corrdrop mask 为：</p>\n<img src=CorrDrop_e11.png width=50% />\n<p>其中， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Phi(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span> 是如公式(1)-(3) 所示的特征相关性计算函数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Ψ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Psi(.)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ψ</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span> 表示如公式(4)-(8)所示的 dropout mask sampling operation。为了生成 structural mask，我们采用最近邻上采样的方法将 corrdrop mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><msup><mo stretchy=\"false\">)</mo><mo mathvariant=\"normal\">′</mo></msup></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><msup><mi>H</mi><mo mathvariant=\"normal\">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant=\"normal\">′</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)&#x27;} \\in \\mathcal{R}^{H&#x27; \\times W&#x27;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2158720000000003em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0992800000000003em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\"><span class=\"mclose mtight\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.94248em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.94248em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8278285714285715em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 上采样到 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)} \\in \\mathcal{R}^{H \\times W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.161392em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">S_s^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.161392em;vertical-align:-0.11659199999999997em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.5834080000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.11659199999999997em;\"><span></span></span></span></span></span></span></span></span></span> 中的每一个 zero entry 将被扩展为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k \\times k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span> blocks。因此，square regions of units 将被丢弃。最后，将 the spatial-wise corrdrop mask 与 the original feature maps <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的每一个通道相乘，并 masks out 掉部分 feature regions，其表示为：</p>\n<img src=CorrDrop_e12.png width=50% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">⊙</span></span></span></span> 表示逐点相乘运算。过程如图 2 所示。采用这种方式，我们根据局部信息来计算 feature correlation，并丢弃具有 small average correlation  的 square of regions。</p>\n</blockquote>\n<h3 id=\"34-channel-wise-corrdrop\"><a class=\"markdownIt-Anchor\" href=\"#34-channel-wise-corrdrop\"></a> 3.4. Channel-wise CorrDrop</h3>\n<blockquote>\n<p>除了 spatial-wise features 之外，值得注意的是，<strong><font color = purple>每个 CNN filter 可以检测到输入数据的不同模式，即 channel-wise features 对应不同的语义模式。</font></strong> <strong><font color = green>【Weighted channel dropout for regularization of deep convolutional neural network_AAAI_2019】中的工作表明，more semantic feature channels 具有 more class-specific，其中包括一些冗余和较少激活的通道。</font></strong> 因此，我们尝试基于 channel-wise feature correlation 来丢弃那些不相关的特征通道并提高泛化能力，从而产生我们的 Channel-wise CorrDrop。类似于 Spatial-wise CorrDrop，给定中间第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层的 feature maps 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>v</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>v</mi><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)} = [v_1^{(l)}, ..., v_N^{(l)}]^T \\in \\mathcal{R}^{N \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>，其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N = H \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是 feature map 中的 unit 数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是通道数，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别是 feature map 的高和宽。我们首先将第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 层 feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>V</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> reshape 为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>U</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mo stretchy=\"false\">[</mo><msubsup><mi>u</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msubsup><mi>u</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">]</mo><mi>T</mi></msup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">U^{(l)} = [u_1^{(l)}, ..., u_C^{(l)}]^T \\in \\mathcal{R}^{C \\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.26630799999999993em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。同理， the channel-wise dropout mask 计算为：</p>\n<img src=CorrDrop_e13.png width=50% />\n<p>其中， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">p</span></span></span></span> 是 dropout probability，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>F</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">F_C^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 是 correlation map，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">S_C^{(l)} \\in \\mathcal{R}^C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span> 是 corrdrop mask。按如下方式执行 the channel-wise corrdrop：</p>\n<img src=CorrDrop_e14.png width=50% />\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.66666em;vertical-align:-0.08333em;\"></span><span class=\"mord\">⊙</span></span></span></span> 指逐通道相乘，如果 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>S</mi><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">S_C^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.29353099999999993em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29353099999999993em;\"><span></span></span></span></span></span></span></span></span></span> 中的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个元素为0则 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>U</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">U^{(l)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 的第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span></span></span></span> 个channel 将被置 0。</p>\n</blockquote>\n<h2 id=\"5-conclusions-2\"><a class=\"markdownIt-Anchor\" href=\"#5-conclusions-2\"></a> 5. Conclusions</h2>\n<blockquote>\n<p>在本文中，我们提出一种新颖且有效的 structural dropout 来有效地正则化 CNNs。与现有的正则化方法会遇到 CNNs 的 under/over-dropping 问题不同，我们的方法通过基于 spatial and channel dimensions 的feature correlation 丢弃 feature 来解决这些问题。大量实验表明我们的方法在不同的机器视觉额任务，网络架构和数据集上优于其他同类方法。此外， the feature activation map 的可视化让我们了解到我们的方法可以强制模型学习更紧凑的表示（learn more compact representations）。除了图像分类任务以外，我们还验证了我们的方法在弱监督目标定位方面的有效性，并揭示了我们的方法在各种计算机视觉任务中的潜在应用。我们还表明，我们的方法可以很容易地插入普通的 CNNs 架构以正则化 CNNs。我们相信我们提出的 CorrDrop 可以作为计算机视觉社区中地通用正则化技术。</p>\n<p>在未来的工作中，我们将进一步研究我们的方法在其他计算机视觉任务中的有效性，例如目标检测，语义分割等等。另一方面，图 8 中的 feature maps  的可视化启发我们继续利用特征的相关性来进一步提高网络的表征能力。</p>\n</blockquote>\n<h1 id=\"channel-dropblock-an-improved-regularization-method-for-fine-grained-visual-classification_2021\"><a class=\"markdownIt-Anchor\" href=\"#channel-dropblock-an-improved-regularization-method-for-fine-grained-visual-classification_2021\"></a> Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification_2021</h1>\n<h2 id=\"abstract-9\"><a class=\"markdownIt-Anchor\" href=\"#abstract-9\"></a> Abstract</h2>\n<blockquote>\n<p>在细粒度视觉分类（FGVC）任务中，从同一超类别（如鸟）中对一个目标的子类别进行分类，<strong><font color = green>高度依赖于多个判别特征</font></strong>。<strong><font color = blue>现有方法主要通过引入  attention mechanisms 来定位判别部分或特征编码方法以弱监督的方式提取高度参数化的特征来解决这个问题</font></strong>。在这项工作中，我们提出了一种名为 Channel DropBlock（CDB）的轻量级但有效的正则化方法，并结合两个可选的相关度量（alternative correlation metrics）来解决这个问题。<strong><font color = purple>关键思想是在训练阶段随机屏蔽（mask out）一组相关通道，从协同适应中破坏特征，从而增强特征表示（enhance feature representations）</font></strong>。在三个基准 FGVC 数据集上的大量实验表明，CDB 有效地提高了性能。</p>\n</blockquote>\n<blockquote>\n<img src=CDB_f1.png width=100% />\n<p>图1 CDB block 的说明。通道相关矩阵（the channel correlation matrix）是根据不同的度量生成的。然后，通过对 input feature map 应用 drop mask ，将一个通道及其对应的视觉组（its corresponding visual group）随机丢弃，丢弃的元素为 0，否则为1。</p>\n</blockquote>\n<h2 id=\"1-introduction-7\"><a class=\"markdownIt-Anchor\" href=\"#1-introduction-7\"></a> 1 Introduction</h2>\n<blockquote>\n<p>本论文贡献总结如下：</p>\n<p>1）我们通过提出一种新颖的轻量级正则化结构来**<font color = purple>解决 FGVC 任务中判别特征学习（discriminative feature learning）的挑战</font>**，该结构丢弃一组相关通道来激发网络增强特征表示，从而提取更多的判别模式（discriminative patterns）。</p>\n<p>2）我们提出两个指标来度量不同特征通道之间的成对相关性，这可以帮助我们深入了解特征通道。</p>\n<p>3）我们在三个流行的细粒度基准数据集上进行了大量实验，结果表明，当应用于基线网络或集成到现有方法时，所提出的 CDB 显著提高了 FGVC 的性能。</p>\n</blockquote>\n<blockquote>\n<img src=CDB_a1.png width=100% />\n</blockquote>\n<h2 id=\"3-cdb-channel-dropblock\"><a class=\"markdownIt-Anchor\" href=\"#3-cdb-channel-dropblock\"></a> 3 CDB: Channel DropBlock</h2>\n<blockquote>\n<p>在本节中，我们介绍了所提出的 Channel DropBlock（CDB）的细节。它是一种基于 dropout 的正则化技术，可以很容易地应用于分类模型的 convolutional feature maps，以改善 feature representations。我们首先描述动机 motivation，并与相关方法进行比较（第 3.1 节）。然后我们描述 Channel DropBlock 算法，该算法基于 channel correlation matrix（第 3.2 和 3.3 节）丢弃 correlated channel groups。</p>\n</blockquote>\n<h3 id=\"31-motivation\"><a class=\"markdownIt-Anchor\" href=\"#31-motivation\"></a> 3.1 Motivation</h3>\n<blockquote>\n<p>正如之前的工作【】所示，<strong><font color = green>卷积特征的每个通道对应一个视觉模式</font>。<strong>然而，由于模式之间的共同适应性，只有部分模式有助于最终预测，这将降低推理准确性，尤其是当子类别接近且难以区分时（例如，在 FGVC 任务中）。虽然 dropout 能有效地破坏特征中的协同适应性，但它对卷积特征通道的效果较差，因为这些通道是成对相关的，并且如果我们单独丢弃通道，关于输入的模式仍然可以发送到下一层。这种直觉建议我们屏蔽一组相关的通道（mask out a correlated group of channels）而不是当个通道（a single channel），以鼓励模型学习更多判别部分（discriminative parts）。</strong><font color = purple>CDB 的主要动机是破坏协同适应性，诱导模型充分利用更具判别性的特征（more discriminative features）。这是通过随机屏蔽整个相关通道组来实现的，这仅仅有助于最终预测的一个视觉证据。</font></strong></p>\n<p>我们最初开发 CDB 作为一种 attention-based  的方法，专门从 the input feature 中移除 the most important channel groups。这条线索类似于 ADL 的想法【】，因为我们开发了一个重要的分支和一个 dropout 分支，它们是随机选择的，并以对抗方式突出重要通道（highlight important channels）并移除最大激活的组（remove maximally activated group）。我们将这个实验作为消融研究进行，与随机选择的实验相比，改进有限，因为随机的实验可以给出更多的遮挡组合，并且更有可能破坏通道之间的协同适应（destruct co-adaptations between channels）。<strong><font color  = orange>我们的所有实验都专注于随机选择的 Channel DropBlock。</font></strong></p>\n<p>与 MA-CNN【】在 the final feature map 上聚类通道（clusters channels）并为每个聚类设置单独的分类器相比，本文提出的 CDB 被设计作为一个正则化块（a regularization block），更灵活地应用于任何分类模型的 convolutional feature maps。</p>\n<p>相比于 SpatialDropout【】，<strong><font color = purple>CDB 强调通道之间是相互关联的，视觉证据仍然可以通过单独的 dropout 发送到下一层。</font></strong></p>\n<p>与 DropBlock 【】在空间上丢弃相关单元（drops correlated units）相比，<strong><font color = purple>提出的 CDB 计算逐通道的相关性（calculates correlations channel-wise），并且可以通过我们提供的两个独特的相关性度量（two unique correlation metrics）来捕获更精确的视觉证据。</font></strong></p>\n</blockquote>\n<h3 id=\"32-channel-dropblock-algorithm\"><a class=\"markdownIt-Anchor\" href=\"#32-channel-dropblock-algorithm\"></a> 3.2 Channel DropBlock Algorithm</h3>\n<blockquote>\n<p>Algorithm 1 和 Figure 1 展示了  the Channel DropBlock 的主要过程。具体来说，CDB 的输入是 a convolutional feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F \\in \\mathcal{R}^{C \\times H \\times W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 是通道的数量，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 分别表示 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 的高和宽。<strong><font color = purple>我们通过计算 each feature channel 之间的两两相似度来获得 the correlation matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">M \\in \\mathcal{R}^{C \\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span></font></strong>(描述在 3.3 节)。为了获得  the drop mask，CDB 首先从 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span> 中随机选择一行，通过将 top <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 个最相关的元素设置为0，其它元素设置为1，来生成 the drop mask <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>M</mi><mi>d</mi></msub><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mi>C</mi></msup></mrow><annotation encoding=\"application/x-tex\">M_d \\in \\mathcal{R}^{C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>。然后，通过广播乘法（broadcasting multiplication）将 the drop mask 应用于 the input feature map。通过这种方式，连续组中的特征（features in a contiguous group）被一起丢掉，这隐藏了一个特定的判别模式，并鼓励模型学习其他有助于最终预测的判别信息（discriminative information）。与 dropout 类似，所提出的 CDB 仅在具有归一化的训练阶段起作用，在推理阶段不涉及额外的参数和计算成本。</p>\n<p>CDB 有两个主要的超参数：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>。参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 表示 CDB 应用的位置，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 控制 dropped group 中的通道数量。</p>\n<p>**<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 的影响：**随着 CNN 结构越来越深，高层神经元对整个图像反应强烈，语义丰富，但不可避免地会丢失来自小的判别区域的细节信息（detailed information from small discriminative regions）。由于 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>r</mi><msub><mi>t</mi><mi>p</mi></msub><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">insert_pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9456279999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span></span></span></span> 的设置不同， the input feature map 的信息也不同。在我们的实验中，我们完成了一项消融实验（图 Table 2 所述），将提出的 CDB block 应用于 CNN 的不同的层。</p>\n<p>**设置 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 的值：**另一个超参数涉及到我们何时将 correlated channels 聚合成 group。这里我们将 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 定义为进行 CDB 时被丢弃的组中（a dropped group）通道的百分比。在实践中，不同的 correlation metrics 会导致不同的簇数 (cluster numbers) 和 each cluster 中的通道数，因此，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> 的设置与我们选择的 correlation metrics 不同。</p>\n<img src=CDB_f2.png width=100% />\n<p>图2：channel correlation metrics 的说明：(a) max activation，将通道分组为有区别的局部区域（discriminative local region）; (b) bilinear pooling metric 双线性池化度量，根据 visual pattern 视觉模式对 channel 进行分组。</p>\n</blockquote>\n<h3 id=\"33-channel-correlation\"><a class=\"markdownIt-Anchor\" href=\"#33-channel-correlation\"></a> 3.3 Channel Correlation</h3>\n<blockquote>\n<p>理想情况下， a correlation metric 应该是对称的，并且可以将 feature channels 聚集到不同的 visual pattern groups。在本文中，我们研究了两个候选 metric 来评估 channel 之间的 correlation。</p>\n<p><strong>Max activation metric.</strong> 为了将 feature channels 分成 group，一个直观的想法是将 feature channels 分成不同的焦点局部区域（different focused local regions）。<strong><font color  = purple>受 MA-CNN【】思想的启发，我们将最大激活位置接近的通道视为一个 pattern group（we treat channels with close maximal activation position as one pattern group）。</font></strong> 我们使用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3 \\times 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span> average pooling 来平滑 feature maps 并使用 argmax(.) 操作来获得 each feature channel 中峰值响应的坐标，这将 the input feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 转换为位置矩阵 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P \\in \\mathcal{R}^{C \\times 2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span>，由下式给出：</p>\n<img src=CDB_e1.png width=60% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>t</mi><mi>x</mi><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">t_x^i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.071664em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.824664em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">x</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span>，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>t</mi><mi>y</mi><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">t_y^i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2077719999999998em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.824664em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span></span></span></span> 是第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.849108em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.849108em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\">h</span></span></span></span></span></span></span></span></span></span></span></span> 个 channel 的峰值响应的坐标。然后计算每个激活位置之间逐对的欧氏距离并获得 the correlation matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span></span></span></span>：</p>\n<img src=CDB_e2.png width=60% />\n<p>在该度量中，feature channels 被分组成 discriminative local regions 具有区别性的局部区域。此外，它是一个无参数的度量，不涉及可学习的参数。 Figure 2(a) 展示了 the max activation metric 的过程。</p>\n<p><strong>Bilinear pooling metric.</strong> 我们还研究了一个基于 bilinear pooling operator 的 correlation metric【】，它计算归一化余弦距离来度量通道相似性（channel similarities）。该方法将 the input feature map <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding=\"application/x-tex\">F</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span></span></span></span> 重构（reshape） 为一个形状为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">C \\times HW</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 的矩阵，记为 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant=\"script\">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X \\in \\mathcal{R}^{C \\times HW}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mbin mtight\">×</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></span></span></span></span></span></span></span>。然后通过 a normalization function 和 a bilinear pooling operator 对 reshaped matrix 重构后的矩阵进行输入，得到 channels 之间的 spatial relationship：</p>\n<img src=CDB_e3.png width=60% />\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N(.)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span></span> 表示矩阵第 2 维度上的 L2 normalization function。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">XX^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span> 是 the homogeneous bilinear feature 齐次双线性特征。相比于 the max activation metric，bilinear pooling metric 中的 each channel group 表示 one specific visual pattern。同样，在训练阶段和推理阶段都不涉及可训练的参数。Figure 2(b) 展示了 the bilinear pooling metric 的过程。</p>\n</blockquote>\n<h2 id=\"5-conclusion-2\"><a class=\"markdownIt-Anchor\" href=\"#5-conclusion-2\"></a> 5 Conclusion</h2>\n<blockquote>\n<p>本文引入了一种新颖的正则化技术，Channel DropBlock（CDB），该技术通过相关性度量对通道进行聚类，并在训练阶段随机丢弃一个相关通道组（a correlated channel group），从而破坏协同适配的特征通道（destructs feature channels from co-adaptations）。我们证明，与现有的 FGVC 方法相比，CDB 在增强特征表示和提取多种判别模式方面更加轻量级和有效。我们在三个经过广泛测试的细粒度数据集上进行了实验，验证了所提出方法的优越性。未来工作的两个特别有趣的方向包括探索具有自适应大小的通道分组方法，以及使用综合指标度量通道相关性。</p>\n</blockquote>\n"},{"title":"Poem","top":false,"cover":false,"toc":true,"date":"2022-06-18T02:17:47.000Z","password":null,"summary":null,"description":null,"_content":"\n# 王国维\n\n> $\\blacksquare$ 王国维的人生 “三境界”：\n>\n> 古今之成大事业、大学问者，必经过三种之境界。\n>\n> “昨夜西风凋碧树。独上高楼，望尽天涯路。”此第一境也。\n>\n> “衣带渐宽终不悔，为伊消得人憔悴。”此第二境也。\n>\n> “众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”此第三境也。\n\n\n\n> 仲夏的凉风吹走炙热的焦虑，橙色的黄昏相拥薄荷的黎明，\n>\n> 我以夏天的名义穿上吊带，乘着晚风给蜜儿送上祝福~\n>\n> 你且听这荒唐，春秋走来一步步；\n>\n> 你且迷这风浪，永远二十赶朝暮。将昨日事，归欢喜处。\n>\n> 寒来暑往，四序迁流，与世界交手的第 26 年，祝宝贝光彩依旧，兴致盎然。\n>\n> 无事绊心弦，所念皆所愿， 26 岁 的雨儿，幸会~\n>\n> 等了362天，终于终于等到跟你同岁的26了哇，我的宝贝不过是又长大了一岁的美女罢了~~~\n>\n> 好久好久没有见面了，我有好多好多心里话想跟你说，，可是，一时却又不知道说啥了，不瞒你说，这是我打了好几遍草稿的祝福语，语短情长，且听我慢慢道来。\n>\n> 首先，我谨代表党和人民热烈祝贺你 生日快乐，并希望你与时俱进，再接再厉，期盼早日迎接下一次生日的到来~\n>\n> 其次，我联合帕瓦特奥特曼，迪迦奥特曼，戴拿奥特曼，盖亚奥特曼，高斯奥特曼，赛文奥特曼，麦克斯奥特曼，杰诺奥特曼，希卡利奥特曼，塞罗奥特曼，银河奥特曼，奥特之父，奥特之母，奥特之王等40位奥特曼祝你生日快乐~\n>\n> 最后，别人都祝你生日快乐，我想了想，身为屎凤，我还是祝你拉屎通畅吧，嘻嘻嘻嘻~\n>\n> 希望我的宝贝永远有一颗少女心，不一定要喜欢粉红色，不一定要有一大堆玩偶，不一定要在打雷的时候尖叫着躲回被子里，，而是不管宝贝懂得多少成人世界的规则，不管蜜儿被这个世界伤害了多少回，第二天早上，太阳升起来的时候，宝贝依旧愿意去好奇，去拥抱，去相信，去发现，去等待，像一个少女一样去爱一个人。\n>\n> 哈哈哈，于是乎，关于爱情，我想告诉蜜儿，宝贝一定要相信，未来和你共度一生的那个人，其实在与你相同的时间里，也忍受着同样的孤独，那个人也一定怀着满心的期待，马不停蹄的赶来和你碰面。\n>\n> 于是，有一天，月亮照回湖心，星星跌进深海，野鹤奔向闲云，他轻轻敲开心门步入你~~\n>\n> 宝贝现在需要做的就是每天开开心心的学习，开开心心的玩儿，衷心的祈祷蜜儿顺利毕业，继续攻读博后，成为自己想成为的人，不负青春不负此生~\n>\n> 最后的最后，\n>\n> 亲爱的宝贝雨儿，秋刀鱼会过期，肉罐头会过期，连保鲜纸都会过期，但是我对你的爱永远永远不会过期，再次祝蜜儿生日快乐~\n>\n> 我的宝贝~深深的话我们浅浅地说，长长的路我们慢慢地走，祝我的蜜儿，顺风顺水，扶摇直上，百事无忌，平安喜乐，万事顺心，前程似锦，有所爱亦被爱，不负遇见，不谈亏欠~\n\n\n\n","source":"_posts/Poem.md","raw":"---\ntitle: Poem\ntop: false\ncover: false\ntoc: true\ndate: 2022-06-18 10:17:47\npassword:\nsummary:\ndescription:\ncategories: Poem\ntags: Poem\n\n---\n\n# 王国维\n\n> $\\blacksquare$ 王国维的人生 “三境界”：\n>\n> 古今之成大事业、大学问者，必经过三种之境界。\n>\n> “昨夜西风凋碧树。独上高楼，望尽天涯路。”此第一境也。\n>\n> “衣带渐宽终不悔，为伊消得人憔悴。”此第二境也。\n>\n> “众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”此第三境也。\n\n\n\n> 仲夏的凉风吹走炙热的焦虑，橙色的黄昏相拥薄荷的黎明，\n>\n> 我以夏天的名义穿上吊带，乘着晚风给蜜儿送上祝福~\n>\n> 你且听这荒唐，春秋走来一步步；\n>\n> 你且迷这风浪，永远二十赶朝暮。将昨日事，归欢喜处。\n>\n> 寒来暑往，四序迁流，与世界交手的第 26 年，祝宝贝光彩依旧，兴致盎然。\n>\n> 无事绊心弦，所念皆所愿， 26 岁 的雨儿，幸会~\n>\n> 等了362天，终于终于等到跟你同岁的26了哇，我的宝贝不过是又长大了一岁的美女罢了~~~\n>\n> 好久好久没有见面了，我有好多好多心里话想跟你说，，可是，一时却又不知道说啥了，不瞒你说，这是我打了好几遍草稿的祝福语，语短情长，且听我慢慢道来。\n>\n> 首先，我谨代表党和人民热烈祝贺你 生日快乐，并希望你与时俱进，再接再厉，期盼早日迎接下一次生日的到来~\n>\n> 其次，我联合帕瓦特奥特曼，迪迦奥特曼，戴拿奥特曼，盖亚奥特曼，高斯奥特曼，赛文奥特曼，麦克斯奥特曼，杰诺奥特曼，希卡利奥特曼，塞罗奥特曼，银河奥特曼，奥特之父，奥特之母，奥特之王等40位奥特曼祝你生日快乐~\n>\n> 最后，别人都祝你生日快乐，我想了想，身为屎凤，我还是祝你拉屎通畅吧，嘻嘻嘻嘻~\n>\n> 希望我的宝贝永远有一颗少女心，不一定要喜欢粉红色，不一定要有一大堆玩偶，不一定要在打雷的时候尖叫着躲回被子里，，而是不管宝贝懂得多少成人世界的规则，不管蜜儿被这个世界伤害了多少回，第二天早上，太阳升起来的时候，宝贝依旧愿意去好奇，去拥抱，去相信，去发现，去等待，像一个少女一样去爱一个人。\n>\n> 哈哈哈，于是乎，关于爱情，我想告诉蜜儿，宝贝一定要相信，未来和你共度一生的那个人，其实在与你相同的时间里，也忍受着同样的孤独，那个人也一定怀着满心的期待，马不停蹄的赶来和你碰面。\n>\n> 于是，有一天，月亮照回湖心，星星跌进深海，野鹤奔向闲云，他轻轻敲开心门步入你~~\n>\n> 宝贝现在需要做的就是每天开开心心的学习，开开心心的玩儿，衷心的祈祷蜜儿顺利毕业，继续攻读博后，成为自己想成为的人，不负青春不负此生~\n>\n> 最后的最后，\n>\n> 亲爱的宝贝雨儿，秋刀鱼会过期，肉罐头会过期，连保鲜纸都会过期，但是我对你的爱永远永远不会过期，再次祝蜜儿生日快乐~\n>\n> 我的宝贝~深深的话我们浅浅地说，长长的路我们慢慢地走，祝我的蜜儿，顺风顺水，扶摇直上，百事无忌，平安喜乐，万事顺心，前程似锦，有所爱亦被爱，不负遇见，不谈亏欠~\n\n\n\n","slug":"Poem","published":1,"updated":"2022-07-13T02:07:34.508Z","_id":"cl4m00udc0000zoul0eqy5b48","comments":1,"layout":"post","photos":[],"link":"","content":"<html><head></head><body><h1 id=\"王国维\"><span class=\"post-title-index\">1. </span><a class=\"markdownIt-Anchor\" href=\"#王国维\"></a> 王国维</h1>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 王国维的人生 “三境界”：</p>\n<p>古今之成大事业、大学问者，必经过三种之境界。</p>\n<p>“昨夜西风凋碧树。独上高楼，望尽天涯路。”此第一境也。</p>\n<p>“衣带渐宽终不悔，为伊消得人憔悴。”此第二境也。</p>\n<p>“众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”此第三境也。</p>\n</blockquote>\n<blockquote>\n<p>仲夏的凉风吹走炙热的焦虑，橙色的黄昏相拥薄荷的黎明，</p>\n<p>我以夏天的名义穿上吊带，乘着晚风给蜜儿送上祝福~</p>\n<p>你且听这荒唐，春秋走来一步步；</p>\n<p>你且迷这风浪，永远二十赶朝暮。将昨日事，归欢喜处。</p>\n<p>寒来暑往，四序迁流，与世界交手的第 26 年，祝宝贝光彩依旧，兴致盎然。</p>\n<p>无事绊心弦，所念皆所愿， 26 岁 的雨儿，幸会~</p>\n<p>等了362天，终于终于等到跟你同岁的26了哇，我的宝贝不过是又长大了一岁的美女罢了~~~</p>\n<p>好久好久没有见面了，我有好多好多心里话想跟你说，，可是，一时却又不知道说啥了，不瞒你说，这是我打了好几遍草稿的祝福语，语短情长，且听我慢慢道来。</p>\n<p>首先，我谨代表党和人民热烈祝贺你 生日快乐，并希望你与时俱进，再接再厉，期盼早日迎接下一次生日的到来~</p>\n<p>其次，我联合帕瓦特奥特曼，迪迦奥特曼，戴拿奥特曼，盖亚奥特曼，高斯奥特曼，赛文奥特曼，麦克斯奥特曼，杰诺奥特曼，希卡利奥特曼，塞罗奥特曼，银河奥特曼，奥特之父，奥特之母，奥特之王等40位奥特曼祝你生日快乐~</p>\n<p>最后，别人都祝你生日快乐，我想了想，身为屎凤，我还是祝你拉屎通畅吧，嘻嘻嘻嘻~</p>\n<p>希望我的宝贝永远有一颗少女心，不一定要喜欢粉红色，不一定要有一大堆玩偶，不一定要在打雷的时候尖叫着躲回被子里，，而是不管宝贝懂得多少成人世界的规则，不管蜜儿被这个世界伤害了多少回，第二天早上，太阳升起来的时候，宝贝依旧愿意去好奇，去拥抱，去相信，去发现，去等待，像一个少女一样去爱一个人。</p>\n<p>哈哈哈，于是乎，关于爱情，我想告诉蜜儿，宝贝一定要相信，未来和你共度一生的那个人，其实在与你相同的时间里，也忍受着同样的孤独，那个人也一定怀着满心的期待，马不停蹄的赶来和你碰面。</p>\n<p>于是，有一天，月亮照回湖心，星星跌进深海，野鹤奔向闲云，他轻轻敲开心门步入你~~</p>\n<p>宝贝现在需要做的就是每天开开心心的学习，开开心心的玩儿，衷心的祈祷蜜儿顺利毕业，继续攻读博后，成为自己想成为的人，不负青春不负此生~</p>\n<p>最后的最后，</p>\n<p>亲爱的宝贝雨儿，秋刀鱼会过期，肉罐头会过期，连保鲜纸都会过期，但是我对你的爱永远永远不会过期，再次祝蜜儿生日快乐~</p>\n<p>我的宝贝<sub>深深的话我们浅浅地说，长长的路我们慢慢地走，祝我的蜜儿，顺风顺水，扶摇直上，百事无忌，平安喜乐，万事顺心，前程似锦，有所爱亦被爱，不负遇见，不谈亏欠</sub></p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"王国维\"><a class=\"markdownIt-Anchor\" href=\"#王国维\"></a> 王国维</h1>\n<blockquote>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">■</mi></mrow><annotation encoding=\"application/x-tex\">\\blacksquare</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.675em;vertical-align:0em;\"></span><span class=\"mord amsrm\">■</span></span></span></span> 王国维的人生 “三境界”：</p>\n<p>古今之成大事业、大学问者，必经过三种之境界。</p>\n<p>“昨夜西风凋碧树。独上高楼，望尽天涯路。”此第一境也。</p>\n<p>“衣带渐宽终不悔，为伊消得人憔悴。”此第二境也。</p>\n<p>“众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”此第三境也。</p>\n</blockquote>\n<blockquote>\n<p>仲夏的凉风吹走炙热的焦虑，橙色的黄昏相拥薄荷的黎明，</p>\n<p>我以夏天的名义穿上吊带，乘着晚风给蜜儿送上祝福~</p>\n<p>你且听这荒唐，春秋走来一步步；</p>\n<p>你且迷这风浪，永远二十赶朝暮。将昨日事，归欢喜处。</p>\n<p>寒来暑往，四序迁流，与世界交手的第 26 年，祝宝贝光彩依旧，兴致盎然。</p>\n<p>无事绊心弦，所念皆所愿， 26 岁 的雨儿，幸会~</p>\n<p>等了362天，终于终于等到跟你同岁的26了哇，我的宝贝不过是又长大了一岁的美女罢了~~~</p>\n<p>好久好久没有见面了，我有好多好多心里话想跟你说，，可是，一时却又不知道说啥了，不瞒你说，这是我打了好几遍草稿的祝福语，语短情长，且听我慢慢道来。</p>\n<p>首先，我谨代表党和人民热烈祝贺你 生日快乐，并希望你与时俱进，再接再厉，期盼早日迎接下一次生日的到来~</p>\n<p>其次，我联合帕瓦特奥特曼，迪迦奥特曼，戴拿奥特曼，盖亚奥特曼，高斯奥特曼，赛文奥特曼，麦克斯奥特曼，杰诺奥特曼，希卡利奥特曼，塞罗奥特曼，银河奥特曼，奥特之父，奥特之母，奥特之王等40位奥特曼祝你生日快乐~</p>\n<p>最后，别人都祝你生日快乐，我想了想，身为屎凤，我还是祝你拉屎通畅吧，嘻嘻嘻嘻~</p>\n<p>希望我的宝贝永远有一颗少女心，不一定要喜欢粉红色，不一定要有一大堆玩偶，不一定要在打雷的时候尖叫着躲回被子里，，而是不管宝贝懂得多少成人世界的规则，不管蜜儿被这个世界伤害了多少回，第二天早上，太阳升起来的时候，宝贝依旧愿意去好奇，去拥抱，去相信，去发现，去等待，像一个少女一样去爱一个人。</p>\n<p>哈哈哈，于是乎，关于爱情，我想告诉蜜儿，宝贝一定要相信，未来和你共度一生的那个人，其实在与你相同的时间里，也忍受着同样的孤独，那个人也一定怀着满心的期待，马不停蹄的赶来和你碰面。</p>\n<p>于是，有一天，月亮照回湖心，星星跌进深海，野鹤奔向闲云，他轻轻敲开心门步入你~~</p>\n<p>宝贝现在需要做的就是每天开开心心的学习，开开心心的玩儿，衷心的祈祷蜜儿顺利毕业，继续攻读博后，成为自己想成为的人，不负青春不负此生~</p>\n<p>最后的最后，</p>\n<p>亲爱的宝贝雨儿，秋刀鱼会过期，肉罐头会过期，连保鲜纸都会过期，但是我对你的爱永远永远不会过期，再次祝蜜儿生日快乐~</p>\n<p>我的宝贝<sub>深深的话我们浅浅地说，长长的路我们慢慢地走，祝我的蜜儿，顺风顺水，扶摇直上，百事无忌，平安喜乐，万事顺心，前程似锦，有所爱亦被爱，不负遇见，不谈亏欠</sub></p>\n</blockquote>\n"}],"PostAsset":[{"_id":"source/_posts/GNN/GraphMatrix.jpg","slug":"GraphMatrix.jpg","post":"cl40nn32n0001mculdth9b62w","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/1.png","slug":"1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/2.png","slug":"2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/3.png","slug":"3.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/4.png","slug":"4.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/D2.png","slug":"D2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t1.png","slug":"t1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t2.png","slug":"t2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t3_1.png","slug":"t3_1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t3_2.png","slug":"t3_2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/5.png","slug":"5.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/e1.png","slug":"e1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t4_1.png","slug":"t4_1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t4_2.png","slug":"t4_2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/t4_3.png","slug":"t4_3.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/7.png","slug":"7.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/h1.png","slug":"h1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/dropout_f7.png","slug":"dropout_f7.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_a1.png","slug":"heuristic_a1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_a2.png","slug":"heuristic_a2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_e1.png","slug":"heuristic_e1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_t1.png","slug":"heuristic_t1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_f1.png","slug":"heuristic_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_f2.png","slug":"heuristic_f2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_f3.png","slug":"heuristic_f3.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/heuristic_f4.png","slug":"heuristic_f4.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/NSDropout_f1.png","slug":"NSDropout_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/NSDropout_e1.png","slug":"NSDropout_e1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/NSDropout_f2.png","slug":"NSDropout_f2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/StructuralDropout_f1.png","slug":"StructuralDropout_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/Clustering-basedAdaptiveDropout_f1.png","slug":"Clustering-basedAdaptiveDropout_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f1.png","slug":"CorrDrop_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e1.png","slug":"CorrDrop_e1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e4.png","slug":"CorrDrop_e4.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e5.png","slug":"CorrDrop_e5.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e6.png","slug":"CorrDrop_e6.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f2.png","slug":"CorrDrop_f2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_f3.png","slug":"CorrDrop_f3.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e10.png","slug":"CorrDrop_e10.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e11.png","slug":"CorrDrop_e11.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e12.png","slug":"CorrDrop_e12.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e13.png","slug":"CorrDrop_e13.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e14.png","slug":"CorrDrop_e14.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CorrDrop_e9.png","slug":"CorrDrop_e9.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_f1.png","slug":"CDB_f1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_a1.png","slug":"CDB_a1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_f2.png","slug":"CDB_f2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_e1.png","slug":"CDB_e1.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_e2.png","slug":"CDB_e2.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0},{"_id":"source/_posts/CNN-Regularization/CDB_e3.png","slug":"CDB_e3.png","post":"cl49vqqoy00001oul5k1wa4n1","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cl40nn32n0001mculdth9b62w","category_id":"cl40nn32t0003mculfa5jbacm","_id":"cl40nn32x0007mcul7u8a4kjt"},{"post_id":"cl40nn32z0008mculcmu852qe","category_id":"cl40nn332000amculfluz2pqy","_id":"cl40nn335000gmcul7rdwg7tv"},{"post_id":"cl40nn3300009mcularvafsm2","category_id":"cl40nn334000dmcula3e9ezux","_id":"cl40nn336000jmculfohbabxt"},{"post_id":"cl40nn333000cmcul9bwk4dzs","category_id":"cl40nn335000hmcul5n3ncmf5","_id":"cl40nn336000kmcul8td17v6z"},{"post_id":"cl49vqqoy00001oul5k1wa4n1","category_id":"cl49vqqs300011ould6nyhwjv","_id":"cl49vqqsb00041oulg00f5ctz"},{"post_id":"cl4m00udc0000zoul0eqy5b48","category_id":"cl4m00ue60001zoulfpu80wcl","_id":"cl4m00uec0004zoul1knvb71o"}],"PostTag":[{"post_id":"cl40nn32n0001mculdth9b62w","tag_id":"cl40nn32v0004mcul53gigx01","_id":"cl40nn32x0006mcul2x729nv7"},{"post_id":"cl40nn32z0008mculcmu852qe","tag_id":"cl40nn332000bmcul60iog4or","_id":"cl40nn335000fmcul3vbd6j0k"},{"post_id":"cl40nn3300009mcularvafsm2","tag_id":"cl40nn334000emcul6r21h5n8","_id":"cl40nn335000imculhxn7ecj1"},{"post_id":"cl49vqqoy00001oul5k1wa4n1","tag_id":"cl49vqqs800021oul12e5g5d1","_id":"cl49vqqsa00031oulfrild2s1"},{"post_id":"cl4m00udc0000zoul0eqy5b48","tag_id":"cl4m00ue80002zoul4qt8gyij","_id":"cl4m00ueb0003zoul6lkp78rg"}],"Tag":[{"name":"GNN","_id":"cl40nn32v0004mcul53gigx01"},{"name":"English Paper Writing","_id":"cl40nn332000bmcul60iog4or"},{"name":"Monocular Vision","_id":"cl40nn334000emcul6r21h5n8"},{"name":"CNN Regularization","_id":"cl49vqqs800021oul12e5g5d1"},{"name":"Poem","_id":"cl4m00ue80002zoul4qt8gyij"}]}}